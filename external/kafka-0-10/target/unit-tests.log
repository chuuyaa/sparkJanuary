21/02/21 22:50:55.107 main INFO ZooKeeperServer: Server environment:zookeeper.version=3.4.7-1713338, built on 11/09/2015 04:32 GMT
21/02/21 22:50:55.109 main INFO ZooKeeperServer: Server environment:host.name=DESKTOP-JPLSL4N
21/02/21 22:50:55.109 main INFO ZooKeeperServer: Server environment:java.version=1.8.0_221
21/02/21 22:50:55.109 main INFO ZooKeeperServer: Server environment:java.vendor=Oracle Corporation
21/02/21 22:50:55.109 main INFO ZooKeeperServer: Server environment:java.home=C:\Progra~1\Java\jdk1.8.0_221\jre
21/02/21 22:50:55.110 main INFO ZooKeeperServer: Server environment:java.class.path=C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\test-classes;C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\classes;C:\Users\User\sparkJanuary\external\kafka-0-10-token-provider\target\spark-token-provider-kafka-0-10_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-library\2.12.10\scala-library-2.12.10.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\User\.m2\repository\org\apache\avro\avro\1.8.2\avro-1.8.2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-compress\1.8.1\commons-compress-1.8.1.jar;C:\Users\User\.m2\repository\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-mapred\1.8.2\avro-mapred-1.8.2-hadoop2.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-ipc\1.8.2\avro-ipc-1.8.2.jar;C:\Users\User\.m2\repository\commons-codec\commons-codec\1.10\commons-codec-1.10.jar;C:\Users\User\.m2\repository\com\twitter\chill_2.12\0.9.5\chill_2.12-0.9.5.jar;C:\Users\User\.m2\repository\com\esotericsoftware\kryo-shaded\4.0.2\kryo-shaded-4.0.2.jar;C:\Users\User\.m2\repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;C:\Users\User\.m2\repository\com\twitter\chill-java\0.9.5\chill-java-0.9.5.jar;C:\Users\User\.m2\repository\org\apache\xbean\xbean-asm7-shaded\4.15\xbean-asm7-shaded-4.15.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-client\2.7.4\hadoop-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-common\2.7.4\hadoop-common-2.7.4.jar;C:\Users\User\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\User\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\User\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\User\.m2\repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;C:\Users\User\.m2\repository\javax\servlet\jsp\jsp-api\2.1\jsp-api-2.1.jar;C:\Users\User\.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\User\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\User\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\User\.m2\repository\commons-beanutils\commons-beanutils\1.9.4\commons-beanutils-1.9.4.jar;C:\Users\User\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\User\.m2\repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-auth\2.7.4\hadoop-auth-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpclient\4.5.6\httpclient-4.5.6.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpcore\4.4.12\httpcore-4.4.12.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-client\2.7.1\curator-client-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-hdfs\2.7.4\hadoop-hdfs-2.7.4.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\User\.m2\repository\xerces\xercesImpl\2.12.0\xercesImpl-2.12.0.jar;C:\Users\User\.m2\repository\xml-apis\xml-apis\1.4.01\xml-apis-1.4.01.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.7.4\hadoop-mapreduce-client-app-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.7.4\hadoop-mapreduce-client-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-client\2.7.4\hadoop-yarn-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-server-common\2.7.4\hadoop-yarn-server-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.7.4\hadoop-mapreduce-client-shuffle-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-api\2.7.4\hadoop-yarn-api-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.4\hadoop-mapreduce-client-core-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-common\2.7.4\hadoop-yarn-common-2.7.4.jar;C:\Users\User\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\User\.m2\repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.7.4\hadoop-mapreduce-client-jobclient-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-annotations\2.7.4\hadoop-annotations-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-launcher_2.12\3.0.1\spark-launcher_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-kvstore_2.12\3.0.1\spark-kvstore_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.10.0\jackson-core-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.10.0\jackson-annotations-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-common_2.12\3.0.1\spark-network-common_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-shuffle_2.12\3.0.1\spark-network-shuffle_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-unsafe_2.12\3.0.1\spark-unsafe_2.12-3.0.1.jar;C:\Users\User\.m2\repository\javax\activation\activation\1.1.1\activation-1.1.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-recipes\2.7.1\curator-recipes-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-framework\2.7.1\curator-framework-2.7.1.jar;C:\Users\User\.m2\repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;C:\Users\User\.m2\repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-lang3\3.9\commons-lang3-3.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-text\1.6\commons-text-1.6.jar;C:\Users\User\.m2\repository\com\google\code\findbugs\jsr305\3.0.0\jsr305-3.0.0.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-api\1.7.30\slf4j-api-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jul-to-slf4j\1.7.30\jul-to-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jcl-over-slf4j\1.7.30\jcl-over-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-log4j12\1.7.30\slf4j-log4j12-1.7.30.jar;C:\Users\User\.m2\repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\User\.m2\repository\org\xerial\snappy\snappy-java\1.1.7.5\snappy-java-1.1.7.5.jar;C:\Users\User\.m2\repository\org\lz4\lz4-java\1.7.1\lz4-java-1.7.1.jar;C:\Users\User\.m2\repository\com\github\luben\zstd-jni\1.4.4-3\zstd-jni-1.4.4-3.jar;C:\Users\User\.m2\repository\org\roaringbitmap\RoaringBitmap\0.7.45\RoaringBitmap-0.7.45.jar;C:\Users\User\.m2\repository\org\roaringbitmap\shims\0.7.45\shims-0.7.45.jar;C:\Users\User\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-xml_2.12\1.2.0\scala-xml_2.12-1.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-reflect\2.12.10\scala-reflect-2.12.10.jar;C:\Users\User\.m2\repository\org\json4s\json4s-jackson_2.12\3.6.6\json4s-jackson_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-core_2.12\3.6.6\json4s-core_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-ast_2.12\3.6.6\json4s-ast_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-scalap_2.12\3.6.6\json4s-scalap_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-client\2.30\jersey-client-2.30.jar;C:\Users\User\.m2\repository\jakarta\ws\rs\jakarta.ws.rs-api\2.1.6\jakarta.ws.rs-api-2.1.6.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\jakarta.inject\2.6.1\jakarta.inject-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-common\2.30\jersey-common-2.30.jar;C:\Users\User\.m2\repository\jakarta\annotation\jakarta.annotation-api\1.3.5\jakarta.annotation-api-1.3.5.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\osgi-resource-locator\1.0.3\osgi-resource-locator-1.0.3.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-server\2.30\jersey-server-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\media\jersey-media-jaxb\2.30\jersey-media-jaxb-2.30.jar;C:\Users\User\.m2\repository\jakarta\validation\jakarta.validation-api\2.0.2\jakarta.validation-api-2.0.2.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet\2.30\jersey-container-servlet-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.30\jersey-container-servlet-core-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\inject\jersey-hk2\2.30\jersey-hk2-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-locator\2.6.1\hk2-locator-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\aopalliance-repackaged\2.6.1\aopalliance-repackaged-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-api\2.6.1\hk2-api-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-utils\2.6.1\hk2-utils-2.6.1.jar;C:\Users\User\.m2\repository\org\javassist\javassist\3.25.0-GA\javassist-3.25.0-GA.jar;C:\Users\User\.m2\repository\io\netty\netty-all\4.1.47.Final\netty-all-4.1.47.Final.jar;C:\Users\User\.m2\repository\com\clearspring\analytics\stream\2.9.6\stream-2.9.6.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-core\4.1.1\metrics-core-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jvm\4.1.1\metrics-jvm-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-json\4.1.1\metrics-json-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-graphite\4.1.1\metrics-graphite-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jmx\4.1.1\metrics-jmx-4.1.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.10.0\jackson-databind-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.12\2.10.0\jackson-module-scala_2.12-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.10.0\jackson-module-paranamer-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\User\.m2\repository\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\User\.m2\repository\net\razorvine\pyrolite\4.30\pyrolite-4.30.jar;C:\Users\User\.m2\repository\net\sf\py4j\py4j\0.10.9\py4j-0.10.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka-clients\2.4.1\kafka-clients-2.4.1.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka_2.12\2.4.1\kafka_2.12-2.4.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.10.0\jackson-dataformat-csv-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.10.0\jackson-datatype-jdk8-2.10.0.jar;C:\Users\User\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.12\2.1.2\scala-collection-compat_2.12-2.1.2.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.12\0.9.0\scala-java8-compat_2.12-0.9.0.jar;C:\Users\User\.m2\repository\com\typesafe\scala-logging\scala-logging_2.12\3.9.2\scala-logging_2.12-3.9.2.jar;C:\Users\User\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\User\.m2\repository\org\apache\zookeeper\zookeeper\3.4.7\zookeeper-3.4.7.jar;C:\Users\User\.m2\repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;C:\Users\User\.m2\repository\org\scalacheck\scalacheck_2.12\1.14.2\scalacheck_2.12-1.14.2.jar;C:\Users\User\.m2\repository\org\scala-sbt\test-interface\1.0\test-interface-1.0.jar;C:\Users\User\.m2\repository\org\mockito\mockito-core\3.1.0\mockito-core-3.1.0.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy\1.9.10\byte-buddy-1.9.10.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy-agent\1.9.10\byte-buddy-agent-1.9.10.jar;C:\Users\User\.m2\repository\org\objenesis\objenesis\2.6\objenesis-2.6.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\jmock\jmock-junit4\2.8.4\jmock-junit4-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock\2.8.4\jmock-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock-testjar\2.8.4\jmock-testjar-2.8.4.jar;C:\Users\User\.m2\repository\cglib\cglib\3.2.0\cglib-3.2.0.jar;C:\Users\User\.m2\repository\org\apache\ant\ant\1.9.4\ant-1.9.4.jar;C:\Users\User\.m2\repository\org\apache\ant\ant-launcher\1.9.4\ant-launcher-1.9.4.jar;C:\Users\User\.m2\repository\org\ow2\asm\asm\5.0.4\asm-5.0.4.jar;C:\Users\User\.m2\repository\org\apache-extras\beanshell\bsh\2.0b6\bsh-2.0b6.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-library\1.3\hamcrest-library-1.3.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\User\.m2\repository\org\scalatest\scalatest_2.12\3.0.8\scalatest_2.12-3.0.8.jar;C:\Users\User\.m2\repository\org\scalactic\scalactic_2.12\3.0.8\scalactic_2.12-3.0.8.jar;C:\Users\User\.m2\repository\junit\junit\4.12\junit-4.12.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;C:\Users\User\.m2\repository\com\novocode\junit-interface\0.11\junit-interface-0.11.jar;
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:java.library.path=C:\Progra~1\Java\jdk1.8.0_221\jre\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\Programs\MiKTeX 2.9\miktex\bin\x64\;C:\Users\User\sparkJanuary\bin;C:\hadoop-3.1.2\sbin;C:\Progra~1\Java\jdk1.8.0_221;C:\hadoop-3.1.2;C:\hadoop-3.1.2\bin;C:\hadoop-3.1.2\lib\native;C:\Progra~1\apache-maven-3.6.3\bin;C:\Program Files\apache-maven-3.6.3\bin;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Rtools 4.0;C:\Users\User\Anaconda3\envs\kublasean\Lib\R\bin\x64\Rgui.exe;C:\Users\User\sparkJanuary\python;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;;;.
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:java.io.tmpdir=C:\Users\User\sparkJanuary\external\kafka-0-10\target/tmp
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:java.compiler=<NA>
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:os.name=Windows 10
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:os.arch=amd64
21/02/21 22:50:55.114 main INFO ZooKeeperServer: Server environment:os.version=10.0
21/02/21 22:50:55.115 main INFO ZooKeeperServer: Server environment:user.name=User
21/02/21 22:50:55.115 main INFO ZooKeeperServer: Server environment:user.home=C:\Users\User
21/02/21 22:50:55.115 main INFO ZooKeeperServer: Server environment:user.dir=C:\Users\User\sparkJanuary\external\kafka-0-10
21/02/21 22:50:55.151 main INFO ZooKeeperServer: Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f5d29dcc-d21e-4fc3-8733-1f0c0bc651f2\version-2 snapdir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-201ce3bc-54f3-4628-ab18-05a098129826\version-2
21/02/21 22:50:55.200 main INFO NIOServerCnxnFactory: binding to port /127.0.0.1:0
21/02/21 22:50:55.352 main INFO Log4jControllerRegistration$: Registered kafka:type=kafka.Log4jController MBean
21/02/21 22:50:55.391 main INFO ZooKeeperClient: [ZooKeeperClient] Initializing a new session to 127.0.0.1:56555.
21/02/21 22:50:55.397 main INFO ZooKeeper: Client environment:zookeeper.version=3.4.7-1713338, built on 11/09/2015 04:32 GMT
21/02/21 22:50:55.398 main INFO ZooKeeper: Client environment:host.name=DESKTOP-JPLSL4N
21/02/21 22:50:55.398 main INFO ZooKeeper: Client environment:java.version=1.8.0_221
21/02/21 22:50:55.398 main INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
21/02/21 22:50:55.398 main INFO ZooKeeper: Client environment:java.home=C:\Progra~1\Java\jdk1.8.0_221\jre
21/02/21 22:50:55.398 main INFO ZooKeeper: Client environment:java.class.path=C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\test-classes;C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\classes;C:\Users\User\sparkJanuary\external\kafka-0-10-token-provider\target\spark-token-provider-kafka-0-10_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-library\2.12.10\scala-library-2.12.10.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\User\.m2\repository\org\apache\avro\avro\1.8.2\avro-1.8.2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-compress\1.8.1\commons-compress-1.8.1.jar;C:\Users\User\.m2\repository\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-mapred\1.8.2\avro-mapred-1.8.2-hadoop2.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-ipc\1.8.2\avro-ipc-1.8.2.jar;C:\Users\User\.m2\repository\commons-codec\commons-codec\1.10\commons-codec-1.10.jar;C:\Users\User\.m2\repository\com\twitter\chill_2.12\0.9.5\chill_2.12-0.9.5.jar;C:\Users\User\.m2\repository\com\esotericsoftware\kryo-shaded\4.0.2\kryo-shaded-4.0.2.jar;C:\Users\User\.m2\repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;C:\Users\User\.m2\repository\com\twitter\chill-java\0.9.5\chill-java-0.9.5.jar;C:\Users\User\.m2\repository\org\apache\xbean\xbean-asm7-shaded\4.15\xbean-asm7-shaded-4.15.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-client\2.7.4\hadoop-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-common\2.7.4\hadoop-common-2.7.4.jar;C:\Users\User\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\User\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\User\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\User\.m2\repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;C:\Users\User\.m2\repository\javax\servlet\jsp\jsp-api\2.1\jsp-api-2.1.jar;C:\Users\User\.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\User\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\User\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\User\.m2\repository\commons-beanutils\commons-beanutils\1.9.4\commons-beanutils-1.9.4.jar;C:\Users\User\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\User\.m2\repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-auth\2.7.4\hadoop-auth-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpclient\4.5.6\httpclient-4.5.6.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpcore\4.4.12\httpcore-4.4.12.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-client\2.7.1\curator-client-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-hdfs\2.7.4\hadoop-hdfs-2.7.4.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\User\.m2\repository\xerces\xercesImpl\2.12.0\xercesImpl-2.12.0.jar;C:\Users\User\.m2\repository\xml-apis\xml-apis\1.4.01\xml-apis-1.4.01.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.7.4\hadoop-mapreduce-client-app-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.7.4\hadoop-mapreduce-client-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-client\2.7.4\hadoop-yarn-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-server-common\2.7.4\hadoop-yarn-server-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.7.4\hadoop-mapreduce-client-shuffle-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-api\2.7.4\hadoop-yarn-api-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.4\hadoop-mapreduce-client-core-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-common\2.7.4\hadoop-yarn-common-2.7.4.jar;C:\Users\User\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\User\.m2\repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.7.4\hadoop-mapreduce-client-jobclient-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-annotations\2.7.4\hadoop-annotations-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-launcher_2.12\3.0.1\spark-launcher_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-kvstore_2.12\3.0.1\spark-kvstore_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.10.0\jackson-core-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.10.0\jackson-annotations-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-common_2.12\3.0.1\spark-network-common_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-shuffle_2.12\3.0.1\spark-network-shuffle_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-unsafe_2.12\3.0.1\spark-unsafe_2.12-3.0.1.jar;C:\Users\User\.m2\repository\javax\activation\activation\1.1.1\activation-1.1.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-recipes\2.7.1\curator-recipes-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-framework\2.7.1\curator-framework-2.7.1.jar;C:\Users\User\.m2\repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;C:\Users\User\.m2\repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-lang3\3.9\commons-lang3-3.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-text\1.6\commons-text-1.6.jar;C:\Users\User\.m2\repository\com\google\code\findbugs\jsr305\3.0.0\jsr305-3.0.0.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-api\1.7.30\slf4j-api-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jul-to-slf4j\1.7.30\jul-to-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jcl-over-slf4j\1.7.30\jcl-over-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-log4j12\1.7.30\slf4j-log4j12-1.7.30.jar;C:\Users\User\.m2\repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\User\.m2\repository\org\xerial\snappy\snappy-java\1.1.7.5\snappy-java-1.1.7.5.jar;C:\Users\User\.m2\repository\org\lz4\lz4-java\1.7.1\lz4-java-1.7.1.jar;C:\Users\User\.m2\repository\com\github\luben\zstd-jni\1.4.4-3\zstd-jni-1.4.4-3.jar;C:\Users\User\.m2\repository\org\roaringbitmap\RoaringBitmap\0.7.45\RoaringBitmap-0.7.45.jar;C:\Users\User\.m2\repository\org\roaringbitmap\shims\0.7.45\shims-0.7.45.jar;C:\Users\User\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-xml_2.12\1.2.0\scala-xml_2.12-1.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-reflect\2.12.10\scala-reflect-2.12.10.jar;C:\Users\User\.m2\repository\org\json4s\json4s-jackson_2.12\3.6.6\json4s-jackson_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-core_2.12\3.6.6\json4s-core_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-ast_2.12\3.6.6\json4s-ast_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-scalap_2.12\3.6.6\json4s-scalap_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-client\2.30\jersey-client-2.30.jar;C:\Users\User\.m2\repository\jakarta\ws\rs\jakarta.ws.rs-api\2.1.6\jakarta.ws.rs-api-2.1.6.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\jakarta.inject\2.6.1\jakarta.inject-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-common\2.30\jersey-common-2.30.jar;C:\Users\User\.m2\repository\jakarta\annotation\jakarta.annotation-api\1.3.5\jakarta.annotation-api-1.3.5.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\osgi-resource-locator\1.0.3\osgi-resource-locator-1.0.3.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-server\2.30\jersey-server-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\media\jersey-media-jaxb\2.30\jersey-media-jaxb-2.30.jar;C:\Users\User\.m2\repository\jakarta\validation\jakarta.validation-api\2.0.2\jakarta.validation-api-2.0.2.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet\2.30\jersey-container-servlet-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.30\jersey-container-servlet-core-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\inject\jersey-hk2\2.30\jersey-hk2-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-locator\2.6.1\hk2-locator-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\aopalliance-repackaged\2.6.1\aopalliance-repackaged-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-api\2.6.1\hk2-api-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-utils\2.6.1\hk2-utils-2.6.1.jar;C:\Users\User\.m2\repository\org\javassist\javassist\3.25.0-GA\javassist-3.25.0-GA.jar;C:\Users\User\.m2\repository\io\netty\netty-all\4.1.47.Final\netty-all-4.1.47.Final.jar;C:\Users\User\.m2\repository\com\clearspring\analytics\stream\2.9.6\stream-2.9.6.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-core\4.1.1\metrics-core-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jvm\4.1.1\metrics-jvm-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-json\4.1.1\metrics-json-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-graphite\4.1.1\metrics-graphite-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jmx\4.1.1\metrics-jmx-4.1.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.10.0\jackson-databind-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.12\2.10.0\jackson-module-scala_2.12-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.10.0\jackson-module-paranamer-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\User\.m2\repository\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\User\.m2\repository\net\razorvine\pyrolite\4.30\pyrolite-4.30.jar;C:\Users\User\.m2\repository\net\sf\py4j\py4j\0.10.9\py4j-0.10.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka-clients\2.4.1\kafka-clients-2.4.1.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka_2.12\2.4.1\kafka_2.12-2.4.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.10.0\jackson-dataformat-csv-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.10.0\jackson-datatype-jdk8-2.10.0.jar;C:\Users\User\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.12\2.1.2\scala-collection-compat_2.12-2.1.2.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.12\0.9.0\scala-java8-compat_2.12-0.9.0.jar;C:\Users\User\.m2\repository\com\typesafe\scala-logging\scala-logging_2.12\3.9.2\scala-logging_2.12-3.9.2.jar;C:\Users\User\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\User\.m2\repository\org\apache\zookeeper\zookeeper\3.4.7\zookeeper-3.4.7.jar;C:\Users\User\.m2\repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;C:\Users\User\.m2\repository\org\scalacheck\scalacheck_2.12\1.14.2\scalacheck_2.12-1.14.2.jar;C:\Users\User\.m2\repository\org\scala-sbt\test-interface\1.0\test-interface-1.0.jar;C:\Users\User\.m2\repository\org\mockito\mockito-core\3.1.0\mockito-core-3.1.0.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy\1.9.10\byte-buddy-1.9.10.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy-agent\1.9.10\byte-buddy-agent-1.9.10.jar;C:\Users\User\.m2\repository\org\objenesis\objenesis\2.6\objenesis-2.6.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\jmock\jmock-junit4\2.8.4\jmock-junit4-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock\2.8.4\jmock-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock-testjar\2.8.4\jmock-testjar-2.8.4.jar;C:\Users\User\.m2\repository\cglib\cglib\3.2.0\cglib-3.2.0.jar;C:\Users\User\.m2\repository\org\apache\ant\ant\1.9.4\ant-1.9.4.jar;C:\Users\User\.m2\repository\org\apache\ant\ant-launcher\1.9.4\ant-launcher-1.9.4.jar;C:\Users\User\.m2\repository\org\ow2\asm\asm\5.0.4\asm-5.0.4.jar;C:\Users\User\.m2\repository\org\apache-extras\beanshell\bsh\2.0b6\bsh-2.0b6.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-library\1.3\hamcrest-library-1.3.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\User\.m2\repository\org\scalatest\scalatest_2.12\3.0.8\scalatest_2.12-3.0.8.jar;C:\Users\User\.m2\repository\org\scalactic\scalactic_2.12\3.0.8\scalactic_2.12-3.0.8.jar;C:\Users\User\.m2\repository\junit\junit\4.12\junit-4.12.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;C:\Users\User\.m2\repository\com\novocode\junit-interface\0.11\junit-interface-0.11.jar;
21/02/21 22:50:55.399 main INFO ZooKeeper: Client environment:java.library.path=C:\Progra~1\Java\jdk1.8.0_221\jre\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\Programs\MiKTeX 2.9\miktex\bin\x64\;C:\Users\User\sparkJanuary\bin;C:\hadoop-3.1.2\sbin;C:\Progra~1\Java\jdk1.8.0_221;C:\hadoop-3.1.2;C:\hadoop-3.1.2\bin;C:\hadoop-3.1.2\lib\native;C:\Progra~1\apache-maven-3.6.3\bin;C:\Program Files\apache-maven-3.6.3\bin;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Rtools 4.0;C:\Users\User\Anaconda3\envs\kublasean\Lib\R\bin\x64\Rgui.exe;C:\Users\User\sparkJanuary\python;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;;;.
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:java.io.tmpdir=C:\Users\User\sparkJanuary\external\kafka-0-10\target/tmp
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:java.compiler=<NA>
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:os.name=Windows 10
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:os.arch=amd64
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:os.version=10.0
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:user.name=User
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:user.home=C:\Users\User
21/02/21 22:50:55.400 main INFO ZooKeeper: Client environment:user.dir=C:\Users\User\sparkJanuary\external\kafka-0-10
21/02/21 22:50:55.403 main INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56555 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6631f5ca
21/02/21 22:50:55.426 main INFO ZooKeeperClient: [ZooKeeperClient] Waiting until connected.
21/02/21 22:50:55.426 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56555. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:50:55.428 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56555, initiating session
21/02/21 22:50:55.428 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56558
21/02/21 22:50:55.437 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56558
21/02/21 22:50:55.441 SyncThread:0 INFO FileTxnLog: Creating new log file: log.1
21/02/21 22:50:55.459 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c5126d690000 with negotiated timeout 10000 for client /127.0.0.1:56558
21/02/21 22:50:55.460 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56555, sessionid = 0x177c5126d690000, negotiated timeout = 10000
21/02/21 22:50:55.465 main INFO ZooKeeperClient: [ZooKeeperClient] Connected.
21/02/21 22:51:00.615 main INFO KafkaServer: starting
21/02/21 22:51:00.616 main INFO KafkaServer: Connecting to zookeeper on 127.0.0.1:56555
21/02/21 22:51:00.622 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:56555.
21/02/21 22:51:00.622 main INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56555 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@15a04efb
21/02/21 22:51:00.624 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Waiting until connected.
21/02/21 22:51:00.624 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56555. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:00.625 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56555, initiating session
21/02/21 22:51:00.625 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56561
21/02/21 22:51:00.625 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56561
21/02/21 22:51:00.630 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c5126d690001 with negotiated timeout 6000 for client /127.0.0.1:56561
21/02/21 22:51:00.630 main-SendThread(127.0.0.1:56555) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56555, sessionid = 0x177c5126d690001, negotiated timeout = 6000
21/02/21 22:51:00.630 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Connected.
21/02/21 22:51:00.710 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x2 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
21/02/21 22:51:00.723 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x6 zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
21/02/21 22:51:00.732 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x9 zxid:0xb txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
21/02/21 22:51:01.140 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x15 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
21/02/21 22:51:01.150 main INFO KafkaServer: Cluster ID = rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:01.158 main WARN BrokerMetadataCheckpoint: No meta.properties file under dir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa\meta.properties
21/02/21 22:51:01.221 main INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56555
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:01.233 main INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56555
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:01.278 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Starting
21/02/21 22:51:01.278 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Starting
21/02/21 22:51:01.281 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Starting
21/02/21 22:51:01.326 main INFO LogManager: Loading logs.
21/02/21 22:51:01.343 main INFO LogManager: Logs loading complete in 17 ms.
21/02/21 22:51:01.383 main INFO LogManager: Starting log cleanup with a period of 300000 ms.
21/02/21 22:51:01.389 main INFO LogManager: Starting log flusher with a default period of 9223372036854775807 ms.
21/02/21 22:51:01.393 main INFO LogCleaner: Starting the log cleaner
21/02/21 22:51:01.512 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:51:02.019 main INFO Acceptor: Awaiting socket connections on 127.0.0.1:56564.
21/02/21 22:51:02.067 main INFO SocketServer: [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(127.0.0.1,0,ListenerName(PLAINTEXT),PLAINTEXT)
21/02/21 22:51:02.068 main INFO SocketServer: [SocketServer brokerId=0] Started 1 acceptor threads for data-plane
21/02/21 22:51:02.101 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Starting
21/02/21 22:51:02.103 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Starting
21/02/21 22:51:02.104 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Starting
21/02/21 22:51:02.105 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Starting
21/02/21 22:51:02.127 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Starting
21/02/21 22:51:02.156 main INFO KafkaZkClient: Creating /brokers/ids/0 (is it secure? false)
21/02/21 22:51:02.182 main INFO KafkaZkClient: Stat of the created znode at /brokers/ids/0 is: 25,25,1613919062173,1613919062173,1,0,0,105769799202177025,190,0,25

21/02/21 22:51:02.182 main INFO KafkaZkClient: Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(127.0.0.1,56564,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25
21/02/21 22:51:02.305 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Starting
21/02/21 22:51:02.317 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Starting
21/02/21 22:51:02.322 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Starting
21/02/21 22:51:02.324 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Starting
21/02/21 22:51:02.345 controller-event-thread INFO KafkaZkClient: Successfully created /controller_epoch with initial epoch 0
21/02/21 22:51:02.366 controller-event-thread INFO KafkaController: [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
21/02/21 22:51:02.367 controller-event-thread INFO KafkaController: [Controller id=0] Registering handlers
21/02/21 22:51:02.372 controller-event-thread INFO KafkaController: [Controller id=0] Deleting log dir event notifications
21/02/21 22:51:02.378 controller-event-thread INFO KafkaController: [Controller id=0] Deleting isr change notifications
21/02/21 22:51:02.381 controller-event-thread INFO KafkaController: [Controller id=0] Initializing controller context
21/02/21 22:51:02.506 main INFO GroupCoordinator: [GroupCoordinator 0]: Starting up.
21/02/21 22:51:02.508 main INFO GroupCoordinator: [GroupCoordinator 0]: Startup complete.
21/02/21 22:51:02.519 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 10 milliseconds.
21/02/21 22:51:02.532 main INFO ProducerIdManager: [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
21/02/21 22:51:02.568 controller-event-thread INFO KafkaController: [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
21/02/21 22:51:02.576 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Starting up.
21/02/21 22:51:02.578 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Startup complete.
21/02/21 22:51:02.580 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Starting
21/02/21 22:51:02.598 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Starting
21/02/21 22:51:02.598 controller-event-thread INFO KafkaController: [Controller id=0] Currently active brokers in the cluster: Set(0)
21/02/21 22:51:02.599 controller-event-thread INFO KafkaController: [Controller id=0] Currently shutting brokers in the cluster: Set()
21/02/21 22:51:02.600 controller-event-thread INFO KafkaController: [Controller id=0] Current list of topics in the cluster: Set()
21/02/21 22:51:02.600 controller-event-thread INFO KafkaController: [Controller id=0] Fetching topic deletions in progress
21/02/21 22:51:02.604 controller-event-thread INFO KafkaController: [Controller id=0] List of topics to be deleted: 
21/02/21 22:51:02.604 controller-event-thread INFO KafkaController: [Controller id=0] List of topics ineligible for deletion: 
21/02/21 22:51:02.605 controller-event-thread INFO KafkaController: [Controller id=0] Initializing topic deletion manager
21/02/21 22:51:02.606 controller-event-thread INFO TopicDeletionManager: [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
21/02/21 22:51:02.606 controller-event-thread INFO KafkaController: [Controller id=0] Sending update metadata request
21/02/21 22:51:02.626 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Initializing replica state
21/02/21 22:51:02.627 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering online replica state changes
21/02/21 22:51:02.633 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Starting
21/02/21 22:51:02.636 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
21/02/21 22:51:02.637 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Initializing partition state
21/02/21 22:51:02.639 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Triggering online partition state changes
21/02/21 22:51:02.642 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Controller 0 connected to 127.0.0.1:56564 (id: 0 rack: null) for sending state change requests
21/02/21 22:51:02.648 controller-event-thread INFO KafkaController: [Controller id=0] Ready to serve as the new controller with epoch 1
21/02/21 22:51:02.655 controller-event-thread INFO KafkaController: [Controller id=0] Partitions undergoing preferred replica election: 
21/02/21 22:51:02.657 controller-event-thread INFO KafkaController: [Controller id=0] Partitions that completed preferred replica election: 
21/02/21 22:51:02.657 controller-event-thread INFO KafkaController: [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
21/02/21 22:51:02.657 controller-event-thread INFO KafkaController: [Controller id=0] Resuming preferred replica election for partitions: 
21/02/21 22:51:02.659 controller-event-thread INFO KafkaController: [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
21/02/21 22:51:02.670 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Starting
21/02/21 22:51:02.672 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:multi cxid:0x31 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
21/02/21 22:51:02.678 controller-event-thread INFO KafkaController: [Controller id=0] Starting the controller scheduler
21/02/21 22:51:02.691 main INFO SocketServer: [SocketServer brokerId=0] Started data-plane processors for 1 acceptors
21/02/21 22:51:02.703 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:02.703 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:02.703 main INFO AppInfoParser: Kafka startTimeMs: 1613919062693
21/02/21 22:51:02.706 main INFO KafkaServer: [KafkaServer id=0] started
21/02/21 22:51:02.707 main INFO Utils: Successfully started service 'KafkaBroker' on port 56564.
21/02/21 22:51:02.770 main INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:03.436 main INFO ResourceUtils: ==============================================================
21/02/21 22:51:03.437 main INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:03.437 main INFO ResourceUtils: ==============================================================
21/02/21 22:51:03.439 main INFO SparkContext: Submitted application: JavaDirectKafkaStreamSuite
21/02/21 22:51:03.538 main INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:03.539 main INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:03.539 main INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:03.540 main INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:03.541 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:05.346 main INFO Utils: Successfully started service 'sparkDriver' on port 56594.
21/02/21 22:51:05.383 main INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:05.428 main INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:05.471 main INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:05.471 main INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:05.477 main INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:05.507 main INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-132eb8a9-f736-47dd-8cf0-6999081df265
21/02/21 22:51:05.551 main INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:05.580 main INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:05.878 main INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:05.931 main INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56613.
21/02/21 22:51:05.932 main INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:56613
21/02/21 22:51:05.934 main INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:05.946 main INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 56613, None)
21/02/21 22:51:05.950 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:56613 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 56613, None)
21/02/21 22:51:05.955 main INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 56613, None)
21/02/21 22:51:05.957 main INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 56613, None)
21/02/21 22:51:06.042 main INFO log: Logging initialized @17943ms to org.sparkproject.jetty.util.log.Slf4jLog
21/02/21 22:51:06.239 main INFO AdminZkClient: Creating topic topic1 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:06.241 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690000 type:setData cxid:0x4 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topic1 Error:KeeperErrorCode = NoNode for /config/topics/topic1
21/02/21 22:51:06.265 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topic1)], deleted topics: [Set()], new partition replica assignment [Map(topic1-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:06.266 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topic1-0
21/02/21 22:51:06.344 data-plane-kafka-request-handler-0 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic1-0)
21/02/21 22:51:06.481 data-plane-kafka-request-handler-0 INFO Log: [Log partition=topic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:06.494 data-plane-kafka-request-handler-0 INFO Log: [Log partition=topic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 86 ms
21/02/21 22:51:06.497 data-plane-kafka-request-handler-0 INFO LogManager: Created log for partition topic1-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa\topic1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:06.499 data-plane-kafka-request-handler-0 INFO Partition: [Partition topic1-0 broker=0] No checkpointed highwatermark is found for partition topic1-0
21/02/21 22:51:06.499 data-plane-kafka-request-handler-0 INFO Partition: [Partition topic1-0 broker=0] Log loaded for partition topic1-0 with initial high watermark 0
21/02/21 22:51:06.500 data-plane-kafka-request-handler-0 INFO Partition: [Partition topic1-0 broker=0] topic1-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:06.867 main INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:56564]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:06.920 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:06.920 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:06.921 main INFO AppInfoParser: Kafka startTimeMs: 1613919066920
21/02/21 22:51:07.008 kafka-producer-network-thread | producer-1 INFO Metadata: [Producer clientId=producer-1] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:07.042 main INFO KafkaProducer: [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:07.246 main INFO AdminZkClient: Creating topic topic2 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:07.247 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690000 type:setData cxid:0xb zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/topic2 Error:KeeperErrorCode = NoNode for /config/topics/topic2
21/02/21 22:51:07.267 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topic2)], deleted topics: [Set()], new partition replica assignment [Map(topic2-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:07.268 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topic2-0
21/02/21 22:51:07.297 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic2-0)
21/02/21 22:51:07.310 data-plane-kafka-request-handler-2 INFO Log: [Log partition=topic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:07.315 data-plane-kafka-request-handler-2 INFO Log: [Log partition=topic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms
21/02/21 22:51:07.316 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition topic2-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa\topic2-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:07.318 data-plane-kafka-request-handler-2 INFO Partition: [Partition topic2-0 broker=0] No checkpointed highwatermark is found for partition topic2-0
21/02/21 22:51:07.318 data-plane-kafka-request-handler-2 INFO Partition: [Partition topic2-0 broker=0] Log loaded for partition topic2-0 with initial high watermark 0
21/02/21 22:51:07.318 data-plane-kafka-request-handler-2 INFO Partition: [Partition topic2-0 broker=0] topic2-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:07.365 main INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:56564]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:07.372 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:07.373 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:07.373 main INFO AppInfoParser: Kafka startTimeMs: 1613919067372
21/02/21 22:51:07.382 kafka-producer-network-thread | producer-2 INFO Metadata: [Producer clientId=producer-2] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:07.385 main INFO KafkaProducer: [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:07.681 controller-event-thread INFO KafkaController: [Controller id=0] Processing automatic preferred replica leader election
21/02/21 22:51:07.753 main WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:07.753 main WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:07.754 main WARN KafkaUtils: overriding executor group.id to spark-executor-java-test-consumer-1174870607-1613919067411
21/02/21 22:51:07.754 main WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:07.919 main WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:07.919 main WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:07.919 main WARN KafkaUtils: overriding executor group.id to spark-executor-java-test-consumer--737770960-1613919067918
21/02/21 22:51:07.919 main WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:07.992 streaming-start INFO DirectKafkaInputDStream: Slide time = 200 ms
21/02/21 22:51:07.993 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.993 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:07.994 streaming-start INFO DirectKafkaInputDStream: Remember interval = 200 ms
21/02/21 22:51:07.994 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@2d2806fc
21/02/21 22:51:07.995 streaming-start INFO TransformedDStream: Slide time = 200 ms
21/02/21 22:51:07.995 streaming-start INFO TransformedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.995 streaming-start INFO TransformedDStream: Checkpoint interval = null
21/02/21 22:51:07.995 streaming-start INFO TransformedDStream: Remember interval = 200 ms
21/02/21 22:51:07.995 streaming-start INFO TransformedDStream: Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@7caf6817
21/02/21 22:51:07.995 streaming-start INFO MappedDStream: Slide time = 200 ms
21/02/21 22:51:07.995 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.995 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Remember interval = 200 ms
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c39b2cd
21/02/21 22:51:07.996 streaming-start INFO DirectKafkaInputDStream: Slide time = 200 ms
21/02/21 22:51:07.996 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.996 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:07.996 streaming-start INFO DirectKafkaInputDStream: Remember interval = 200 ms
21/02/21 22:51:07.996 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@538b16fa
21/02/21 22:51:07.996 streaming-start INFO TransformedDStream: Slide time = 200 ms
21/02/21 22:51:07.996 streaming-start INFO TransformedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.996 streaming-start INFO TransformedDStream: Checkpoint interval = null
21/02/21 22:51:07.996 streaming-start INFO TransformedDStream: Remember interval = 200 ms
21/02/21 22:51:07.996 streaming-start INFO TransformedDStream: Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@3e79f591
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Slide time = 200 ms
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Remember interval = 200 ms
21/02/21 22:51:07.996 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3991cd99
21/02/21 22:51:07.996 streaming-start INFO UnionDStream: Slide time = 200 ms
21/02/21 22:51:07.996 streaming-start INFO UnionDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.997 streaming-start INFO UnionDStream: Checkpoint interval = null
21/02/21 22:51:07.997 streaming-start INFO UnionDStream: Remember interval = 200 ms
21/02/21 22:51:07.997 streaming-start INFO UnionDStream: Initialized and validated org.apache.spark.streaming.dstream.UnionDStream@48cfbfbc
21/02/21 22:51:07.997 streaming-start INFO ForEachDStream: Slide time = 200 ms
21/02/21 22:51:07.997 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:07.997 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:07.997 streaming-start INFO ForEachDStream: Remember interval = 200 ms
21/02/21 22:51:07.997 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@43785c99
21/02/21 22:51:08.138 scala-execution-context-global-106 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56564]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = java-test-consumer-1174870607-1613919067411
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:08.138 scala-execution-context-global-107 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56564]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = java-test-consumer--737770960-1613919067918
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:08.218 scala-execution-context-global-106 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:08.219 scala-execution-context-global-106 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:08.219 scala-execution-context-global-106 INFO AppInfoParser: Kafka startTimeMs: 1613919068218
21/02/21 22:51:08.219 scala-execution-context-global-107 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:08.220 scala-execution-context-global-107 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:08.220 scala-execution-context-global-107 INFO AppInfoParser: Kafka startTimeMs: 1613919068218
21/02/21 22:51:08.221 scala-execution-context-global-107 INFO KafkaConsumer: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Subscribed to topic(s): topic2
21/02/21 22:51:08.221 scala-execution-context-global-106 INFO KafkaConsumer: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Subscribed to topic(s): topic1
21/02/21 22:51:08.235 scala-execution-context-global-107 INFO Metadata: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:08.235 scala-execution-context-global-106 INFO Metadata: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:08.243 data-plane-kafka-request-handler-7 INFO AdminZkClient: Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:08.243 data-plane-kafka-request-handler-1 INFO AdminZkClient: Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:08.244 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:setData cxid:0x4f zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
21/02/21 22:51:08.245 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:setData cxid:0x50 zxid:0x2b txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
21/02/21 22:51:08.251 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x52 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /config/topics/__consumer_offsets
21/02/21 22:51:08.256 data-plane-kafka-request-handler-7 INFO KafkaApis: [KafkaApi-0] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful
21/02/21 22:51:08.258 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c5126d690001 type:create cxid:0x57 zxid:0x30 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets
21/02/21 22:51:08.259 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:08.259 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for __consumer_offsets-0
21/02/21 22:51:08.277 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-0)
21/02/21 22:51:08.286 data-plane-kafka-request-handler-2 INFO Log: [Log partition=__consumer_offsets-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:08.290 data-plane-kafka-request-handler-2 INFO Log: [Log partition=__consumer_offsets-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms
21/02/21 22:51:08.292 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition __consumer_offsets-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-8a6d5b89-d93b-4c73-9fe9-7b84d69b6baa\__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:08.292 data-plane-kafka-request-handler-2 INFO Partition: [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
21/02/21 22:51:08.292 data-plane-kafka-request-handler-2 INFO Partition: [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
21/02/21 22:51:08.293 data-plane-kafka-request-handler-2 INFO Partition: [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:08.303 data-plane-kafka-request-handler-2 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
21/02/21 22:51:08.313 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds.
21/02/21 22:51:08.345 scala-execution-context-global-107 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Discovered group coordinator 127.0.0.1:56564 (id: 2147483647 rack: null)
21/02/21 22:51:08.345 scala-execution-context-global-106 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Discovered group coordinator 127.0.0.1:56564 (id: 2147483647 rack: null)
21/02/21 22:51:08.353 scala-execution-context-global-107 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] (Re-)joining group
21/02/21 22:51:08.353 scala-execution-context-global-106 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] (Re-)joining group
21/02/21 22:51:08.404 scala-execution-context-global-106 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] (Re-)joining group
21/02/21 22:51:08.404 scala-execution-context-global-107 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] (Re-)joining group
21/02/21 22:51:08.420 data-plane-kafka-request-handler-0 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group java-test-consumer-1174870607-1613919067411 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-java-test-consumer-1174870607-1613919067411-1-60a1b9c9-b0ca-4d16-ac7a-6789369a4bc4 with group instanceid None)
21/02/21 22:51:08.420 data-plane-kafka-request-handler-1 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group java-test-consumer--737770960-1613919067918 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-java-test-consumer--737770960-1613919067918-2-ac1ae761-85a6-44b6-ab59-fba9c1a61767 with group instanceid None)
21/02/21 22:51:08.449 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group java-test-consumer--737770960-1613919067918 generation 1 (__consumer_offsets-0)
21/02/21 22:51:08.456 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group java-test-consumer-1174870607-1613919067411 generation 1 (__consumer_offsets-0)
21/02/21 22:51:08.462 scala-execution-context-global-107 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Finished assignment for group at generation 1: {consumer-java-test-consumer--737770960-1613919067918-2-ac1ae761-85a6-44b6-ab59-fba9c1a61767=Assignment(partitions=[topic2-0])}
21/02/21 22:51:08.462 scala-execution-context-global-106 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Finished assignment for group at generation 1: {consumer-java-test-consumer-1174870607-1613919067411-1-60a1b9c9-b0ca-4d16-ac7a-6789369a4bc4=Assignment(partitions=[topic1-0])}
21/02/21 22:51:08.470 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group java-test-consumer--737770960-1613919067918 for generation 1
21/02/21 22:51:08.470 data-plane-kafka-request-handler-7 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group java-test-consumer-1174870607-1613919067411 for generation 1
21/02/21 22:51:08.501 scala-execution-context-global-107 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Successfully joined group with generation 1
21/02/21 22:51:08.501 scala-execution-context-global-106 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Successfully joined group with generation 1
21/02/21 22:51:08.508 scala-execution-context-global-107 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Adding newly assigned partitions: topic2-0
21/02/21 22:51:08.508 scala-execution-context-global-106 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Adding newly assigned partitions: topic1-0
21/02/21 22:51:08.528 scala-execution-context-global-106 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Found no committed offset for partition topic1-0
21/02/21 22:51:08.528 scala-execution-context-global-107 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Found no committed offset for partition topic2-0
21/02/21 22:51:08.554 scala-execution-context-global-106 INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 0.
21/02/21 22:51:08.554 scala-execution-context-global-107 INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 0.
21/02/21 22:51:08.556 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919068000
21/02/21 22:51:08.556 streaming-start INFO JobGenerator: Started JobGenerator at 1613919068000 ms
21/02/21 22:51:08.559 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:08.565 main INFO StreamingContext: StreamingContext started
21/02/21 22:51:08.591 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:08.596 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:08.646 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:08.649 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:08.669 JobGenerator INFO JobScheduler: Added jobs for time 1613919068000 ms
21/02/21 22:51:08.673 JobScheduler INFO JobScheduler: Starting job streaming job 1613919068000 ms.0 from job set of time 1613919068000 ms
21/02/21 22:51:08.674 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:08.676 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:08.681 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:08.684 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:08.688 JobGenerator INFO JobScheduler: Added jobs for time 1613919068200 ms
21/02/21 22:51:08.690 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:08.692 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:08.696 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:08.700 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:08.703 JobGenerator INFO JobScheduler: Added jobs for time 1613919068400 ms
21/02/21 22:51:08.705 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:08.708 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:08.712 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:08.715 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:08.719 JobGenerator INFO JobScheduler: Added jobs for time 1613919068600 ms
21/02/21 22:51:08.774 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:08.792 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:08.792 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:08.793 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:08.794 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:08.801 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (UnionRDD[4] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:08.801 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:08.804 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:08.809 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:08.812 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:08.816 JobGenerator INFO JobScheduler: Added jobs for time 1613919068800 ms
21/02/21 22:51:08.901 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:08.983 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:08.986 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:08.990 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:09.005 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:09.010 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:09.012 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:09.015 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (UnionRDD[4] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.016 JobGenerator INFO JobScheduler: Added jobs for time 1613919069000 ms
21/02/21 22:51:09.017 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
21/02/21 22:51:09.087 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.092 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.107 Executor task launch worker for task 1 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
21/02/21 22:51:09.107 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:09.203 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:09.207 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:09.211 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:09.214 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:09.218 JobGenerator INFO JobScheduler: Added jobs for time 1613919069200 ms
21/02/21 22:51:09.405 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:09.408 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:09.412 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:09.414 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:09.418 JobGenerator INFO JobScheduler: Added jobs for time 1613919069400 ms
21/02/21 22:51:09.599 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic topic2, partition 0 offsets 0 -> 3
21/02/21 22:51:09.599 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic topic1, partition 0 offsets 0 -> 3
21/02/21 22:51:09.603 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:09.605 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:09.608 Executor task launch worker for task 0 INFO KafkaDataConsumer: Initializing cache 16 64 0.75
21/02/21 22:51:09.610 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:09.612 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:09.613 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:56564]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-java-test-consumer-1174870607-1613919067411
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:09.615 JobGenerator INFO JobScheduler: Added jobs for time 1613919069600 ms
21/02/21 22:51:09.618 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:09.619 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:09.619 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919069618
21/02/21 22:51:09.619 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1174870607-1613919067411-3, groupId=spark-executor-java-test-consumer-1174870607-1613919067411] Subscribed to partition(s): topic1-0
21/02/21 22:51:09.622 Executor task launch worker for task 1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:56564]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-java-test-consumer--737770960-1613919067918
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:09.622 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer-1174870607-1613919067411 topic1-0 0
21/02/21 22:51:09.623 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1174870607-1613919067411-3, groupId=spark-executor-java-test-consumer-1174870607-1613919067411] Seeking to offset 0 for partition topic1-0
21/02/21 22:51:09.628 Executor task launch worker for task 1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:09.628 Executor task launch worker for task 1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:09.628 Executor task launch worker for task 1 INFO AppInfoParser: Kafka startTimeMs: 1613919069628
21/02/21 22:51:09.629 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer--737770960-1613919067918-4, groupId=spark-executor-java-test-consumer--737770960-1613919067918] Subscribed to partition(s): topic2-0
21/02/21 22:51:09.629 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer--737770960-1613919067918 topic2-0 0
21/02/21 22:51:09.629 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer--737770960-1613919067918-4, groupId=spark-executor-java-test-consumer--737770960-1613919067918] Seeking to offset 0 for partition topic2-0
21/02/21 22:51:09.630 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-java-test-consumer-1174870607-1613919067411-3, groupId=spark-executor-java-test-consumer-1174870607-1613919067411] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:09.634 Executor task launch worker for task 1 INFO Metadata: [Consumer clientId=consumer-spark-executor-java-test-consumer--737770960-1613919067918-4, groupId=spark-executor-java-test-consumer--737770960-1613919067918] Cluster ID: rFZckPfXS0eeMt7CvmzlrA
21/02/21 22:51:09.734 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 913 bytes result sent to driver
21/02/21 22:51:09.734 Executor task launch worker for task 1 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 913 bytes result sent to driver
21/02/21 22:51:09.743 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 671 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.745 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 653 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.747 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.754 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.912 s
21/02/21 22:51:09.757 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.758 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:09.760 streaming-job-executor-0 INFO DAGScheduler: Job 0 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.985940 s
21/02/21 22:51:09.765 JobScheduler INFO JobScheduler: Finished job streaming job 1613919068000 ms.0 from job set of time 1613919068000 ms
21/02/21 22:51:09.766 JobScheduler INFO JobScheduler: Total delay: 1.765 s for time 1613919068000 ms (execution: 1.093 s)
21/02/21 22:51:09.767 JobScheduler INFO JobScheduler: Starting job streaming job 1613919068200 ms.0 from job set of time 1613919068200 ms
21/02/21 22:51:09.772 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:09.779 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:09.780 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:09.781 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:09.782 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:09.782 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:09.782 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:09.783 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (UnionRDD[9] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:09.789 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:09.791 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:09.792 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:09.792 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.793 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (UnionRDD[9] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.793 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
21/02/21 22:51:09.794 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.795 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.795 Executor task launch worker for task 2 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
21/02/21 22:51:09.795 Executor task launch worker for task 3 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
21/02/21 22:51:09.803 Executor task launch worker for task 2 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:09.803 Executor task launch worker for task 3 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:09.803 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Seeking to LATEST offset of partition topic1-0
21/02/21 22:51:09.806 Executor task launch worker for task 2 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 794 bytes result sent to driver
21/02/21 22:51:09.808 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Resetting offset for partition topic1-0 to offset 3.
21/02/21 22:51:09.809 Executor task launch worker for task 3 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 794 bytes result sent to driver
21/02/21 22:51:09.812 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 18 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.812 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 18 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.812 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.813 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.027 s
21/02/21 22:51:09.813 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Seeking to LATEST offset of partition topic2-0
21/02/21 22:51:09.813 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.814 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:51:09.814 streaming-job-executor-0 INFO DAGScheduler: Job 1 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.032855 s
21/02/21 22:51:09.815 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Resetting offset for partition topic2-0 to offset 3.
21/02/21 22:51:09.815 JobScheduler INFO JobScheduler: Finished job streaming job 1613919068200 ms.0 from job set of time 1613919068200 ms
21/02/21 22:51:09.816 JobScheduler INFO JobScheduler: Total delay: 1.615 s for time 1613919068200 ms (execution: 0.049 s)
21/02/21 22:51:09.816 JobScheduler INFO JobScheduler: Starting job streaming job 1613919068400 ms.0 from job set of time 1613919068400 ms
21/02/21 22:51:09.819 JobGenerator INFO JobScheduler: Added jobs for time 1613919069800 ms
21/02/21 22:51:09.820 JobGenerator INFO UnionRDD: Removing RDD 4 from persistence list
21/02/21 22:51:09.827 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:09.830 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:09.830 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:09.830 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:09.830 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:09.831 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (UnionRDD[14] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:09.832 block-manager-slave-async-thread-pool-0 INFO BlockManager: Removing RDD 4
21/02/21 22:51:09.834 main INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:09.835 main INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:09.837 JobGenerator INFO MapPartitionsRDD: Removing RDD 1 from persistence list
21/02/21 22:51:09.837 main INFO RecurringTimer: Stopped timer for JobGenerator after time 1613919069800
21/02/21 22:51:09.838 JobGenerator INFO KafkaRDD: Removing RDD 0 from persistence list
21/02/21 22:51:09.838 block-manager-slave-async-thread-pool-2 INFO BlockManager: Removing RDD 1
21/02/21 22:51:09.841 JobGenerator INFO KafkaRDD: Removing RDD 0 from persistence list
21/02/21 22:51:09.841 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:09.844 JobGenerator INFO MapPartitionsRDD: Removing RDD 3 from persistence list
21/02/21 22:51:09.845 JobGenerator INFO KafkaRDD: Removing RDD 2 from persistence list
21/02/21 22:51:09.845 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:09.846 JobGenerator INFO KafkaRDD: Removing RDD 2 from persistence list
21/02/21 22:51:09.846 block-manager-slave-async-thread-pool-5 INFO BlockManager: Removing RDD 0
21/02/21 22:51:09.846 block-manager-slave-async-thread-pool-7 INFO BlockManager: Removing RDD 0
21/02/21 22:51:09.846 block-manager-slave-async-thread-pool-8 INFO BlockManager: Removing RDD 3
21/02/21 22:51:09.846 block-manager-slave-async-thread-pool-9 INFO BlockManager: Removing RDD 2
21/02/21 22:51:09.847 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:09.847 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.847 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:09.847 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:09.848 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (UnionRDD[14] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.848 block-manager-slave-async-thread-pool-16 INFO BlockManager: Removing RDD 2
21/02/21 22:51:09.848 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
21/02/21 22:51:09.850 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.850 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.851 Executor task launch worker for task 5 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)
21/02/21 22:51:09.851 Executor task launch worker for task 4 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)
21/02/21 22:51:09.859 Executor task launch worker for task 4 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:09.859 Executor task launch worker for task 5 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:09.861 Executor task launch worker for task 4 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 751 bytes result sent to driver
21/02/21 22:51:09.861 Executor task launch worker for task 5 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 751 bytes result sent to driver
21/02/21 22:51:09.862 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 12 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.863 task-result-getter-0 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 13 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.863 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.864 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.027 s
21/02/21 22:51:09.865 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.865 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/02/21 22:51:09.865 streaming-job-executor-0 INFO DAGScheduler: Job 2 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.037163 s
21/02/21 22:51:09.867 JobScheduler INFO JobScheduler: Finished job streaming job 1613919068400 ms.0 from job set of time 1613919068400 ms
21/02/21 22:51:09.867 JobScheduler INFO JobScheduler: Total delay: 1.466 s for time 1613919068400 ms (execution: 0.050 s)
21/02/21 22:51:09.867 JobScheduler INFO JobScheduler: Starting job streaming job 1613919068600 ms.0 from job set of time 1613919068600 ms
21/02/21 22:51:09.884 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:09.886 scala-execution-context-global-107 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Revoke previously assigned partitions topic2-0
21/02/21 22:51:09.886 scala-execution-context-global-107 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer--737770960-1613919067918-2, groupId=java-test-consumer--737770960-1613919067918] Member consumer-java-test-consumer--737770960-1613919067918-2-ac1ae761-85a6-44b6-ab59-fba9c1a61767 sending LeaveGroup request to coordinator 127.0.0.1:56564 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:09.886 scala-execution-context-global-106 INFO ConsumerCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Revoke previously assigned partitions topic1-0
21/02/21 22:51:09.886 scala-execution-context-global-106 INFO AbstractCoordinator: [Consumer clientId=consumer-java-test-consumer-1174870607-1613919067411-1, groupId=java-test-consumer-1174870607-1613919067411] Member consumer-java-test-consumer-1174870607-1613919067411-1-60a1b9c9-b0ca-4d16-ac7a-6789369a4bc4 sending LeaveGroup request to coordinator 127.0.0.1:56564 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:09.887 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:09.887 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:09.887 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:09.888 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:09.889 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (UnionRDD[19] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:09.894 data-plane-kafka-request-handler-6 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-java-test-consumer-1174870607-1613919067411-1-60a1b9c9-b0ca-4d16-ac7a-6789369a4bc4] in group java-test-consumer-1174870607-1613919067411 has left, removing it from the group
21/02/21 22:51:09.894 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-java-test-consumer--737770960-1613919067918-2-ac1ae761-85a6-44b6-ab59-fba9c1a61767] in group java-test-consumer--737770960-1613919067918 has left, removing it from the group
21/02/21 22:51:09.896 data-plane-kafka-request-handler-6 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group java-test-consumer-1174870607-1613919067411 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-java-test-consumer-1174870607-1613919067411-1-60a1b9c9-b0ca-4d16-ac7a-6789369a4bc4 on LeaveGroup)
21/02/21 22:51:09.896 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group java-test-consumer--737770960-1613919067918 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-java-test-consumer--737770960-1613919067918-2-ac1ae761-85a6-44b6-ab59-fba9c1a61767 on LeaveGroup)
21/02/21 22:51:09.896 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:09.898 data-plane-kafka-request-handler-6 INFO GroupCoordinator: [GroupCoordinator 0]: Group java-test-consumer-1174870607-1613919067411 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:09.898 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Group java-test-consumer--737770960-1613919067918 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:09.900 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:09.901 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:09.902 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.903 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (UnionRDD[19] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.903 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
21/02/21 22:51:09.905 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.905 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.906 Executor task launch worker for task 7 INFO Executor: Running task 1.0 in stage 3.0 (TID 7)
21/02/21 22:51:09.906 Executor task launch worker for task 6 INFO Executor: Running task 0.0 in stage 3.0 (TID 6)
21/02/21 22:51:09.913 Executor task launch worker for task 6 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:09.913 JobGenerator INFO UnionRDD: Removing RDD 9 from persistence list
21/02/21 22:51:09.915 main INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:09.915 Executor task launch worker for task 7 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:09.915 Executor task launch worker for task 6 INFO Executor: Finished task 0.0 in stage 3.0 (TID 6). 794 bytes result sent to driver
21/02/21 22:51:09.916 block-manager-slave-async-thread-pool-21 INFO BlockManager: Removing RDD 9
21/02/21 22:51:09.916 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 11 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.916 Executor task launch worker for task 7 INFO Executor: Finished task 1.0 in stage 3.0 (TID 7). 794 bytes result sent to driver
21/02/21 22:51:09.917 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 12 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.917 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.918 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.025 s
21/02/21 22:51:09.918 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.918 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/21 22:51:09.919 streaming-job-executor-0 INFO DAGScheduler: Job 3 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.033630 s
21/02/21 22:51:09.920 JobScheduler INFO JobScheduler: Finished job streaming job 1613919068600 ms.0 from job set of time 1613919068600 ms
21/02/21 22:51:09.920 JobScheduler INFO JobScheduler: Total delay: 1.320 s for time 1613919068600 ms (execution: 0.053 s)
21/02/21 22:51:09.921 JobScheduler INFO JobScheduler: Starting job streaming job 1613919068800 ms.0 from job set of time 1613919068800 ms
21/02/21 22:51:09.932 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:09.933 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:09.934 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:09.934 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:09.934 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:09.935 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (UnionRDD[24] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:09.940 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:09.943 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:09.944 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:09.945 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.946 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (UnionRDD[24] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.946 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
21/02/21 22:51:09.947 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.948 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.948 Executor task launch worker for task 8 INFO Executor: Running task 0.0 in stage 4.0 (TID 8)
21/02/21 22:51:09.948 Executor task launch worker for task 9 INFO Executor: Running task 1.0 in stage 4.0 (TID 9)
21/02/21 22:51:09.953 Executor task launch worker for task 9 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:09.953 Executor task launch worker for task 8 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:09.954 Executor task launch worker for task 9 INFO Executor: Finished task 1.0 in stage 4.0 (TID 9). 751 bytes result sent to driver
21/02/21 22:51:09.954 Executor task launch worker for task 8 INFO Executor: Finished task 0.0 in stage 4.0 (TID 8). 751 bytes result sent to driver
21/02/21 22:51:09.955 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 7 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.956 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 9 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.956 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.956 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.019 s
21/02/21 22:51:09.957 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.957 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/02/21 22:51:09.957 streaming-job-executor-0 INFO DAGScheduler: Job 4 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.024716 s
21/02/21 22:51:09.958 JobScheduler INFO JobScheduler: Finished job streaming job 1613919068800 ms.0 from job set of time 1613919068800 ms
21/02/21 22:51:09.958 JobScheduler INFO JobScheduler: Total delay: 1.158 s for time 1613919068800 ms (execution: 0.038 s)
21/02/21 22:51:09.959 JobScheduler INFO JobScheduler: Starting job streaming job 1613919069000 ms.0 from job set of time 1613919069000 ms
21/02/21 22:51:09.968 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:09.969 dag-scheduler-event-loop INFO DAGScheduler: Got job 5 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:09.969 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 5 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:09.969 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:09.969 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:09.970 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 5 (UnionRDD[29] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:09.975 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:09.978 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:09.980 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:09.980 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:09.981 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (UnionRDD[29] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:09.981 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
21/02/21 22:51:09.983 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.983 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:09.984 Executor task launch worker for task 10 INFO Executor: Running task 0.0 in stage 5.0 (TID 10)
21/02/21 22:51:09.984 Executor task launch worker for task 11 INFO Executor: Running task 1.0 in stage 5.0 (TID 11)
21/02/21 22:51:09.987 Executor task launch worker for task 11 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:09.987 Executor task launch worker for task 10 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:09.989 Executor task launch worker for task 11 INFO Executor: Finished task 1.0 in stage 5.0 (TID 11). 751 bytes result sent to driver
21/02/21 22:51:09.989 Executor task launch worker for task 10 INFO Executor: Finished task 0.0 in stage 5.0 (TID 10). 751 bytes result sent to driver
21/02/21 22:51:09.990 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 11) in 6 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:09.990 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 10) in 8 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:09.990 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/02/21 22:51:09.992 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 5 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.019 s
21/02/21 22:51:09.992 dag-scheduler-event-loop INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:09.992 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/02/21 22:51:09.992 streaming-job-executor-0 INFO DAGScheduler: Job 5 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.024589 s
21/02/21 22:51:09.993 JobScheduler INFO JobScheduler: Finished job streaming job 1613919069000 ms.0 from job set of time 1613919069000 ms
21/02/21 22:51:09.994 JobScheduler INFO JobScheduler: Total delay: 0.993 s for time 1613919069000 ms (execution: 0.035 s)
21/02/21 22:51:09.994 JobScheduler INFO JobScheduler: Starting job streaming job 1613919069200 ms.0 from job set of time 1613919069200 ms
21/02/21 22:51:10.003 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:10.004 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:10.004 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 6 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:10.004 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:10.004 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:10.005 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 6 (UnionRDD[34] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:10.009 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:10.012 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:10.015 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:10.015 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:10.016 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (UnionRDD[34] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:10.016 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
21/02/21 22:51:10.017 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.017 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.018 Executor task launch worker for task 13 INFO Executor: Running task 1.0 in stage 6.0 (TID 13)
21/02/21 22:51:10.018 Executor task launch worker for task 12 INFO Executor: Running task 0.0 in stage 6.0 (TID 12)
21/02/21 22:51:10.021 Executor task launch worker for task 13 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:10.021 Executor task launch worker for task 12 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:10.022 Executor task launch worker for task 13 INFO Executor: Finished task 1.0 in stage 6.0 (TID 13). 751 bytes result sent to driver
21/02/21 22:51:10.022 Executor task launch worker for task 12 INFO Executor: Finished task 0.0 in stage 6.0 (TID 12). 751 bytes result sent to driver
21/02/21 22:51:10.023 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 6 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:10.024 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 7 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:10.024 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/02/21 22:51:10.024 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 6 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.017 s
21/02/21 22:51:10.025 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:10.025 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/02/21 22:51:10.025 streaming-job-executor-0 INFO DAGScheduler: Job 6 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.021897 s
21/02/21 22:51:10.026 JobScheduler INFO JobScheduler: Finished job streaming job 1613919069200 ms.0 from job set of time 1613919069200 ms
21/02/21 22:51:10.026 JobScheduler INFO JobScheduler: Total delay: 0.826 s for time 1613919069200 ms (execution: 0.032 s)
21/02/21 22:51:10.026 JobScheduler INFO JobScheduler: Starting job streaming job 1613919069400 ms.0 from job set of time 1613919069400 ms
21/02/21 22:51:10.036 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:10.037 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:10.037 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:10.037 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:10.037 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:10.039 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (UnionRDD[39] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:10.043 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:10.045 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:10.046 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:10.047 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:10.048 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (UnionRDD[39] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:10.048 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
21/02/21 22:51:10.050 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.051 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 15, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.052 Executor task launch worker for task 15 INFO Executor: Running task 1.0 in stage 7.0 (TID 15)
21/02/21 22:51:10.052 Executor task launch worker for task 14 INFO Executor: Running task 0.0 in stage 7.0 (TID 14)
21/02/21 22:51:10.055 Executor task launch worker for task 14 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:10.056 Executor task launch worker for task 15 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:10.057 Executor task launch worker for task 14 INFO Executor: Finished task 0.0 in stage 7.0 (TID 14). 794 bytes result sent to driver
21/02/21 22:51:10.057 Executor task launch worker for task 15 INFO Executor: Finished task 1.0 in stage 7.0 (TID 15). 751 bytes result sent to driver
21/02/21 22:51:10.058 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 9 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:10.059 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 15) in 9 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:10.059 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/02/21 22:51:10.059 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.019 s
21/02/21 22:51:10.060 dag-scheduler-event-loop INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:10.060 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/02/21 22:51:10.060 streaming-job-executor-0 INFO DAGScheduler: Job 7 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.024371 s
21/02/21 22:51:10.061 JobScheduler INFO JobScheduler: Finished job streaming job 1613919069400 ms.0 from job set of time 1613919069400 ms
21/02/21 22:51:10.061 JobScheduler INFO JobScheduler: Total delay: 0.661 s for time 1613919069400 ms (execution: 0.035 s)
21/02/21 22:51:10.061 JobScheduler INFO JobScheduler: Starting job streaming job 1613919069600 ms.0 from job set of time 1613919069600 ms
21/02/21 22:51:10.068 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:10.069 dag-scheduler-event-loop INFO DAGScheduler: Got job 8 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:10.069 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 8 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:10.069 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:10.070 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:10.071 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 8 (UnionRDD[44] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:10.075 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:10.078 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:10.082 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:10.082 dag-scheduler-event-loop INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:10.083 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (UnionRDD[44] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:10.083 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
21/02/21 22:51:10.084 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 16, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.085 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 17, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.085 Executor task launch worker for task 16 INFO Executor: Running task 0.0 in stage 8.0 (TID 16)
21/02/21 22:51:10.085 Executor task launch worker for task 17 INFO Executor: Running task 1.0 in stage 8.0 (TID 17)
21/02/21 22:51:10.090 Executor task launch worker for task 17 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:10.090 Executor task launch worker for task 16 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:10.092 Executor task launch worker for task 17 INFO Executor: Finished task 1.0 in stage 8.0 (TID 17). 751 bytes result sent to driver
21/02/21 22:51:10.092 Executor task launch worker for task 16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 16). 751 bytes result sent to driver
21/02/21 22:51:10.093 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 17) in 9 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:10.093 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 16) in 9 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:10.093 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/02/21 22:51:10.094 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 8 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.021 s
21/02/21 22:51:10.095 dag-scheduler-event-loop INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:10.095 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
21/02/21 22:51:10.095 streaming-job-executor-0 INFO DAGScheduler: Job 8 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.025982 s
21/02/21 22:51:10.096 JobScheduler INFO JobScheduler: Finished job streaming job 1613919069600 ms.0 from job set of time 1613919069600 ms
21/02/21 22:51:10.096 JobScheduler INFO JobScheduler: Total delay: 0.496 s for time 1613919069600 ms (execution: 0.035 s)
21/02/21 22:51:10.096 JobScheduler INFO JobScheduler: Starting job streaming job 1613919069800 ms.0 from job set of time 1613919069800 ms
21/02/21 22:51:10.107 streaming-job-executor-0 INFO SparkContext: Starting job: collect at JavaDirectKafkaStreamSuite.java:159
21/02/21 22:51:10.108 dag-scheduler-event-loop INFO DAGScheduler: Got job 9 (collect at JavaDirectKafkaStreamSuite.java:159) with 2 output partitions
21/02/21 22:51:10.108 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 9 (collect at JavaDirectKafkaStreamSuite.java:159)
21/02/21 22:51:10.108 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:10.109 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:10.110 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 9 (UnionRDD[49] at testKafkaStream at NativeMethodAccessorImpl.java:0), which has no missing parents
21/02/21 22:51:10.113 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.4 KiB, free 2.1 GiB)
21/02/21 22:51:10.116 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:10.117 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on DESKTOP-JPLSL4N:56613 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:10.118 dag-scheduler-event-loop INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:10.118 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (UnionRDD[49] at testKafkaStream at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:10.118 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks
21/02/21 22:51:10.120 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 18, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.120 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 19, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7335 bytes)
21/02/21 22:51:10.120 Executor task launch worker for task 18 INFO Executor: Running task 0.0 in stage 9.0 (TID 18)
21/02/21 22:51:10.120 Executor task launch worker for task 19 INFO Executor: Running task 1.0 in stage 9.0 (TID 19)
21/02/21 22:51:10.124 Executor task launch worker for task 18 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic1 0
21/02/21 22:51:10.124 Executor task launch worker for task 19 INFO KafkaRDD: Beginning offset 3 is the same as ending offset skipping topic2 0
21/02/21 22:51:10.125 Executor task launch worker for task 18 INFO Executor: Finished task 0.0 in stage 9.0 (TID 18). 751 bytes result sent to driver
21/02/21 22:51:10.125 Executor task launch worker for task 19 INFO Executor: Finished task 1.0 in stage 9.0 (TID 19). 751 bytes result sent to driver
21/02/21 22:51:10.127 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 18) in 8 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:10.128 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 19) in 8 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:10.128 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
21/02/21 22:51:10.129 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 9 (collect at JavaDirectKafkaStreamSuite.java:159) finished in 0.017 s
21/02/21 22:51:10.129 dag-scheduler-event-loop INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:10.129 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
21/02/21 22:51:10.129 streaming-job-executor-0 INFO DAGScheduler: Job 9 finished: collect at JavaDirectKafkaStreamSuite.java:159, took 0.022258 s
21/02/21 22:51:10.131 JobScheduler INFO JobScheduler: Finished job streaming job 1613919069800 ms.0 from job set of time 1613919069800 ms
21/02/21 22:51:10.131 JobScheduler INFO JobScheduler: Total delay: 0.330 s for time 1613919069800 ms (execution: 0.034 s)
21/02/21 22:51:10.135 main INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:10.139 main INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:10.174 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:10.239 main INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:10.240 main INFO BlockManager: BlockManager stopped
21/02/21 22:51:10.245 main INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:10.249 dispatcher-event-loop-2 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:10.255 main INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:10.255 main WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:10.256 main INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:10.256 main INFO KafkaServer: [KafkaServer id=0] shutting down
21/02/21 22:51:10.256 main INFO KafkaServer: [KafkaServer id=0] Starting controlled shutdown
21/02/21 22:51:10.266 controller-event-thread INFO KafkaController: [Controller id=0] Shutting down broker 0
21/02/21 22:51:10.276 main INFO KafkaServer: [KafkaServer id=0] Controlled shutdown succeeded
21/02/21 22:51:10.280 main INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutting down
21/02/21 22:51:10.280 main INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutdown completed
21/02/21 22:51:10.280 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Stopped
21/02/21 22:51:10.281 main INFO SocketServer: [SocketServer brokerId=0] Stopping socket server request processors
21/02/21 22:51:10.292 main INFO SocketServer: [SocketServer brokerId=0] Stopped socket server request processors
21/02/21 22:51:10.293 main INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shutting down
21/02/21 22:51:10.296 main INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shut down completely
21/02/21 22:51:10.300 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutting down
21/02/21 22:51:10.368 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Stopped
21/02/21 22:51:10.368 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutdown completed
21/02/21 22:51:10.369 main INFO KafkaApis: [KafkaApi-0] Shutdown complete.
21/02/21 22:51:10.370 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutting down
21/02/21 22:51:10.566 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Stopped
21/02/21 22:51:10.566 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutdown completed
21/02/21 22:51:10.574 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutting down.
21/02/21 22:51:10.575 main INFO ProducerIdManager: [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
21/02/21 22:51:10.577 main INFO TransactionStateManager: [Transaction State Manager 0]: Shutdown complete
21/02/21 22:51:10.577 main INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutting down
21/02/21 22:51:10.580 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Stopped
21/02/21 22:51:10.581 main INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutdown completed
21/02/21 22:51:10.583 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutdown complete.
21/02/21 22:51:10.585 main INFO GroupCoordinator: [GroupCoordinator 0]: Shutting down.
21/02/21 22:51:10.585 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutting down
21/02/21 22:51:10.766 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Stopped
21/02/21 22:51:10.766 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutdown completed
21/02/21 22:51:10.766 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutting down
21/02/21 22:51:10.846 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Stopped
21/02/21 22:51:10.846 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutdown completed
21/02/21 22:51:10.847 main INFO GroupCoordinator: [GroupCoordinator 0]: Shutdown complete.
21/02/21 22:51:10.849 main INFO ReplicaManager: [ReplicaManager broker=0] Shutting down
21/02/21 22:51:10.850 main INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutting down
21/02/21 22:51:10.850 main INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutdown completed
21/02/21 22:51:10.850 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Stopped
21/02/21 22:51:10.851 main INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutting down
21/02/21 22:51:10.855 main INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutdown completed
21/02/21 22:51:10.855 main INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutting down
21/02/21 22:51:10.856 main INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutdown completed
21/02/21 22:51:10.856 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutting down
21/02/21 22:51:10.970 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutdown completed
21/02/21 22:51:10.970 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Stopped
21/02/21 22:51:10.970 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutting down
21/02/21 22:51:11.167 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Stopped
21/02/21 22:51:11.167 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutdown completed
21/02/21 22:51:11.168 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutting down
21/02/21 22:51:11.171 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Stopped
21/02/21 22:51:11.171 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutdown completed
21/02/21 22:51:11.171 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutting down
21/02/21 22:51:11.367 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutdown completed
21/02/21 22:51:11.367 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Stopped
21/02/21 22:51:11.379 main INFO ReplicaManager: [ReplicaManager broker=0] Shut down completely
21/02/21 22:51:11.380 main INFO LogManager: Shutting down.
21/02/21 22:51:11.383 main INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:51:11.383 main INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:51:11.383 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:51:11.383 main INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:51:11.390 pool-29-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topic1-0] Writing producer snapshot at offset 3
21/02/21 22:51:11.428 pool-29-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 6
21/02/21 22:51:11.444 pool-29-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topic2-0] Writing producer snapshot at offset 3
21/02/21 22:51:11.503 main INFO LogManager: Shutdown complete.
21/02/21 22:51:11.504 main INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutting down
21/02/21 22:51:11.505 main INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutdown completed
21/02/21 22:51:11.505 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Stopped
21/02/21 22:51:11.511 main INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Stopped partition state machine
21/02/21 22:51:11.514 main INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Stopped replica state machine
21/02/21 22:51:11.516 main INFO RequestSendThread: [RequestSendThread controllerId=0] Shutting down
21/02/21 22:51:11.516 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Stopped
21/02/21 22:51:11.517 main INFO RequestSendThread: [RequestSendThread controllerId=0] Shutdown completed
21/02/21 22:51:11.521 main INFO KafkaController: [Controller id=0] Resigned
21/02/21 22:51:11.522 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closing.
21/02/21 22:51:11.524 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c5126d690001
21/02/21 22:51:11.527 main INFO ZooKeeper: Session: 0x177c5126d690001 closed
21/02/21 22:51:11.528 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56561 which had sessionid 0x177c5126d690001
21/02/21 22:51:11.528 main-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c5126d690001
21/02/21 22:51:11.530 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closed.
21/02/21 22:51:11.531 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutting down
21/02/21 22:51:12.285 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Stopped
21/02/21 22:51:12.286 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutdown completed
21/02/21 22:51:12.286 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutting down
21/02/21 22:51:12.289 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Stopped
21/02/21 22:51:12.289 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutdown completed
21/02/21 22:51:12.289 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutting down
21/02/21 22:51:12.291 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Stopped
21/02/21 22:51:12.291 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutdown completed
21/02/21 22:51:12.295 main INFO SocketServer: [SocketServer brokerId=0] Shutting down socket server
21/02/21 22:51:12.367 main INFO SocketServer: [SocketServer brokerId=0] Shutdown completed
21/02/21 22:51:12.372 main INFO KafkaServer: [KafkaServer id=0] shut down completed
21/02/21 22:51:12.399 main INFO ZooKeeperClient: [ZooKeeperClient] Closing.
21/02/21 22:51:12.399 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c5126d690000
21/02/21 22:51:12.403 main INFO ZooKeeper: Session: 0x177c5126d690000 closed
21/02/21 22:51:12.403 main-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c5126d690000
21/02/21 22:51:12.403 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56558 which had sessionid 0x177c5126d690000
21/02/21 22:51:12.403 main INFO ZooKeeperClient: [ZooKeeperClient] Closed.
21/02/21 22:51:12.404 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method
21/02/21 22:51:12.405 main INFO ZooKeeperServer: shutting down
21/02/21 22:51:12.405 main INFO SessionTrackerImpl: Shutting down
21/02/21 22:51:12.405 main INFO PrepRequestProcessor: Shutting down
21/02/21 22:51:12.405 main INFO SyncRequestProcessor: Shutting down
21/02/21 22:51:12.405 ProcessThread(sid:0 cport:56555): INFO PrepRequestProcessor: PrepRequestProcessor exited loop!
21/02/21 22:51:12.405 SyncThread:0 INFO SyncRequestProcessor: SyncRequestProcessor exited!
21/02/21 22:51:12.405 main INFO FinalRequestProcessor: shutdown of request processor complete
21/02/21 22:51:12.411 main WARN KafkaTestUtils: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f5d29dcc-d21e-4fc3-8733-1f0c0bc651f2\version-2\log.1
21/02/21 22:51:12.426 main INFO ZooKeeperServer: Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-a00a9497-4cc7-4729-85de-43bc897b35b7\version-2 snapdir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-736de8fc-fe0a-45be-a735-fcb76d284b6a\version-2
21/02/21 22:51:12.427 main INFO NIOServerCnxnFactory: binding to port /127.0.0.1:0
21/02/21 22:51:12.430 main INFO ZooKeeperClient: [ZooKeeperClient] Initializing a new session to 127.0.0.1:56645.
21/02/21 22:51:12.430 main INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56645 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7d17ee50
21/02/21 22:51:12.432 main INFO ZooKeeperClient: [ZooKeeperClient] Waiting until connected.
21/02/21 22:51:12.432 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56645. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:12.433 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56645, initiating session
21/02/21 22:51:12.433 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56648
21/02/21 22:51:12.433 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56648
21/02/21 22:51:12.433 SyncThread:0 INFO FileTxnLog: Creating new log file: log.1
21/02/21 22:51:12.437 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c512b0ad0000 with negotiated timeout 10000 for client /127.0.0.1:56648
21/02/21 22:51:12.437 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56645, sessionid = 0x177c512b0ad0000, negotiated timeout = 10000
21/02/21 22:51:12.438 main INFO ZooKeeperClient: [ZooKeeperClient] Connected.
21/02/21 22:51:12.449 main INFO KafkaServer: starting
21/02/21 22:51:12.449 main INFO KafkaServer: Connecting to zookeeper on 127.0.0.1:56645
21/02/21 22:51:12.450 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:56645.
21/02/21 22:51:12.450 main INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56645 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@44fdce3c
21/02/21 22:51:12.451 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Waiting until connected.
21/02/21 22:51:12.451 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56645. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:12.452 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56645, initiating session
21/02/21 22:51:12.452 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56651
21/02/21 22:51:12.452 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56651
21/02/21 22:51:12.454 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c512b0ad0001 with negotiated timeout 6000 for client /127.0.0.1:56651
21/02/21 22:51:12.455 main-SendThread(127.0.0.1:56645) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56645, sessionid = 0x177c512b0ad0001, negotiated timeout = 6000
21/02/21 22:51:12.455 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Connected.
21/02/21 22:51:12.459 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0001 type:create cxid:0x2 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
21/02/21 22:51:12.469 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0001 type:create cxid:0x6 zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
21/02/21 22:51:12.477 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0001 type:create cxid:0x9 zxid:0xb txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
21/02/21 22:51:12.500 SessionTracker INFO SessionTrackerImpl: SessionTrackerImpl exited loop!
21/02/21 22:51:12.503 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0001 type:create cxid:0x15 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
21/02/21 22:51:12.511 main INFO KafkaServer: Cluster ID = ZLz4mnh0RoSByRox4M1WYA
21/02/21 22:51:12.511 main WARN BrokerMetadataCheckpoint: No meta.properties file under dir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f\meta.properties
21/02/21 22:51:12.515 main INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56645
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:12.520 main INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56645
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:12.523 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Starting
21/02/21 22:51:12.524 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Starting
21/02/21 22:51:12.524 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Starting
21/02/21 22:51:12.528 main INFO LogManager: Loading logs.
21/02/21 22:51:12.530 main INFO LogManager: Logs loading complete in 2 ms.
21/02/21 22:51:12.532 main INFO LogManager: Starting log cleanup with a period of 300000 ms.
21/02/21 22:51:12.532 main INFO LogManager: Starting log flusher with a default period of 9223372036854775807 ms.
21/02/21 22:51:12.533 main INFO LogCleaner: Starting the log cleaner
21/02/21 22:51:12.612 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:51:12.654 main INFO Acceptor: Awaiting socket connections on 127.0.0.1:56654.
21/02/21 22:51:12.662 main INFO SocketServer: [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(127.0.0.1,0,ListenerName(PLAINTEXT),PLAINTEXT)
21/02/21 22:51:12.663 main INFO SocketServer: [SocketServer brokerId=0] Started 1 acceptor threads for data-plane
21/02/21 22:51:12.664 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Starting
21/02/21 22:51:12.665 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Starting
21/02/21 22:51:12.665 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Starting
21/02/21 22:51:12.666 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Starting
21/02/21 22:51:12.669 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Starting
21/02/21 22:51:12.671 main INFO KafkaZkClient: Creating /brokers/ids/0 (is it secure? false)
21/02/21 22:51:12.677 main INFO KafkaZkClient: Stat of the created znode at /brokers/ids/0 is: 25,25,1613919072672,1613919072672,1,0,0,105769800330706945,190,0,25

21/02/21 22:51:12.677 main INFO KafkaZkClient: Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(127.0.0.1,56654,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25
21/02/21 22:51:12.686 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Starting
21/02/21 22:51:12.687 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Starting
21/02/21 22:51:12.688 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Starting
21/02/21 22:51:12.689 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Starting
21/02/21 22:51:12.690 main INFO GroupCoordinator: [GroupCoordinator 0]: Starting up.
21/02/21 22:51:12.691 main INFO GroupCoordinator: [GroupCoordinator 0]: Startup complete.
21/02/21 22:51:12.691 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds.
21/02/21 22:51:12.692 controller-event-thread INFO KafkaZkClient: Successfully created /controller_epoch with initial epoch 0
21/02/21 22:51:12.696 controller-event-thread INFO KafkaController: [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
21/02/21 22:51:12.696 main INFO ProducerIdManager: [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
21/02/21 22:51:12.696 controller-event-thread INFO KafkaController: [Controller id=0] Registering handlers
21/02/21 22:51:12.697 controller-event-thread INFO KafkaController: [Controller id=0] Deleting log dir event notifications
21/02/21 22:51:12.698 controller-event-thread INFO KafkaController: [Controller id=0] Deleting isr change notifications
21/02/21 22:51:12.698 controller-event-thread INFO KafkaController: [Controller id=0] Initializing controller context
21/02/21 22:51:12.699 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Starting up.
21/02/21 22:51:12.700 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Startup complete.
21/02/21 22:51:12.701 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Starting
21/02/21 22:51:12.702 controller-event-thread INFO KafkaController: [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
21/02/21 22:51:12.703 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Starting
21/02/21 22:51:12.705 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Starting
21/02/21 22:51:12.708 controller-event-thread INFO KafkaController: [Controller id=0] Currently active brokers in the cluster: Set(0)
21/02/21 22:51:12.708 controller-event-thread INFO KafkaController: [Controller id=0] Currently shutting brokers in the cluster: Set()
21/02/21 22:51:12.708 controller-event-thread INFO KafkaController: [Controller id=0] Current list of topics in the cluster: Set()
21/02/21 22:51:12.708 controller-event-thread INFO KafkaController: [Controller id=0] Fetching topic deletions in progress
21/02/21 22:51:12.709 main INFO SocketServer: [SocketServer brokerId=0] Started data-plane processors for 1 acceptors
21/02/21 22:51:12.709 controller-event-thread INFO KafkaController: [Controller id=0] List of topics to be deleted: 
21/02/21 22:51:12.709 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:12.709 controller-event-thread INFO KafkaController: [Controller id=0] List of topics ineligible for deletion: 
21/02/21 22:51:12.709 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:12.710 controller-event-thread INFO KafkaController: [Controller id=0] Initializing topic deletion manager
21/02/21 22:51:12.710 main INFO AppInfoParser: Kafka startTimeMs: 1613919072709
21/02/21 22:51:12.710 controller-event-thread INFO TopicDeletionManager: [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
21/02/21 22:51:12.710 controller-event-thread INFO KafkaController: [Controller id=0] Sending update metadata request
21/02/21 22:51:12.710 main INFO KafkaServer: [KafkaServer id=0] started
21/02/21 22:51:12.710 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Starting
21/02/21 22:51:12.710 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Initializing replica state
21/02/21 22:51:12.710 main INFO Utils: Successfully started service 'KafkaBroker' on port 56654.
21/02/21 22:51:12.710 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering online replica state changes
21/02/21 22:51:12.710 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
21/02/21 22:51:12.711 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Initializing partition state
21/02/21 22:51:12.711 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Triggering online partition state changes
21/02/21 22:51:12.711 controller-event-thread INFO KafkaController: [Controller id=0] Ready to serve as the new controller with epoch 1
21/02/21 22:51:12.711 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Controller 0 connected to 127.0.0.1:56654 (id: 0 rack: null) for sending state change requests
21/02/21 22:51:12.712 main INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:12.713 controller-event-thread INFO KafkaController: [Controller id=0] Partitions undergoing preferred replica election: 
21/02/21 22:51:12.713 controller-event-thread INFO KafkaController: [Controller id=0] Partitions that completed preferred replica election: 
21/02/21 22:51:12.713 controller-event-thread INFO KafkaController: [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
21/02/21 22:51:12.713 controller-event-thread INFO KafkaController: [Controller id=0] Resuming preferred replica election for partitions: 
21/02/21 22:51:12.713 controller-event-thread INFO KafkaController: [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
21/02/21 22:51:12.713 main INFO ResourceUtils: ==============================================================
21/02/21 22:51:12.714 main INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:12.714 main INFO ResourceUtils: ==============================================================
21/02/21 22:51:12.714 main INFO SparkContext: Submitted application: JavaKafkaRDDSuite
21/02/21 22:51:12.715 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0001 type:multi cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
21/02/21 22:51:12.715 main INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:12.715 main INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:12.715 main INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:12.715 main INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:12.715 main INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:12.716 controller-event-thread INFO KafkaController: [Controller id=0] Starting the controller scheduler
21/02/21 22:51:12.775 main INFO Utils: Successfully started service 'sparkDriver' on port 56684.
21/02/21 22:51:12.778 main INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:12.779 main INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:12.780 main INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:12.780 main INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:12.780 main INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:12.783 main INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-09dfbd5d-eba5-40d2-a772-5d98eb3bdebd
21/02/21 22:51:12.783 main INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:12.785 main INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:12.834 main INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:12.846 main INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56703.
21/02/21 22:51:12.846 main INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:56703
21/02/21 22:51:12.846 main INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:12.846 main INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 56703, None)
21/02/21 22:51:12.848 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:56703 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 56703, None)
21/02/21 22:51:12.848 main INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 56703, None)
21/02/21 22:51:12.848 main INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 56703, None)
21/02/21 22:51:12.863 main INFO AdminZkClient: Creating topic topic1 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:12.864 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0000 type:setData cxid:0x4 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topic1 Error:KeeperErrorCode = NoNode for /config/topics/topic1
21/02/21 22:51:12.875 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topic1)], deleted topics: [Set()], new partition replica assignment [Map(topic1-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:12.876 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topic1-0
21/02/21 22:51:12.887 data-plane-kafka-request-handler-1 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic1-0)
21/02/21 22:51:12.895 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:12.898 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:12.899 data-plane-kafka-request-handler-1 INFO LogManager: Created log for partition topic1-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f\topic1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:12.899 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic1-0 broker=0] No checkpointed highwatermark is found for partition topic1-0
21/02/21 22:51:12.899 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic1-0 broker=0] Log loaded for partition topic1-0 with initial high watermark 0
21/02/21 22:51:12.899 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic1-0 broker=0] topic1-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:12.976 main INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:56654]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:12.982 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:12.983 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:12.983 main INFO AppInfoParser: Kafka startTimeMs: 1613919072982
21/02/21 22:51:12.992 kafka-producer-network-thread | producer-3 INFO Metadata: [Producer clientId=producer-3] Cluster ID: ZLz4mnh0RoSByRox4M1WYA
21/02/21 22:51:12.995 main INFO KafkaProducer: [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:13.020 main INFO AdminZkClient: Creating topic topic2 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:13.020 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512b0ad0000 type:setData cxid:0xb zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/topic2 Error:KeeperErrorCode = NoNode for /config/topics/topic2
21/02/21 22:51:13.032 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topic2)], deleted topics: [Set()], new partition replica assignment [Map(topic2-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:13.032 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topic2-0
21/02/21 22:51:13.047 data-plane-kafka-request-handler-1 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic2-0)
21/02/21 22:51:13.059 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:13.064 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms
21/02/21 22:51:13.066 data-plane-kafka-request-handler-1 INFO LogManager: Created log for partition topic2-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-6472cf62-669a-4a9d-8cd5-66e25a9b643f\topic2-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:13.066 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic2-0 broker=0] No checkpointed highwatermark is found for partition topic2-0
21/02/21 22:51:13.067 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic2-0 broker=0] Log loaded for partition topic2-0 with initial high watermark 0
21/02/21 22:51:13.067 data-plane-kafka-request-handler-1 INFO Partition: [Partition topic2-0 broker=0] topic2-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:13.132 main INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:56654]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:13.136 main INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:13.137 main INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:13.137 main INFO AppInfoParser: Kafka startTimeMs: 1613919073136
21/02/21 22:51:13.145 kafka-producer-network-thread | producer-4 INFO Metadata: [Producer clientId=producer-4] Cluster ID: ZLz4mnh0RoSByRox4M1WYA
21/02/21 22:51:13.147 main INFO KafkaProducer: [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:13.166 main WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:13.166 main WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:13.166 main WARN KafkaUtils: overriding executor group.id to spark-executor-java-test-consumer-1040324098-1613919073164
21/02/21 22:51:13.166 main WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:13.174 main WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:13.174 main WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:13.174 main WARN KafkaUtils: overriding executor group.id to spark-executor-java-test-consumer-1040324098-1613919073164
21/02/21 22:51:13.174 main WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:13.190 main INFO SparkContext: Starting job: count at JavaKafkaRDDSuite.java:115
21/02/21 22:51:13.192 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (count at JavaKafkaRDDSuite.java:115) with 2 output partitions
21/02/21 22:51:13.192 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (count at JavaKafkaRDDSuite.java:115)
21/02/21 22:51:13.192 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:13.192 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:13.193 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at JavaKafkaRDDSuite.java:105), which has no missing parents
21/02/21 22:51:13.210 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KiB, free 2.1 GiB)
21/02/21 22:51:13.214 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.7 KiB, free 2.1 GiB)
21/02/21 22:51:13.215 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:56703 (size: 2.7 KiB, free: 2.1 GiB)
21/02/21 22:51:13.216 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:13.217 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at JavaKafkaRDDSuite.java:105) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:13.217 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
21/02/21 22:51:13.219 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:13.219 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:13.221 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:13.222 Executor task launch worker for task 1 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
21/02/21 22:51:13.229 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic topic1, partition 0 offsets 0 -> 1
21/02/21 22:51:13.229 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic topic2, partition 0 offsets 0 -> 1
21/02/21 22:51:13.230 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:56654]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-java-test-consumer-1040324098-1613919073164
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919073235
21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-5, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Subscribed to partition(s): topic1-0
21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer-1040324098-1613919073164 topic1-0 0
21/02/21 22:51:13.235 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-5, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Seeking to offset 0 for partition topic1-0
21/02/21 22:51:13.236 Executor task launch worker for task 1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:56654]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-java-test-consumer-1040324098-1613919073164
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:13.243 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-5, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Cluster ID: ZLz4mnh0RoSByRox4M1WYA
21/02/21 22:51:13.243 Executor task launch worker for task 1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:13.245 Executor task launch worker for task 1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:13.245 Executor task launch worker for task 1 INFO AppInfoParser: Kafka startTimeMs: 1613919073243
21/02/21 22:51:13.245 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-6, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Subscribed to partition(s): topic2-0
21/02/21 22:51:13.245 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer-1040324098-1613919073164 topic2-0 0
21/02/21 22:51:13.245 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-6, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Seeking to offset 0 for partition topic2-0
21/02/21 22:51:13.253 Executor task launch worker for task 1 INFO Metadata: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-6, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Cluster ID: ZLz4mnh0RoSByRox4M1WYA
21/02/21 22:51:13.259 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 832 bytes result sent to driver
21/02/21 22:51:13.262 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 44 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:13.267 Executor task launch worker for task 1 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 875 bytes result sent to driver
21/02/21 22:51:13.268 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 49 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:13.269 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:13.270 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (count at JavaKafkaRDDSuite.java:115) finished in 0.067 s
21/02/21 22:51:13.270 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:13.271 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:13.271 main INFO DAGScheduler: Job 0 finished: count at JavaKafkaRDDSuite.java:115, took 0.080667 s
21/02/21 22:51:13.284 main INFO SparkContext: Starting job: count at JavaKafkaRDDSuite.java:116
21/02/21 22:51:13.286 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (count at JavaKafkaRDDSuite.java:116) with 2 output partitions
21/02/21 22:51:13.286 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (count at JavaKafkaRDDSuite.java:116)
21/02/21 22:51:13.286 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:13.287 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:13.287 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at map at JavaKafkaRDDSuite.java:112), which has no missing parents
21/02/21 22:51:13.292 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.4 KiB, free 2.1 GiB)
21/02/21 22:51:13.294 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 2.1 GiB)
21/02/21 22:51:13.295 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:56703 (size: 2.6 KiB, free: 2.1 GiB)
21/02/21 22:51:13.295 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:13.296 dag-scheduler-event-loop INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at JavaKafkaRDDSuite.java:112) (first 15 tasks are for partitions Vector(0, 1))
21/02/21 22:51:13.296 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
21/02/21 22:51:13.297 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:13.297 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:13.297 Executor task launch worker for task 2 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
21/02/21 22:51:13.297 Executor task launch worker for task 3 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
21/02/21 22:51:13.300 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic topic1, partition 0 offsets 0 -> 1
21/02/21 22:51:13.300 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic topic2, partition 0 offsets 0 -> 1
21/02/21 22:51:13.301 Executor task launch worker for task 2 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer-1040324098-1613919073164 topic1-0 0
21/02/21 22:51:13.301 Executor task launch worker for task 3 INFO InternalKafkaConsumer: Initial fetch for spark-executor-java-test-consumer-1040324098-1613919073164 topic2-0 0
21/02/21 22:51:13.301 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-5, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Seeking to offset 0 for partition topic1-0
21/02/21 22:51:13.301 Executor task launch worker for task 3 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-java-test-consumer-1040324098-1613919073164-6, groupId=spark-executor-java-test-consumer-1040324098-1613919073164] Seeking to offset 0 for partition topic2-0
21/02/21 22:51:13.829 Executor task launch worker for task 3 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 832 bytes result sent to driver
21/02/21 22:51:13.831 Executor task launch worker for task 2 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 832 bytes result sent to driver
21/02/21 22:51:13.831 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 534 ms on DESKTOP-JPLSL4N (executor driver) (1/2)
21/02/21 22:51:13.832 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 535 ms on DESKTOP-JPLSL4N (executor driver) (2/2)
21/02/21 22:51:13.832 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:51:13.833 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (count at JavaKafkaRDDSuite.java:116) finished in 0.543 s
21/02/21 22:51:13.833 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:13.833 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:51:13.834 main INFO DAGScheduler: Job 1 finished: count at JavaKafkaRDDSuite.java:116, took 0.549546 s
21/02/21 22:51:13.839 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:13.855 main INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:13.856 main INFO BlockManager: BlockManager stopped
21/02/21 22:51:13.857 main INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:13.857 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:13.864 main INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:13.864 main INFO KafkaServer: [KafkaServer id=0] shutting down
21/02/21 22:51:13.864 main INFO KafkaServer: [KafkaServer id=0] Starting controlled shutdown
21/02/21 22:51:13.869 controller-event-thread INFO KafkaController: [Controller id=0] Shutting down broker 0
21/02/21 22:51:13.871 main INFO KafkaServer: [KafkaServer id=0] Controlled shutdown succeeded
21/02/21 22:51:13.872 main INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutting down
21/02/21 22:51:13.872 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Stopped
21/02/21 22:51:13.872 main INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutdown completed
21/02/21 22:51:13.872 main INFO SocketServer: [SocketServer brokerId=0] Stopping socket server request processors
21/02/21 22:51:13.879 main INFO SocketServer: [SocketServer brokerId=0] Stopped socket server request processors
21/02/21 22:51:13.879 main INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shutting down
21/02/21 22:51:13.880 main INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shut down completely
21/02/21 22:51:13.883 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutting down
21/02/21 22:51:13.906 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Stopped
21/02/21 22:51:13.906 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutdown completed
21/02/21 22:51:13.906 main INFO KafkaApis: [KafkaApi-0] Shutdown complete.
21/02/21 22:51:13.906 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutting down
21/02/21 22:51:14.092 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Stopped
21/02/21 22:51:14.092 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutdown completed
21/02/21 22:51:14.092 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutting down.
21/02/21 22:51:14.092 main INFO ProducerIdManager: [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
21/02/21 22:51:14.093 main INFO TransactionStateManager: [Transaction State Manager 0]: Shutdown complete
21/02/21 22:51:14.093 main INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutting down
21/02/21 22:51:14.093 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Stopped
21/02/21 22:51:14.093 main INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutdown completed
21/02/21 22:51:14.093 main INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutdown complete.
21/02/21 22:51:14.093 main INFO GroupCoordinator: [GroupCoordinator 0]: Shutting down.
21/02/21 22:51:14.094 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutting down
21/02/21 22:51:14.293 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Stopped
21/02/21 22:51:14.293 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutdown completed
21/02/21 22:51:14.293 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutting down
21/02/21 22:51:14.294 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Stopped
21/02/21 22:51:14.294 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutdown completed
21/02/21 22:51:14.294 main INFO GroupCoordinator: [GroupCoordinator 0]: Shutdown complete.
21/02/21 22:51:14.294 main INFO ReplicaManager: [ReplicaManager broker=0] Shutting down
21/02/21 22:51:14.294 main INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutting down
21/02/21 22:51:14.294 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Stopped
21/02/21 22:51:14.294 main INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutdown completed
21/02/21 22:51:14.294 main INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutting down
21/02/21 22:51:14.295 main INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutdown completed
21/02/21 22:51:14.295 main INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutting down
21/02/21 22:51:14.295 main INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutdown completed
21/02/21 22:51:14.295 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutting down
21/02/21 22:51:14.421 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Stopped
21/02/21 22:51:14.422 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutdown completed
21/02/21 22:51:14.422 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutting down
21/02/21 22:51:14.469 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Stopped
21/02/21 22:51:14.469 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutdown completed
21/02/21 22:51:14.469 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutting down
21/02/21 22:51:14.472 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Stopped
21/02/21 22:51:14.472 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutdown completed
21/02/21 22:51:14.473 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutting down
21/02/21 22:51:14.671 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Stopped
21/02/21 22:51:14.671 main INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutdown completed
21/02/21 22:51:14.680 main INFO ReplicaManager: [ReplicaManager broker=0] Shut down completely
21/02/21 22:51:14.680 main INFO LogManager: Shutting down.
21/02/21 22:51:14.680 main INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:51:14.680 main INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:51:14.680 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:51:14.680 main INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:51:14.682 pool-57-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topic1-0] Writing producer snapshot at offset 3
21/02/21 22:51:14.696 pool-57-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topic2-0] Writing producer snapshot at offset 3
21/02/21 22:51:14.729 main INFO LogManager: Shutdown complete.
21/02/21 22:51:14.729 main INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutting down
21/02/21 22:51:14.729 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Stopped
21/02/21 22:51:14.729 main INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutdown completed
21/02/21 22:51:14.730 main INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Stopped partition state machine
21/02/21 22:51:14.731 main INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Stopped replica state machine
21/02/21 22:51:14.731 main INFO RequestSendThread: [RequestSendThread controllerId=0] Shutting down
21/02/21 22:51:14.731 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Stopped
21/02/21 22:51:14.731 main INFO RequestSendThread: [RequestSendThread controllerId=0] Shutdown completed
21/02/21 22:51:14.734 main INFO KafkaController: [Controller id=0] Resigned
21/02/21 22:51:14.734 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closing.
21/02/21 22:51:14.735 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c512b0ad0001
21/02/21 22:51:14.737 main INFO ZooKeeper: Session: 0x177c512b0ad0001 closed
21/02/21 22:51:14.737 main-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c512b0ad0001
21/02/21 22:51:14.738 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56651 which had sessionid 0x177c512b0ad0001
21/02/21 22:51:14.739 main INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closed.
21/02/21 22:51:14.739 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutting down
21/02/21 22:51:15.527 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Stopped
21/02/21 22:51:15.527 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutdown completed
21/02/21 22:51:15.527 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutting down
21/02/21 22:51:16.528 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Stopped
21/02/21 22:51:16.528 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutdown completed
21/02/21 22:51:16.528 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutting down
21/02/21 22:51:17.528 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Stopped
21/02/21 22:51:17.528 main INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutdown completed
21/02/21 22:51:17.528 main INFO SocketServer: [SocketServer brokerId=0] Shutting down socket server
21/02/21 22:51:17.570 main INFO SocketServer: [SocketServer brokerId=0] Shutdown completed
21/02/21 22:51:17.573 main INFO KafkaServer: [KafkaServer id=0] shut down completed
21/02/21 22:51:17.614 main INFO ZooKeeperClient: [ZooKeeperClient] Closing.
21/02/21 22:51:17.614 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c512b0ad0000
21/02/21 22:51:17.620 main INFO ZooKeeper: Session: 0x177c512b0ad0000 closed
21/02/21 22:51:17.620 main-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c512b0ad0000
21/02/21 22:51:17.620 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56648 which had sessionid 0x177c512b0ad0000
21/02/21 22:51:17.620 main INFO ZooKeeperClient: [ZooKeeperClient] Closed.
21/02/21 22:51:17.621 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method
21/02/21 22:51:17.622 main INFO ZooKeeperServer: shutting down
21/02/21 22:51:17.622 main INFO SessionTrackerImpl: Shutting down
21/02/21 22:51:17.622 main INFO PrepRequestProcessor: Shutting down
21/02/21 22:51:17.622 main INFO SyncRequestProcessor: Shutting down
21/02/21 22:51:17.622 ProcessThread(sid:0 cport:56645): INFO PrepRequestProcessor: PrepRequestProcessor exited loop!
21/02/21 22:51:17.622 SyncThread:0 INFO SyncRequestProcessor: SyncRequestProcessor exited!
21/02/21 22:51:17.622 main INFO FinalRequestProcessor: shutdown of request processor complete
21/02/21 22:51:17.631 main WARN KafkaTestUtils: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-a00a9497-4cc7-4729-85de-43bc897b35b7\version-2\log.1
21/02/21 22:51:17.697 Thread-1 INFO ShutdownHookManager: Shutdown hook called
21/02/21 22:51:17.698 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-e3a8509c-8a3e-4096-80fb-a3c0c186b655
21/02/21 22:51:17.701 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-a00a9497-4cc7-4729-85de-43bc897b35b7
21/02/21 22:51:17.706 Thread-1 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-a00a9497-4cc7-4729-85de-43bc897b35b7
java.io.IOException: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-a00a9497-4cc7-4729-85de-43bc897b35b7\version-2\log.1
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
21/02/21 22:51:17.711 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f5d29dcc-d21e-4fc3-8733-1f0c0bc651f2
21/02/21 22:51:17.717 Thread-1 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f5d29dcc-d21e-4fc3-8733-1f0c0bc651f2
java.io.IOException: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f5d29dcc-d21e-4fc3-8733-1f0c0bc651f2\version-2\log.1
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
21/02/21 22:51:31.520 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:zookeeper.version=3.4.7-1713338, built on 11/09/2015 04:32 GMT
21/02/21 22:51:31.521 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:host.name=DESKTOP-JPLSL4N
21/02/21 22:51:31.522 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.version=1.8.0_221
21/02/21 22:51:31.522 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.vendor=Oracle Corporation
21/02/21 22:51:31.522 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.home=C:\Progra~1\Java\jdk1.8.0_221\jre
21/02/21 22:51:31.522 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.class.path=C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\test-classes;C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\classes;C:\Users\User\sparkJanuary\external\kafka-0-10-token-provider\target\spark-token-provider-kafka-0-10_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-library\2.12.10\scala-library-2.12.10.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\User\.m2\repository\org\apache\avro\avro\1.8.2\avro-1.8.2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-compress\1.8.1\commons-compress-1.8.1.jar;C:\Users\User\.m2\repository\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-mapred\1.8.2\avro-mapred-1.8.2-hadoop2.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-ipc\1.8.2\avro-ipc-1.8.2.jar;C:\Users\User\.m2\repository\commons-codec\commons-codec\1.10\commons-codec-1.10.jar;C:\Users\User\.m2\repository\com\twitter\chill_2.12\0.9.5\chill_2.12-0.9.5.jar;C:\Users\User\.m2\repository\com\esotericsoftware\kryo-shaded\4.0.2\kryo-shaded-4.0.2.jar;C:\Users\User\.m2\repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;C:\Users\User\.m2\repository\com\twitter\chill-java\0.9.5\chill-java-0.9.5.jar;C:\Users\User\.m2\repository\org\apache\xbean\xbean-asm7-shaded\4.15\xbean-asm7-shaded-4.15.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-client\2.7.4\hadoop-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-common\2.7.4\hadoop-common-2.7.4.jar;C:\Users\User\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\User\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\User\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\User\.m2\repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;C:\Users\User\.m2\repository\javax\servlet\jsp\jsp-api\2.1\jsp-api-2.1.jar;C:\Users\User\.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\User\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\User\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\User\.m2\repository\commons-beanutils\commons-beanutils\1.9.4\commons-beanutils-1.9.4.jar;C:\Users\User\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\User\.m2\repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-auth\2.7.4\hadoop-auth-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpclient\4.5.6\httpclient-4.5.6.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpcore\4.4.12\httpcore-4.4.12.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-client\2.7.1\curator-client-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-hdfs\2.7.4\hadoop-hdfs-2.7.4.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\User\.m2\repository\xerces\xercesImpl\2.12.0\xercesImpl-2.12.0.jar;C:\Users\User\.m2\repository\xml-apis\xml-apis\1.4.01\xml-apis-1.4.01.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.7.4\hadoop-mapreduce-client-app-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.7.4\hadoop-mapreduce-client-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-client\2.7.4\hadoop-yarn-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-server-common\2.7.4\hadoop-yarn-server-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.7.4\hadoop-mapreduce-client-shuffle-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-api\2.7.4\hadoop-yarn-api-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.4\hadoop-mapreduce-client-core-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-common\2.7.4\hadoop-yarn-common-2.7.4.jar;C:\Users\User\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\User\.m2\repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.7.4\hadoop-mapreduce-client-jobclient-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-annotations\2.7.4\hadoop-annotations-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-launcher_2.12\3.0.1\spark-launcher_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-kvstore_2.12\3.0.1\spark-kvstore_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.10.0\jackson-core-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.10.0\jackson-annotations-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-common_2.12\3.0.1\spark-network-common_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-shuffle_2.12\3.0.1\spark-network-shuffle_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-unsafe_2.12\3.0.1\spark-unsafe_2.12-3.0.1.jar;C:\Users\User\.m2\repository\javax\activation\activation\1.1.1\activation-1.1.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-recipes\2.7.1\curator-recipes-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-framework\2.7.1\curator-framework-2.7.1.jar;C:\Users\User\.m2\repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;C:\Users\User\.m2\repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-lang3\3.9\commons-lang3-3.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-text\1.6\commons-text-1.6.jar;C:\Users\User\.m2\repository\com\google\code\findbugs\jsr305\3.0.0\jsr305-3.0.0.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-api\1.7.30\slf4j-api-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jul-to-slf4j\1.7.30\jul-to-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jcl-over-slf4j\1.7.30\jcl-over-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-log4j12\1.7.30\slf4j-log4j12-1.7.30.jar;C:\Users\User\.m2\repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\User\.m2\repository\org\xerial\snappy\snappy-java\1.1.7.5\snappy-java-1.1.7.5.jar;C:\Users\User\.m2\repository\org\lz4\lz4-java\1.7.1\lz4-java-1.7.1.jar;C:\Users\User\.m2\repository\com\github\luben\zstd-jni\1.4.4-3\zstd-jni-1.4.4-3.jar;C:\Users\User\.m2\repository\org\roaringbitmap\RoaringBitmap\0.7.45\RoaringBitmap-0.7.45.jar;C:\Users\User\.m2\repository\org\roaringbitmap\shims\0.7.45\shims-0.7.45.jar;C:\Users\User\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-xml_2.12\1.2.0\scala-xml_2.12-1.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-reflect\2.12.10\scala-reflect-2.12.10.jar;C:\Users\User\.m2\repository\org\json4s\json4s-jackson_2.12\3.6.6\json4s-jackson_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-core_2.12\3.6.6\json4s-core_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-ast_2.12\3.6.6\json4s-ast_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-scalap_2.12\3.6.6\json4s-scalap_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-client\2.30\jersey-client-2.30.jar;C:\Users\User\.m2\repository\jakarta\ws\rs\jakarta.ws.rs-api\2.1.6\jakarta.ws.rs-api-2.1.6.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\jakarta.inject\2.6.1\jakarta.inject-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-common\2.30\jersey-common-2.30.jar;C:\Users\User\.m2\repository\jakarta\annotation\jakarta.annotation-api\1.3.5\jakarta.annotation-api-1.3.5.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\osgi-resource-locator\1.0.3\osgi-resource-locator-1.0.3.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-server\2.30\jersey-server-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\media\jersey-media-jaxb\2.30\jersey-media-jaxb-2.30.jar;C:\Users\User\.m2\repository\jakarta\validation\jakarta.validation-api\2.0.2\jakarta.validation-api-2.0.2.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet\2.30\jersey-container-servlet-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.30\jersey-container-servlet-core-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\inject\jersey-hk2\2.30\jersey-hk2-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-locator\2.6.1\hk2-locator-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\aopalliance-repackaged\2.6.1\aopalliance-repackaged-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-api\2.6.1\hk2-api-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-utils\2.6.1\hk2-utils-2.6.1.jar;C:\Users\User\.m2\repository\org\javassist\javassist\3.25.0-GA\javassist-3.25.0-GA.jar;C:\Users\User\.m2\repository\io\netty\netty-all\4.1.47.Final\netty-all-4.1.47.Final.jar;C:\Users\User\.m2\repository\com\clearspring\analytics\stream\2.9.6\stream-2.9.6.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-core\4.1.1\metrics-core-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jvm\4.1.1\metrics-jvm-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-json\4.1.1\metrics-json-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-graphite\4.1.1\metrics-graphite-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jmx\4.1.1\metrics-jmx-4.1.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.10.0\jackson-databind-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.12\2.10.0\jackson-module-scala_2.12-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.10.0\jackson-module-paranamer-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\User\.m2\repository\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\User\.m2\repository\net\razorvine\pyrolite\4.30\pyrolite-4.30.jar;C:\Users\User\.m2\repository\net\sf\py4j\py4j\0.10.9\py4j-0.10.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka-clients\2.4.1\kafka-clients-2.4.1.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka_2.12\2.4.1\kafka_2.12-2.4.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.10.0\jackson-dataformat-csv-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.10.0\jackson-datatype-jdk8-2.10.0.jar;C:\Users\User\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.12\2.1.2\scala-collection-compat_2.12-2.1.2.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.12\0.9.0\scala-java8-compat_2.12-0.9.0.jar;C:\Users\User\.m2\repository\com\typesafe\scala-logging\scala-logging_2.12\3.9.2\scala-logging_2.12-3.9.2.jar;C:\Users\User\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\User\.m2\repository\org\apache\zookeeper\zookeeper\3.4.7\zookeeper-3.4.7.jar;C:\Users\User\.m2\repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;C:\Users\User\.m2\repository\org\scalacheck\scalacheck_2.12\1.14.2\scalacheck_2.12-1.14.2.jar;C:\Users\User\.m2\repository\org\scala-sbt\test-interface\1.0\test-interface-1.0.jar;C:\Users\User\.m2\repository\org\mockito\mockito-core\3.1.0\mockito-core-3.1.0.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy\1.9.10\byte-buddy-1.9.10.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy-agent\1.9.10\byte-buddy-agent-1.9.10.jar;C:\Users\User\.m2\repository\org\objenesis\objenesis\2.6\objenesis-2.6.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\jmock\jmock-junit4\2.8.4\jmock-junit4-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock\2.8.4\jmock-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock-testjar\2.8.4\jmock-testjar-2.8.4.jar;C:\Users\User\.m2\repository\cglib\cglib\3.2.0\cglib-3.2.0.jar;C:\Users\User\.m2\repository\org\apache\ant\ant\1.9.4\ant-1.9.4.jar;C:\Users\User\.m2\repository\org\apache\ant\ant-launcher\1.9.4\ant-launcher-1.9.4.jar;C:\Users\User\.m2\repository\org\ow2\asm\asm\5.0.4\asm-5.0.4.jar;C:\Users\User\.m2\repository\org\apache-extras\beanshell\bsh\2.0b6\bsh-2.0b6.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-library\1.3\hamcrest-library-1.3.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\User\.m2\repository\org\scalatest\scalatest_2.12\3.0.8\scalatest_2.12-3.0.8.jar;C:\Users\User\.m2\repository\org\scalactic\scalactic_2.12\3.0.8\scalactic_2.12-3.0.8.jar;C:\Users\User\.m2\repository\junit\junit\4.12\junit-4.12.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;C:\Users\User\.m2\repository\com\novocode\junit-interface\0.11\junit-interface-0.11.jar
21/02/21 22:51:31.523 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.library.path=C:\Progra~1\Java\jdk1.8.0_221\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\Programs\MiKTeX 2.9\miktex\bin\x64\;C:\Users\User\sparkJanuary\bin;C:\hadoop-3.1.2\sbin;C:\Progra~1\Java\jdk1.8.0_221;C:\hadoop-3.1.2;C:\hadoop-3.1.2\bin;C:\hadoop-3.1.2\lib\native;C:\Progra~1\apache-maven-3.6.3\bin;C:\Program Files\apache-maven-3.6.3\bin;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Rtools 4.0;C:\Users\User\Anaconda3\envs\kublasean\Lib\R\bin\x64\Rgui.exe;C:\Users\User\sparkJanuary\python;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;;;.
21/02/21 22:51:31.524 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.io.tmpdir=C:\Users\User\sparkJanuary\external\kafka-0-10\target/tmp
21/02/21 22:51:31.524 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:java.compiler=<NA>
21/02/21 22:51:31.524 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:os.name=Windows 10
21/02/21 22:51:31.524 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:os.arch=amd64
21/02/21 22:51:31.524 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:os.version=10.0
21/02/21 22:51:31.525 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:user.name=User
21/02/21 22:51:31.525 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:user.home=C:\Users\User
21/02/21 22:51:31.525 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Server environment:user.dir=C:\Users\User\sparkJanuary\external\kafka-0-10
21/02/21 22:51:31.564 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-fa748bea-6e08-420f-9dd0-681fe31ab7c1\version-2 snapdir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-465334ad-6dca-47fe-a80d-441bfd13037c\version-2
21/02/21 22:51:31.603 ScalaTest-main-running-DiscoverySuite INFO NIOServerCnxnFactory: binding to port /127.0.0.1:0
21/02/21 22:51:31.776 ScalaTest-main-running-DiscoverySuite INFO Log4jControllerRegistration$: Registered kafka:type=kafka.Log4jController MBean
21/02/21 22:51:31.807 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Initializing a new session to 127.0.0.1:56728.
21/02/21 22:51:31.813 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:zookeeper.version=3.4.7-1713338, built on 11/09/2015 04:32 GMT
21/02/21 22:51:31.818 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:host.name=DESKTOP-JPLSL4N
21/02/21 22:51:31.818 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.version=1.8.0_221
21/02/21 22:51:31.819 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
21/02/21 22:51:31.819 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.home=C:\Progra~1\Java\jdk1.8.0_221\jre
21/02/21 22:51:31.819 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.class.path=C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\test-classes;C:\Users\User\sparkJanuary\external\kafka-0-10\target\scala-2.12\classes;C:\Users\User\sparkJanuary\external\kafka-0-10-token-provider\target\spark-token-provider-kafka-0-10_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-library\2.12.10\scala-library-2.12.10.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\User\.m2\repository\org\apache\avro\avro\1.8.2\avro-1.8.2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-compress\1.8.1\commons-compress-1.8.1.jar;C:\Users\User\.m2\repository\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-mapred\1.8.2\avro-mapred-1.8.2-hadoop2.jar;C:\Users\User\.m2\repository\org\apache\avro\avro-ipc\1.8.2\avro-ipc-1.8.2.jar;C:\Users\User\.m2\repository\commons-codec\commons-codec\1.10\commons-codec-1.10.jar;C:\Users\User\.m2\repository\com\twitter\chill_2.12\0.9.5\chill_2.12-0.9.5.jar;C:\Users\User\.m2\repository\com\esotericsoftware\kryo-shaded\4.0.2\kryo-shaded-4.0.2.jar;C:\Users\User\.m2\repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;C:\Users\User\.m2\repository\com\twitter\chill-java\0.9.5\chill-java-0.9.5.jar;C:\Users\User\.m2\repository\org\apache\xbean\xbean-asm7-shaded\4.15\xbean-asm7-shaded-4.15.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-client\2.7.4\hadoop-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-common\2.7.4\hadoop-common-2.7.4.jar;C:\Users\User\.m2\repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;C:\Users\User\.m2\repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\User\.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\User\.m2\repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;C:\Users\User\.m2\repository\javax\servlet\jsp\jsp-api\2.1\jsp-api-2.1.jar;C:\Users\User\.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\User\.m2\repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\User\.m2\repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\User\.m2\repository\commons-beanutils\commons-beanutils\1.9.4\commons-beanutils-1.9.4.jar;C:\Users\User\.m2\repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\User\.m2\repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-auth\2.7.4\hadoop-auth-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpclient\4.5.6\httpclient-4.5.6.jar;C:\Users\User\.m2\repository\org\apache\httpcomponents\httpcore\4.4.12\httpcore-4.4.12.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-client\2.7.1\curator-client-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-hdfs\2.7.4\hadoop-hdfs-2.7.4.jar;C:\Users\User\.m2\repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\User\.m2\repository\xerces\xercesImpl\2.12.0\xercesImpl-2.12.0.jar;C:\Users\User\.m2\repository\xml-apis\xml-apis\1.4.01\xml-apis-1.4.01.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.7.4\hadoop-mapreduce-client-app-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.7.4\hadoop-mapreduce-client-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-client\2.7.4\hadoop-yarn-client-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-server-common\2.7.4\hadoop-yarn-server-common-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.7.4\hadoop-mapreduce-client-shuffle-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-api\2.7.4\hadoop-yarn-api-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.4\hadoop-mapreduce-client-core-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-yarn-common\2.7.4\hadoop-yarn-common-2.7.4.jar;C:\Users\User\.m2\repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\User\.m2\repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;C:\Users\User\.m2\repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.7.4\hadoop-mapreduce-client-jobclient-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\hadoop\hadoop-annotations\2.7.4\hadoop-annotations-2.7.4.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-launcher_2.12\3.0.1\spark-launcher_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-kvstore_2.12\3.0.1\spark-kvstore_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-core\2.10.0\jackson-core-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.10.0\jackson-annotations-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-common_2.12\3.0.1\spark-network-common_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-network-shuffle_2.12\3.0.1\spark-network-shuffle_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-unsafe_2.12\3.0.1\spark-unsafe_2.12-3.0.1.jar;C:\Users\User\.m2\repository\javax\activation\activation\1.1.1\activation-1.1.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-recipes\2.7.1\curator-recipes-2.7.1.jar;C:\Users\User\.m2\repository\org\apache\curator\curator-framework\2.7.1\curator-framework-2.7.1.jar;C:\Users\User\.m2\repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;C:\Users\User\.m2\repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-lang3\3.9\commons-lang3-3.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-text\1.6\commons-text-1.6.jar;C:\Users\User\.m2\repository\com\google\code\findbugs\jsr305\3.0.0\jsr305-3.0.0.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-api\1.7.30\slf4j-api-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jul-to-slf4j\1.7.30\jul-to-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\org\slf4j\jcl-over-slf4j\1.7.30\jcl-over-slf4j-1.7.30.jar;C:\Users\User\.m2\repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\User\.m2\repository\org\slf4j\slf4j-log4j12\1.7.30\slf4j-log4j12-1.7.30.jar;C:\Users\User\.m2\repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\User\.m2\repository\org\xerial\snappy\snappy-java\1.1.7.5\snappy-java-1.1.7.5.jar;C:\Users\User\.m2\repository\org\lz4\lz4-java\1.7.1\lz4-java-1.7.1.jar;C:\Users\User\.m2\repository\com\github\luben\zstd-jni\1.4.4-3\zstd-jni-1.4.4-3.jar;C:\Users\User\.m2\repository\org\roaringbitmap\RoaringBitmap\0.7.45\RoaringBitmap-0.7.45.jar;C:\Users\User\.m2\repository\org\roaringbitmap\shims\0.7.45\shims-0.7.45.jar;C:\Users\User\.m2\repository\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-xml_2.12\1.2.0\scala-xml_2.12-1.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\scala-reflect\2.12.10\scala-reflect-2.12.10.jar;C:\Users\User\.m2\repository\org\json4s\json4s-jackson_2.12\3.6.6\json4s-jackson_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-core_2.12\3.6.6\json4s-core_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-ast_2.12\3.6.6\json4s-ast_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\json4s\json4s-scalap_2.12\3.6.6\json4s-scalap_2.12-3.6.6.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-client\2.30\jersey-client-2.30.jar;C:\Users\User\.m2\repository\jakarta\ws\rs\jakarta.ws.rs-api\2.1.6\jakarta.ws.rs-api-2.1.6.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\jakarta.inject\2.6.1\jakarta.inject-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-common\2.30\jersey-common-2.30.jar;C:\Users\User\.m2\repository\jakarta\annotation\jakarta.annotation-api\1.3.5\jakarta.annotation-api-1.3.5.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\osgi-resource-locator\1.0.3\osgi-resource-locator-1.0.3.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\core\jersey-server\2.30\jersey-server-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\media\jersey-media-jaxb\2.30\jersey-media-jaxb-2.30.jar;C:\Users\User\.m2\repository\jakarta\validation\jakarta.validation-api\2.0.2\jakarta.validation-api-2.0.2.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet\2.30\jersey-container-servlet-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.30\jersey-container-servlet-core-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\jersey\inject\jersey-hk2\2.30\jersey-hk2-2.30.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-locator\2.6.1\hk2-locator-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\external\aopalliance-repackaged\2.6.1\aopalliance-repackaged-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-api\2.6.1\hk2-api-2.6.1.jar;C:\Users\User\.m2\repository\org\glassfish\hk2\hk2-utils\2.6.1\hk2-utils-2.6.1.jar;C:\Users\User\.m2\repository\org\javassist\javassist\3.25.0-GA\javassist-3.25.0-GA.jar;C:\Users\User\.m2\repository\io\netty\netty-all\4.1.47.Final\netty-all-4.1.47.Final.jar;C:\Users\User\.m2\repository\com\clearspring\analytics\stream\2.9.6\stream-2.9.6.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-core\4.1.1\metrics-core-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jvm\4.1.1\metrics-jvm-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-json\4.1.1\metrics-json-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-graphite\4.1.1\metrics-graphite-4.1.1.jar;C:\Users\User\.m2\repository\io\dropwizard\metrics\metrics-jmx\4.1.1\metrics-jmx-4.1.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.10.0\jackson-databind-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-scala_2.12\2.10.0\jackson-module-scala_2.12-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.10.0\jackson-module-paranamer-2.10.0.jar;C:\Users\User\.m2\repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\User\.m2\repository\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\User\.m2\repository\net\razorvine\pyrolite\4.30\pyrolite-4.30.jar;C:\Users\User\.m2\repository\net\sf\py4j\py4j\0.10.9\py4j-0.10.9.jar;C:\Users\User\.m2\repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka-clients\2.4.1\kafka-clients-2.4.1.jar;C:\Users\User\.m2\repository\org\apache\kafka\kafka_2.12\2.4.1\kafka_2.12-2.4.1.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-csv\2.10.0\jackson-dataformat-csv-2.10.0.jar;C:\Users\User\.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk8\2.10.0\jackson-datatype-jdk8-2.10.0.jar;C:\Users\User\.m2\repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-collection-compat_2.12\2.1.2\scala-collection-compat_2.12-2.1.2.jar;C:\Users\User\.m2\repository\org\scala-lang\modules\scala-java8-compat_2.12\0.9.0\scala-java8-compat_2.12-0.9.0.jar;C:\Users\User\.m2\repository\com\typesafe\scala-logging\scala-logging_2.12\3.9.2\scala-logging_2.12-3.9.2.jar;C:\Users\User\.m2\repository\commons-cli\commons-cli\1.4\commons-cli-1.4.jar;C:\Users\User\.m2\repository\org\apache\zookeeper\zookeeper\3.4.7\zookeeper-3.4.7.jar;C:\Users\User\.m2\repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;C:\Users\User\.m2\repository\org\scalacheck\scalacheck_2.12\1.14.2\scalacheck_2.12-1.14.2.jar;C:\Users\User\.m2\repository\org\scala-sbt\test-interface\1.0\test-interface-1.0.jar;C:\Users\User\.m2\repository\org\mockito\mockito-core\3.1.0\mockito-core-3.1.0.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy\1.9.10\byte-buddy-1.9.10.jar;C:\Users\User\.m2\repository\net\bytebuddy\byte-buddy-agent\1.9.10\byte-buddy-agent-1.9.10.jar;C:\Users\User\.m2\repository\org\objenesis\objenesis\2.6\objenesis-2.6.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1.jar;C:\Users\User\.m2\repository\org\jmock\jmock-junit4\2.8.4\jmock-junit4-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock\2.8.4\jmock-2.8.4.jar;C:\Users\User\.m2\repository\org\jmock\jmock-testjar\2.8.4\jmock-testjar-2.8.4.jar;C:\Users\User\.m2\repository\cglib\cglib\3.2.0\cglib-3.2.0.jar;C:\Users\User\.m2\repository\org\apache\ant\ant\1.9.4\ant-1.9.4.jar;C:\Users\User\.m2\repository\org\apache\ant\ant-launcher\1.9.4\ant-launcher-1.9.4.jar;C:\Users\User\.m2\repository\org\ow2\asm\asm\5.0.4\asm-5.0.4.jar;C:\Users\User\.m2\repository\org\apache-extras\beanshell\bsh\2.0b6\bsh-2.0b6.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-library\1.3\hamcrest-library-1.3.jar;C:\Users\User\.m2\repository\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1-tests.jar;C:\Users\User\.m2\repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\User\.m2\repository\org\scalatest\scalatest_2.12\3.0.8\scalatest_2.12-3.0.8.jar;C:\Users\User\.m2\repository\org\scalactic\scalactic_2.12\3.0.8\scalactic_2.12-3.0.8.jar;C:\Users\User\.m2\repository\junit\junit\4.12\junit-4.12.jar;C:\Users\User\.m2\repository\org\hamcrest\hamcrest-core\1.3\hamcrest-core-1.3.jar;C:\Users\User\.m2\repository\com\novocode\junit-interface\0.11\junit-interface-0.11.jar
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.library.path=C:\Progra~1\Java\jdk1.8.0_221\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\Programs\MiKTeX 2.9\miktex\bin\x64\;C:\Users\User\sparkJanuary\bin;C:\hadoop-3.1.2\sbin;C:\Progra~1\Java\jdk1.8.0_221;C:\hadoop-3.1.2;C:\hadoop-3.1.2\bin;C:\hadoop-3.1.2\lib\native;C:\Progra~1\apache-maven-3.6.3\bin;C:\Program Files\apache-maven-3.6.3\bin;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Rtools 4.0;C:\Users\User\Anaconda3\envs\kublasean\Lib\R\bin\x64\Rgui.exe;C:\Users\User\sparkJanuary\python;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files (x86)\sbt\bin;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Java\jdk1.8.0_221\bin;C:\hadoop-3.1.2\sbin;C:\hadoop-3.1.2\bin;C:\Progra~1\Java\jdk1.8.0_221;C:\Program Files\PuTTY\;C:\Program Files\apache-maven-3.6.3\bin;;;.
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.io.tmpdir=C:\Users\User\sparkJanuary\external\kafka-0-10\target/tmp
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:java.compiler=<NA>
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:os.name=Windows 10
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:os.arch=amd64
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:os.version=10.0
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:user.name=User
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:user.home=C:\Users\User
21/02/21 22:51:31.820 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Client environment:user.dir=C:\Users\User\sparkJanuary\external\kafka-0-10
21/02/21 22:51:31.822 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56728 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6722db6e
21/02/21 22:51:31.840 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Waiting until connected.
21/02/21 22:51:31.841 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56728. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:31.843 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56728, initiating session
21/02/21 22:51:31.843 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56731
21/02/21 22:51:31.851 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56731
21/02/21 22:51:31.856 SyncThread:0 INFO FileTxnLog: Creating new log file: log.1
21/02/21 22:51:31.873 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c512fba00000 with negotiated timeout 10000 for client /127.0.0.1:56731
21/02/21 22:51:31.874 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56728, sessionid = 0x177c512fba00000, negotiated timeout = 10000
21/02/21 22:51:31.878 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Connected.
21/02/21 22:51:32.106 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: starting
21/02/21 22:51:32.107 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Connecting to zookeeper on 127.0.0.1:56728
21/02/21 22:51:32.113 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:56728.
21/02/21 22:51:32.113 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:56728 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7e70bd39
21/02/21 22:51:32.115 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Waiting until connected.
21/02/21 22:51:32.116 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:56728. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:32.117 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:56728, initiating session
21/02/21 22:51:32.117 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:56734
21/02/21 22:51:32.118 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:56734
21/02/21 22:51:32.121 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c512fba00001 with negotiated timeout 6000 for client /127.0.0.1:56734
21/02/21 22:51:32.122 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:56728, sessionid = 0x177c512fba00001, negotiated timeout = 6000
21/02/21 22:51:32.122 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Connected.
21/02/21 22:51:32.231 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00001 type:create cxid:0x2 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
21/02/21 22:51:32.247 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00001 type:create cxid:0x6 zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
21/02/21 22:51:32.259 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00001 type:create cxid:0x9 zxid:0xb txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
21/02/21 22:51:32.672 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00001 type:create cxid:0x15 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
21/02/21 22:51:32.684 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Cluster ID = xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:32.690 ScalaTest-main-running-DiscoverySuite WARN BrokerMetadataCheckpoint: No meta.properties file under dir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4\meta.properties
21/02/21 22:51:32.743 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56728
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:32.763 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:56728
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:32.809 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Starting
21/02/21 22:51:32.809 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Starting
21/02/21 22:51:32.812 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Starting
21/02/21 22:51:32.867 ScalaTest-main-running-DiscoverySuite INFO LogManager: Loading logs.
21/02/21 22:51:32.879 ScalaTest-main-running-DiscoverySuite INFO LogManager: Logs loading complete in 12 ms.
21/02/21 22:51:32.902 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log cleanup with a period of 300000 ms.
21/02/21 22:51:32.906 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log flusher with a default period of 9223372036854775807 ms.
21/02/21 22:51:32.910 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Starting the log cleaner
21/02/21 22:51:33.001 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:51:33.630 ScalaTest-main-running-DiscoverySuite INFO Acceptor: Awaiting socket connections on 127.0.0.1:56738.
21/02/21 22:51:33.680 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(127.0.0.1,0,ListenerName(PLAINTEXT),PLAINTEXT)
21/02/21 22:51:33.681 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started 1 acceptor threads for data-plane
21/02/21 22:51:33.715 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Starting
21/02/21 22:51:33.716 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Starting
21/02/21 22:51:33.718 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Starting
21/02/21 22:51:33.719 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Starting
21/02/21 22:51:33.741 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Starting
21/02/21 22:51:33.772 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Creating /brokers/ids/0 (is it secure? false)
21/02/21 22:51:33.801 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Stat of the created znode at /brokers/ids/0 is: 25,25,1613919093791,1613919093791,1,0,0,105769801588146177,190,0,25

21/02/21 22:51:33.802 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(127.0.0.1,56738,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25
21/02/21 22:51:33.901 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Starting
21/02/21 22:51:33.910 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Starting
21/02/21 22:51:33.915 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Starting
21/02/21 22:51:33.915 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Starting
21/02/21 22:51:33.932 controller-event-thread INFO KafkaZkClient: Successfully created /controller_epoch with initial epoch 0
21/02/21 22:51:33.950 controller-event-thread INFO KafkaController: [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
21/02/21 22:51:33.950 controller-event-thread INFO KafkaController: [Controller id=0] Registering handlers
21/02/21 22:51:33.954 controller-event-thread INFO KafkaController: [Controller id=0] Deleting log dir event notifications
21/02/21 22:51:33.957 controller-event-thread INFO KafkaController: [Controller id=0] Deleting isr change notifications
21/02/21 22:51:33.960 controller-event-thread INFO KafkaController: [Controller id=0] Initializing controller context
21/02/21 22:51:33.965 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Starting up.
21/02/21 22:51:33.967 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Startup complete.
21/02/21 22:51:33.977 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 10 milliseconds.
21/02/21 22:51:33.988 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
21/02/21 22:51:34.021 controller-event-thread INFO KafkaController: [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
21/02/21 22:51:34.031 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Starting up.
21/02/21 22:51:34.033 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Startup complete.
21/02/21 22:51:34.033 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Starting
21/02/21 22:51:34.050 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Starting
21/02/21 22:51:34.050 controller-event-thread INFO KafkaController: [Controller id=0] Currently active brokers in the cluster: Set(0)
21/02/21 22:51:34.050 controller-event-thread INFO KafkaController: [Controller id=0] Currently shutting brokers in the cluster: Set()
21/02/21 22:51:34.051 controller-event-thread INFO KafkaController: [Controller id=0] Current list of topics in the cluster: Set()
21/02/21 22:51:34.052 controller-event-thread INFO KafkaController: [Controller id=0] Fetching topic deletions in progress
21/02/21 22:51:34.055 controller-event-thread INFO KafkaController: [Controller id=0] List of topics to be deleted: 
21/02/21 22:51:34.055 controller-event-thread INFO KafkaController: [Controller id=0] List of topics ineligible for deletion: 
21/02/21 22:51:34.056 controller-event-thread INFO KafkaController: [Controller id=0] Initializing topic deletion manager
21/02/21 22:51:34.056 controller-event-thread INFO TopicDeletionManager: [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
21/02/21 22:51:34.057 controller-event-thread INFO KafkaController: [Controller id=0] Sending update metadata request
21/02/21 22:51:34.072 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Initializing replica state
21/02/21 22:51:34.073 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering online replica state changes
21/02/21 22:51:34.081 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Starting
21/02/21 22:51:34.084 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
21/02/21 22:51:34.085 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Initializing partition state
21/02/21 22:51:34.086 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Triggering online partition state changes
21/02/21 22:51:34.090 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Controller 0 connected to 127.0.0.1:56738 (id: 0 rack: null) for sending state change requests
21/02/21 22:51:34.095 controller-event-thread INFO KafkaController: [Controller id=0] Ready to serve as the new controller with epoch 1
21/02/21 22:51:34.100 controller-event-thread INFO KafkaController: [Controller id=0] Partitions undergoing preferred replica election: 
21/02/21 22:51:34.101 controller-event-thread INFO KafkaController: [Controller id=0] Partitions that completed preferred replica election: 
21/02/21 22:51:34.101 controller-event-thread INFO KafkaController: [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
21/02/21 22:51:34.101 controller-event-thread INFO KafkaController: [Controller id=0] Resuming preferred replica election for partitions: 
21/02/21 22:51:34.103 controller-event-thread INFO KafkaController: [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
21/02/21 22:51:34.112 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00001 type:multi cxid:0x31 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
21/02/21 22:51:34.117 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Starting
21/02/21 22:51:34.121 controller-event-thread INFO KafkaController: [Controller id=0] Starting the controller scheduler
21/02/21 22:51:34.132 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started data-plane processors for 1 acceptors
21/02/21 22:51:34.138 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:34.138 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:34.139 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka startTimeMs: 1613919094133
21/02/21 22:51:34.141 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] started
21/02/21 22:51:34.142 ScalaTest-main-running-DiscoverySuite INFO Utils: Successfully started service 'KafkaBroker' on port 56738.
21/02/21 22:51:34.144 ScalaTest-main-running-DiscoverySuite INFO KafkaDataConsumer: Initializing cache 16 64 0.75
21/02/21 22:51:34.173 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'KafkaDataConsumer reuse in case of same groupId and TopicPartition' =====

21/02/21 22:51:34.735 ScalaTest-main-running-KafkaDataConsumerSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:34.790 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:34.790 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:34.790 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka startTimeMs: 1613919094790
21/02/21 22:51:34.791 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaConsumer: [Consumer clientId=consumer-groupId-1, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:34.801 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'KafkaDataConsumer reuse in case of same groupId and TopicPartition' =====

21/02/21 22:51:34.809 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'new KafkaDataConsumer instance in case of Task retry' =====

21/02/21 22:51:34.826 ScalaTest-main-running-KafkaDataConsumerSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:34.831 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:34.831 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:34.831 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka startTimeMs: 1613919094831
21/02/21 22:51:34.831 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaConsumer: [Consumer clientId=consumer-groupId-2, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:34.837 ScalaTest-main-running-KafkaDataConsumerSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:34.842 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:34.842 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:34.842 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka startTimeMs: 1613919094842
21/02/21 22:51:34.842 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaConsumer: [Consumer clientId=consumer-groupId-3, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:34.844 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'new KafkaDataConsumer instance in case of Task retry' =====

21/02/21 22:51:34.845 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'concurrent use of KafkaDataConsumer' =====

21/02/21 22:51:34.870 ScalaTest-main-running-KafkaDataConsumerSuite INFO AdminZkClient: Creating topic topic1286212732 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:34.872 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c512fba00000 type:setData cxid:0x4 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topic1286212732 Error:KeeperErrorCode = NoNode for /config/topics/topic1286212732
21/02/21 22:51:34.896 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topic1286212732)], deleted topics: [Set()], new partition replica assignment [Map(topic1286212732-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:34.896 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topic1286212732-0
21/02/21 22:51:34.966 data-plane-kafka-request-handler-4 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic1286212732-0)
21/02/21 22:51:35.057 data-plane-kafka-request-handler-4 INFO Log: [Log partition=topic1286212732-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:35.067 data-plane-kafka-request-handler-4 INFO Log: [Log partition=topic1286212732-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 67 ms
21/02/21 22:51:35.070 data-plane-kafka-request-handler-4 INFO LogManager: Created log for partition topic1286212732-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-1fb6f449-7deb-4e7d-9949-521bf2a623b4\topic1286212732-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:35.071 data-plane-kafka-request-handler-4 INFO Partition: [Partition topic1286212732-0 broker=0] No checkpointed highwatermark is found for partition topic1286212732-0
21/02/21 22:51:35.072 data-plane-kafka-request-handler-4 INFO Partition: [Partition topic1286212732-0 broker=0] Log loaded for partition topic1286212732-0 with initial high watermark 0
21/02/21 22:51:35.073 data-plane-kafka-request-handler-4 INFO Partition: [Partition topic1286212732-0 broker=0] topic1286212732-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:35.217 ScalaTest-main-running-KafkaDataConsumerSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:56738]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:35.237 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.237 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.237 ScalaTest-main-running-KafkaDataConsumerSuite INFO AppInfoParser: Kafka startTimeMs: 1613919095237
21/02/21 22:51:35.279 kafka-producer-network-thread | producer-1 INFO Metadata: [Producer clientId=producer-1] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.340 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaProducer: [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:35.478 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.488 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.488 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.488 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919095484
21/02/21 22:51:35.489 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-4, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.493 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.494 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.495 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-4, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.503 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.504 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.504 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919095503
21/02/21 22:51:35.504 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-5, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.505 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.505 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-5, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.510 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.514 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-4, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.516 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-5, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.518 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.519 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.519 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919095518
21/02/21 22:51:35.519 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-6, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.519 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.519 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-6, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.525 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.528 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-6, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.532 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.533 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.533 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919095532
21/02/21 22:51:35.533 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-7, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.533 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.534 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-7, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.535 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.541 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-7, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.542 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.543 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.543 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919095542
21/02/21 22:51:35.546 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-8, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.546 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.546 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-8, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.548 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.557 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.557 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.557 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919095557
21/02/21 22:51:35.557 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-9, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.558 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.558 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-9, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.559 pool-9-thread-69 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.561 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-8, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.565 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-9, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.569 pool-9-thread-69 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.569 pool-9-thread-69 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.569 pool-9-thread-69 INFO AppInfoParser: Kafka startTimeMs: 1613919095569
21/02/21 22:51:35.569 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-10, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.570 pool-9-thread-69 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.570 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-10, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.573 pool-9-thread-63 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.579 pool-9-thread-63 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.579 pool-9-thread-63 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.579 pool-9-thread-63 INFO AppInfoParser: Kafka startTimeMs: 1613919095579
21/02/21 22:51:35.581 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-11, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.582 pool-9-thread-63 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.582 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-11, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.583 pool-9-thread-69 INFO Metadata: [Consumer clientId=consumer-groupId-10, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.585 pool-9-thread-66 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.591 pool-9-thread-63 INFO Metadata: [Consumer clientId=consumer-groupId-11, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.600 pool-9-thread-66 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.601 pool-9-thread-66 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.601 pool-9-thread-66 INFO AppInfoParser: Kafka startTimeMs: 1613919095600
21/02/21 22:51:35.601 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-12, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.602 pool-9-thread-66 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.602 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-12, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.612 pool-9-thread-67 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.617 pool-9-thread-67 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.617 pool-9-thread-67 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.617 pool-9-thread-67 INFO AppInfoParser: Kafka startTimeMs: 1613919095617
21/02/21 22:51:35.618 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-13, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.618 pool-9-thread-67 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.618 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-13, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.619 pool-9-thread-74 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.625 pool-9-thread-74 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.625 pool-9-thread-74 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.625 pool-9-thread-74 INFO AppInfoParser: Kafka startTimeMs: 1613919095625
21/02/21 22:51:35.626 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-14, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.626 pool-9-thread-74 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.626 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-14, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.629 pool-9-thread-75 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.633 pool-9-thread-75 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.633 pool-9-thread-75 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.634 pool-9-thread-75 INFO AppInfoParser: Kafka startTimeMs: 1613919095633
21/02/21 22:51:35.634 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-15, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.634 pool-9-thread-75 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.634 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-15, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.635 pool-9-thread-78 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.645 pool-9-thread-78 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.645 pool-9-thread-78 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.645 pool-9-thread-78 INFO AppInfoParser: Kafka startTimeMs: 1613919095645
21/02/21 22:51:35.645 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-16, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.647 pool-9-thread-78 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.648 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-16, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.649 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.664 pool-9-thread-74 INFO Metadata: [Consumer clientId=consumer-groupId-14, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.666 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.667 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.667 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919095666
21/02/21 22:51:35.668 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-17, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.668 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.668 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-17, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.670 pool-9-thread-80 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.676 pool-9-thread-67 INFO Metadata: [Consumer clientId=consumer-groupId-13, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.683 pool-9-thread-75 INFO Metadata: [Consumer clientId=consumer-groupId-15, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.688 pool-9-thread-78 INFO Metadata: [Consumer clientId=consumer-groupId-16, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.711 pool-9-thread-66 INFO Metadata: [Consumer clientId=consumer-groupId-12, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.717 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-17, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.733 pool-9-thread-80 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.733 pool-9-thread-80 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.733 pool-9-thread-80 INFO AppInfoParser: Kafka startTimeMs: 1613919095733
21/02/21 22:51:35.735 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-18, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.736 pool-9-thread-80 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.736 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-18, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.739 pool-9-thread-60 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.749 pool-9-thread-80 INFO Metadata: [Consumer clientId=consumer-groupId-18, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.749 pool-9-thread-60 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.750 pool-9-thread-60 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.750 pool-9-thread-60 INFO AppInfoParser: Kafka startTimeMs: 1613919095749
21/02/21 22:51:35.750 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-19, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.750 pool-9-thread-60 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.750 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-19, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.752 pool-9-thread-57 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.756 pool-9-thread-57 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.756 pool-9-thread-60 INFO Metadata: [Consumer clientId=consumer-groupId-19, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.756 pool-9-thread-57 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.758 pool-9-thread-57 INFO AppInfoParser: Kafka startTimeMs: 1613919095756
21/02/21 22:51:35.758 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-20, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.758 pool-9-thread-57 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.759 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-20, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.763 pool-9-thread-56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.769 pool-9-thread-57 INFO Metadata: [Consumer clientId=consumer-groupId-20, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.771 pool-9-thread-56 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.772 pool-9-thread-56 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.772 pool-9-thread-56 INFO AppInfoParser: Kafka startTimeMs: 1613919095771
21/02/21 22:51:35.773 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-21, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.774 pool-9-thread-55 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.774 pool-9-thread-56 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.775 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-21, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.780 pool-9-thread-55 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.781 pool-9-thread-55 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.781 pool-9-thread-55 INFO AppInfoParser: Kafka startTimeMs: 1613919095780
21/02/21 22:51:35.781 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-22, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.781 pool-9-thread-56 INFO Metadata: [Consumer clientId=consumer-groupId-21, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.781 pool-9-thread-55 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.781 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-22, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.786 pool-9-thread-53 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.789 pool-9-thread-55 INFO Metadata: [Consumer clientId=consumer-groupId-22, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.792 pool-9-thread-53 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.793 pool-9-thread-53 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.794 pool-9-thread-53 INFO AppInfoParser: Kafka startTimeMs: 1613919095792
21/02/21 22:51:35.794 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-23, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.794 pool-9-thread-53 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.794 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-23, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.795 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.800 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.800 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.800 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919095800
21/02/21 22:51:35.800 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-24, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.801 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.801 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-24, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.803 pool-9-thread-53 INFO Metadata: [Consumer clientId=consumer-groupId-23, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.806 pool-9-thread-59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.810 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-24, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.814 pool-9-thread-59 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.814 pool-9-thread-59 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.814 pool-9-thread-59 INFO AppInfoParser: Kafka startTimeMs: 1613919095814
21/02/21 22:51:35.814 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-25, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.815 pool-9-thread-59 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.815 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-25, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.818 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.823 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.824 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.824 pool-9-thread-59 INFO Metadata: [Consumer clientId=consumer-groupId-25, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.824 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919095823
21/02/21 22:51:35.825 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-26, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.825 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.825 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-26, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.826 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.830 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.830 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.830 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919095830
21/02/21 22:51:35.831 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-27, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.831 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.831 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-27, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.833 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-26, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.834 pool-9-thread-51 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.841 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-27, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.845 pool-9-thread-51 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.846 pool-9-thread-51 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.846 pool-9-thread-51 INFO AppInfoParser: Kafka startTimeMs: 1613919095845
21/02/21 22:51:35.846 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-28, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.847 pool-9-thread-51 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.847 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-28, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.851 pool-9-thread-54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.854 pool-9-thread-51 INFO Metadata: [Consumer clientId=consumer-groupId-28, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.858 pool-9-thread-54 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.858 pool-9-thread-54 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.859 pool-9-thread-54 INFO AppInfoParser: Kafka startTimeMs: 1613919095858
21/02/21 22:51:35.859 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-29, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.859 pool-9-thread-54 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.859 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-29, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.862 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.866 pool-9-thread-54 INFO Metadata: [Consumer clientId=consumer-groupId-29, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.869 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.870 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.870 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919095869
21/02/21 22:51:35.870 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-30, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.871 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.871 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-30, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.873 pool-9-thread-39 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.878 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-30, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.883 pool-9-thread-39 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.884 pool-9-thread-39 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.884 pool-9-thread-39 INFO AppInfoParser: Kafka startTimeMs: 1613919095883
21/02/21 22:51:35.885 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-31, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.885 pool-9-thread-39 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.885 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-31, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.887 pool-9-thread-41 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.894 pool-9-thread-39 INFO Metadata: [Consumer clientId=consumer-groupId-31, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.894 pool-9-thread-41 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.895 pool-9-thread-41 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.895 pool-9-thread-41 INFO AppInfoParser: Kafka startTimeMs: 1613919095894
21/02/21 22:51:35.895 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-32, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.895 pool-9-thread-41 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.895 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-32, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.899 pool-9-thread-47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.903 pool-9-thread-47 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.903 pool-9-thread-47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.903 pool-9-thread-47 INFO AppInfoParser: Kafka startTimeMs: 1613919095903
21/02/21 22:51:35.903 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-33, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.904 pool-9-thread-41 INFO Metadata: [Consumer clientId=consumer-groupId-32, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.905 pool-9-thread-47 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.906 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-33, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.906 pool-9-thread-49 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.912 pool-9-thread-49 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.912 pool-9-thread-49 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.912 pool-9-thread-49 INFO AppInfoParser: Kafka startTimeMs: 1613919095912
21/02/21 22:51:35.913 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-34, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.913 pool-9-thread-49 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.913 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-34, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.914 pool-9-thread-47 INFO Metadata: [Consumer clientId=consumer-groupId-33, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.916 pool-9-thread-36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.921 pool-9-thread-36 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.921 pool-9-thread-36 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.921 pool-9-thread-36 INFO AppInfoParser: Kafka startTimeMs: 1613919095921
21/02/21 22:51:35.921 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-35, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.921 pool-9-thread-36 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.922 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-35, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.922 pool-9-thread-49 INFO Metadata: [Consumer clientId=consumer-groupId-34, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.925 pool-9-thread-46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.928 pool-9-thread-36 INFO Metadata: [Consumer clientId=consumer-groupId-35, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.939 pool-9-thread-46 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.940 pool-9-thread-46 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.940 pool-9-thread-46 INFO AppInfoParser: Kafka startTimeMs: 1613919095939
21/02/21 22:51:35.940 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-36, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.940 pool-9-thread-46 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.940 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-36, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.944 pool-9-thread-42 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.947 pool-9-thread-46 INFO Metadata: [Consumer clientId=consumer-groupId-36, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.951 pool-9-thread-42 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.951 pool-9-thread-42 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.952 pool-9-thread-42 INFO AppInfoParser: Kafka startTimeMs: 1613919095951
21/02/21 22:51:35.952 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-37, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.952 pool-9-thread-42 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.952 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-37, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.953 pool-9-thread-37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.958 pool-9-thread-37 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.958 pool-9-thread-37 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.958 pool-9-thread-37 INFO AppInfoParser: Kafka startTimeMs: 1613919095958
21/02/21 22:51:35.958 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-38, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.959 pool-9-thread-37 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.959 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-38, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.960 pool-9-thread-48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.961 pool-9-thread-42 INFO Metadata: [Consumer clientId=consumer-groupId-37, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.967 pool-9-thread-48 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.967 pool-9-thread-48 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.967 pool-9-thread-48 INFO AppInfoParser: Kafka startTimeMs: 1613919095967
21/02/21 22:51:35.967 pool-9-thread-37 INFO Metadata: [Consumer clientId=consumer-groupId-38, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.967 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-39, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.968 pool-9-thread-48 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.970 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-39, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.970 pool-9-thread-21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.976 pool-9-thread-48 INFO Metadata: [Consumer clientId=consumer-groupId-39, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.976 pool-9-thread-21 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.976 pool-9-thread-21 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.976 pool-9-thread-21 INFO AppInfoParser: Kafka startTimeMs: 1613919095976
21/02/21 22:51:35.977 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-40, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.977 pool-9-thread-21 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.977 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-40, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.981 pool-9-thread-45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.984 pool-9-thread-21 INFO Metadata: [Consumer clientId=consumer-groupId-40, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.988 pool-9-thread-45 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.988 pool-9-thread-45 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.988 pool-9-thread-45 INFO AppInfoParser: Kafka startTimeMs: 1613919095988
21/02/21 22:51:35.988 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-41, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.988 pool-9-thread-45 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.988 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-41, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:35.991 pool-9-thread-40 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:35.994 pool-9-thread-45 INFO Metadata: [Consumer clientId=consumer-groupId-41, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:35.997 pool-9-thread-40 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:35.998 pool-9-thread-40 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:35.998 pool-9-thread-40 INFO AppInfoParser: Kafka startTimeMs: 1613919095997
21/02/21 22:51:35.998 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-42, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:35.998 pool-9-thread-40 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:35.998 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-42, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.000 pool-9-thread-44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.004 pool-9-thread-40 INFO Metadata: [Consumer clientId=consumer-groupId-42, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.005 pool-9-thread-44 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.005 pool-9-thread-44 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.005 pool-9-thread-44 INFO AppInfoParser: Kafka startTimeMs: 1613919096005
21/02/21 22:51:36.005 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-43, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.005 pool-9-thread-44 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.007 pool-9-thread-38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.007 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-43, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.012 pool-9-thread-44 INFO Metadata: [Consumer clientId=consumer-groupId-43, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.013 pool-9-thread-38 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.013 pool-9-thread-38 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.013 pool-9-thread-38 INFO AppInfoParser: Kafka startTimeMs: 1613919096013
21/02/21 22:51:36.014 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-44, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.014 pool-9-thread-38 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.015 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-44, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.018 pool-9-thread-43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.020 pool-9-thread-38 INFO Metadata: [Consumer clientId=consumer-groupId-44, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.024 pool-9-thread-43 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.024 pool-9-thread-43 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.024 pool-9-thread-43 INFO AppInfoParser: Kafka startTimeMs: 1613919096024
21/02/21 22:51:36.024 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-45, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.025 pool-9-thread-43 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.025 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-45, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.027 pool-9-thread-35 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.034 pool-9-thread-43 INFO Metadata: [Consumer clientId=consumer-groupId-45, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.036 pool-9-thread-35 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.037 pool-9-thread-35 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.037 pool-9-thread-35 INFO AppInfoParser: Kafka startTimeMs: 1613919096036
21/02/21 22:51:36.037 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-46, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.037 pool-9-thread-35 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.037 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-46, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.038 pool-9-thread-34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.043 pool-9-thread-35 INFO Metadata: [Consumer clientId=consumer-groupId-46, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.043 pool-9-thread-34 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.043 pool-9-thread-34 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.043 pool-9-thread-34 INFO AppInfoParser: Kafka startTimeMs: 1613919096043
21/02/21 22:51:36.044 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-47, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.044 pool-9-thread-34 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.044 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-47, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.046 pool-9-thread-29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.049 pool-9-thread-34 INFO Metadata: [Consumer clientId=consumer-groupId-47, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.051 pool-9-thread-29 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.051 pool-9-thread-29 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.051 pool-9-thread-29 INFO AppInfoParser: Kafka startTimeMs: 1613919096051
21/02/21 22:51:36.051 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-48, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.051 pool-9-thread-29 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.051 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-48, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.056 pool-9-thread-24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.058 pool-9-thread-29 INFO Metadata: [Consumer clientId=consumer-groupId-48, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.062 pool-9-thread-24 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.062 pool-9-thread-24 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.062 pool-9-thread-24 INFO AppInfoParser: Kafka startTimeMs: 1613919096062
21/02/21 22:51:36.062 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-49, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.062 pool-9-thread-24 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.062 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-49, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.064 pool-9-thread-30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.090 pool-9-thread-30 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.091 pool-9-thread-24 INFO Metadata: [Consumer clientId=consumer-groupId-49, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.091 pool-9-thread-30 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.091 pool-9-thread-30 INFO AppInfoParser: Kafka startTimeMs: 1613919096090
21/02/21 22:51:36.091 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-50, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.091 pool-9-thread-30 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.091 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-50, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.094 pool-9-thread-25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.096 pool-9-thread-30 INFO Metadata: [Consumer clientId=consumer-groupId-50, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.097 pool-9-thread-25 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.097 pool-9-thread-25 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.097 pool-9-thread-25 INFO AppInfoParser: Kafka startTimeMs: 1613919096097
21/02/21 22:51:36.097 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-51, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.098 pool-9-thread-25 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.098 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-51, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.099 pool-9-thread-15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.102 pool-9-thread-15 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.102 pool-9-thread-15 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.102 pool-9-thread-15 INFO AppInfoParser: Kafka startTimeMs: 1613919096102
21/02/21 22:51:36.103 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-52, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.103 pool-9-thread-15 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.103 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-52, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.104 pool-9-thread-25 INFO Metadata: [Consumer clientId=consumer-groupId-51, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.106 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.109 pool-9-thread-15 INFO Metadata: [Consumer clientId=consumer-groupId-52, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.110 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.110 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.110 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919096110
21/02/21 22:51:36.111 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-53, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.111 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.111 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-53, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.113 pool-9-thread-26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.117 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-53, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.117 pool-9-thread-26 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.117 pool-9-thread-26 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.117 pool-9-thread-26 INFO AppInfoParser: Kafka startTimeMs: 1613919096117
21/02/21 22:51:36.120 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-54, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.120 pool-9-thread-26 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.120 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-54, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.121 pool-9-thread-19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.126 pool-9-thread-26 INFO Metadata: [Consumer clientId=consumer-groupId-54, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.127 pool-9-thread-19 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.127 pool-9-thread-19 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.129 pool-9-thread-19 INFO AppInfoParser: Kafka startTimeMs: 1613919096126
21/02/21 22:51:36.129 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-55, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.129 pool-9-thread-19 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.129 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-55, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.132 pool-9-thread-31 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.136 pool-9-thread-19 INFO Metadata: [Consumer clientId=consumer-groupId-55, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.137 pool-9-thread-31 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.137 pool-9-thread-31 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.137 pool-9-thread-31 INFO AppInfoParser: Kafka startTimeMs: 1613919096137
21/02/21 22:51:36.138 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-56, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.139 pool-9-thread-31 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.139 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-56, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.141 pool-9-thread-23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.143 pool-9-thread-31 INFO Metadata: [Consumer clientId=consumer-groupId-56, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.148 pool-9-thread-23 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.148 pool-9-thread-23 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.148 pool-9-thread-23 INFO AppInfoParser: Kafka startTimeMs: 1613919096148
21/02/21 22:51:36.148 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-57, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.148 pool-9-thread-23 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.148 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-57, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.150 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.153 pool-9-thread-23 INFO Metadata: [Consumer clientId=consumer-groupId-57, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.157 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.157 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.157 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919096157
21/02/21 22:51:36.157 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-58, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.157 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.157 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-58, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.158 pool-9-thread-27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.162 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-58, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.165 pool-9-thread-27 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.165 pool-9-thread-27 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.165 pool-9-thread-27 INFO AppInfoParser: Kafka startTimeMs: 1613919096165
21/02/21 22:51:36.166 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-59, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.166 pool-9-thread-27 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.166 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-59, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.167 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.171 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.171 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.171 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919096171
21/02/21 22:51:36.172 pool-9-thread-27 INFO Metadata: [Consumer clientId=consumer-groupId-59, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.172 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-60, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.172 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.172 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-60, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.174 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.177 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.177 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.177 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919096177
21/02/21 22:51:36.177 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-61, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.178 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.178 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-61, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.178 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-60, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.183 pool-9-thread-22 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.184 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-61, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.188 pool-9-thread-22 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.189 pool-9-thread-22 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.189 pool-9-thread-22 INFO AppInfoParser: Kafka startTimeMs: 1613919096188
21/02/21 22:51:36.189 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-62, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.189 pool-9-thread-22 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.189 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-62, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.191 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.196 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.196 pool-9-thread-22 INFO Metadata: [Consumer clientId=consumer-groupId-62, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.198 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.198 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919096196
21/02/21 22:51:36.198 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-63, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.199 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.199 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-63, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.200 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.203 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.203 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.203 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919096202
21/02/21 22:51:36.203 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-64, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.203 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.203 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-64, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.204 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-63, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.208 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.208 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-64, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.215 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.215 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.215 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919096215
21/02/21 22:51:36.215 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-65, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.215 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.215 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-65, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.216 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.220 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.220 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.220 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919096220
21/02/21 22:51:36.220 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-65, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.220 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-66, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.220 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.220 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-66, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.222 pool-9-thread-7 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.225 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-66, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.228 pool-9-thread-7 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.228 pool-9-thread-7 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.228 pool-9-thread-7 INFO AppInfoParser: Kafka startTimeMs: 1613919096228
21/02/21 22:51:36.228 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-67, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.229 pool-9-thread-7 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.229 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-67, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.231 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.234 pool-9-thread-7 INFO Metadata: [Consumer clientId=consumer-groupId-67, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.234 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.234 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.234 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919096234
21/02/21 22:51:36.235 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-68, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.235 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.235 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-68, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.236 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.239 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.240 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.240 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919096239
21/02/21 22:51:36.240 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-69, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.240 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.240 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-69, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.240 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-68, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.243 pool-9-thread-10 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.245 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-69, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.252 pool-9-thread-10 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.252 pool-9-thread-10 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.252 pool-9-thread-10 INFO AppInfoParser: Kafka startTimeMs: 1613919096252
21/02/21 22:51:36.252 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-70, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.252 pool-9-thread-10 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.252 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-70, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.254 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.256 pool-9-thread-10 INFO Metadata: [Consumer clientId=consumer-groupId-70, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.257 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.257 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.258 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919096257
21/02/21 22:51:36.258 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-71, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.258 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.258 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-71, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.259 pool-9-thread-16 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.263 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-71, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.263 pool-9-thread-16 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.264 pool-9-thread-16 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.264 pool-9-thread-16 INFO AppInfoParser: Kafka startTimeMs: 1613919096263
21/02/21 22:51:36.264 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-72, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.265 pool-9-thread-16 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.265 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-72, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.267 pool-9-thread-12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.270 pool-9-thread-16 INFO Metadata: [Consumer clientId=consumer-groupId-72, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.274 pool-9-thread-12 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.274 pool-9-thread-12 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.274 pool-9-thread-12 INFO AppInfoParser: Kafka startTimeMs: 1613919096274
21/02/21 22:51:36.274 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-73, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.274 pool-9-thread-12 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.274 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-73, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.275 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.279 pool-9-thread-12 INFO Metadata: [Consumer clientId=consumer-groupId-73, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.281 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.281 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.281 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919096281
21/02/21 22:51:36.282 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-74, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.282 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.282 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-74, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.284 pool-9-thread-17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.285 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-74, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.291 pool-9-thread-17 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.291 pool-9-thread-17 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.291 pool-9-thread-17 INFO AppInfoParser: Kafka startTimeMs: 1613919096291
21/02/21 22:51:36.291 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-75, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.291 pool-9-thread-17 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.291 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-75, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.293 pool-9-thread-13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.296 pool-9-thread-17 INFO Metadata: [Consumer clientId=consumer-groupId-75, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.298 pool-9-thread-13 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.298 pool-9-thread-13 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.298 pool-9-thread-13 INFO AppInfoParser: Kafka startTimeMs: 1613919096298
21/02/21 22:51:36.298 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-76, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.298 pool-9-thread-13 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.298 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-76, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.301 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.303 pool-9-thread-13 INFO Metadata: [Consumer clientId=consumer-groupId-76, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.306 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.306 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.307 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919096306
21/02/21 22:51:36.307 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-77, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.307 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.308 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-77, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.309 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.313 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.314 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.314 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919096313
21/02/21 22:51:36.314 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-78, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.314 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.314 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-78, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.315 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-77, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.319 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.319 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-78, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.326 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.326 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.326 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919096326
21/02/21 22:51:36.326 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-79, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.326 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.326 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-79, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.329 pool-9-thread-9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.330 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-79, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.331 pool-9-thread-9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.332 pool-9-thread-9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.332 pool-9-thread-9 INFO AppInfoParser: Kafka startTimeMs: 1613919096331
21/02/21 22:51:36.332 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-80, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.332 pool-9-thread-9 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.332 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-80, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.334 pool-9-thread-5 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.336 pool-9-thread-9 INFO Metadata: [Consumer clientId=consumer-groupId-80, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.341 pool-9-thread-5 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.341 pool-9-thread-5 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.341 pool-9-thread-5 INFO AppInfoParser: Kafka startTimeMs: 1613919096340
21/02/21 22:51:36.341 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-81, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.341 pool-9-thread-5 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.341 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-81, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.343 pool-9-thread-4 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.346 pool-9-thread-5 INFO Metadata: [Consumer clientId=consumer-groupId-81, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.347 pool-9-thread-4 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.347 pool-9-thread-4 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.347 pool-9-thread-4 INFO AppInfoParser: Kafka startTimeMs: 1613919096347
21/02/21 22:51:36.347 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-82, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.348 pool-9-thread-4 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.348 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-82, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.349 pool-9-thread-9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.352 pool-9-thread-9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.352 pool-9-thread-9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.352 pool-9-thread-9 INFO AppInfoParser: Kafka startTimeMs: 1613919096352
21/02/21 22:51:36.352 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-83, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.352 pool-9-thread-9 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.352 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-83, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.353 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.354 pool-9-thread-4 INFO Metadata: [Consumer clientId=consumer-groupId-82, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.357 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.357 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.357 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919096357
21/02/21 22:51:36.358 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-84, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.358 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.358 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-84, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.360 pool-9-thread-9 INFO Metadata: [Consumer clientId=consumer-groupId-83, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.362 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.364 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-84, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.368 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.368 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.368 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919096367
21/02/21 22:51:36.368 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-85, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.368 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.368 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-85, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.369 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.373 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.374 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.374 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919096373
21/02/21 22:51:36.374 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-86, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.374 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.374 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-86, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.375 pool-9-thread-13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.375 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-85, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.379 pool-9-thread-13 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.379 pool-9-thread-13 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.379 pool-9-thread-13 INFO AppInfoParser: Kafka startTimeMs: 1613919096378
21/02/21 22:51:36.379 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-87, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.379 pool-9-thread-13 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.379 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-87, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.380 pool-9-thread-17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.381 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-86, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.384 pool-9-thread-17 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.384 pool-9-thread-17 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.384 pool-9-thread-17 INFO AppInfoParser: Kafka startTimeMs: 1613919096384
21/02/21 22:51:36.385 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-88, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.385 pool-9-thread-17 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.385 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-88, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.389 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.389 pool-9-thread-13 INFO Metadata: [Consumer clientId=consumer-groupId-87, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.392 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.392 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.392 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919096392
21/02/21 22:51:36.393 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-89, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.393 pool-9-thread-17 INFO Metadata: [Consumer clientId=consumer-groupId-88, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.393 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.393 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-89, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.395 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.400 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-89, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.404 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.405 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.405 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919096404
21/02/21 22:51:36.405 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-90, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.405 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.405 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-90, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.406 pool-9-thread-12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.412 pool-9-thread-12 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.412 pool-9-thread-12 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.412 pool-9-thread-12 INFO AppInfoParser: Kafka startTimeMs: 1613919096411
21/02/21 22:51:36.412 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-91, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.412 pool-9-thread-12 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.412 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-91, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.413 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-90, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.415 pool-9-thread-16 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.418 pool-9-thread-12 INFO Metadata: [Consumer clientId=consumer-groupId-91, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.421 pool-9-thread-16 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.421 pool-9-thread-16 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.421 pool-9-thread-16 INFO AppInfoParser: Kafka startTimeMs: 1613919096421
21/02/21 22:51:36.423 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-92, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.423 pool-9-thread-16 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.423 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-92, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.425 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.429 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.429 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.429 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919096429
21/02/21 22:51:36.430 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-93, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.430 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.430 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-93, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.430 pool-9-thread-16 INFO Metadata: [Consumer clientId=consumer-groupId-92, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.432 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.434 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-93, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.436 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.436 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.436 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919096436
21/02/21 22:51:36.436 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-94, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.437 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.437 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-94, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.440 pool-9-thread-10 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.443 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-94, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.444 pool-9-thread-10 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.444 pool-9-thread-10 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.444 pool-9-thread-10 INFO AppInfoParser: Kafka startTimeMs: 1613919096444
21/02/21 22:51:36.444 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-95, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.444 pool-9-thread-10 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.444 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-95, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.445 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.448 pool-9-thread-10 INFO Metadata: [Consumer clientId=consumer-groupId-95, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.450 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.451 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.451 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919096450
21/02/21 22:51:36.451 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-96, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.451 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.451 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-96, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.452 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.455 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-96, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.456 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.456 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.456 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919096456
21/02/21 22:51:36.456 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-97, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.456 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.456 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-97, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.458 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.462 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.462 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-97, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.462 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.463 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919096462
21/02/21 22:51:36.463 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-98, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.463 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.464 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-98, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.465 pool-9-thread-7 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.469 pool-9-thread-7 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.469 pool-9-thread-7 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.469 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-98, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.470 pool-9-thread-7 INFO AppInfoParser: Kafka startTimeMs: 1613919096469
21/02/21 22:51:36.470 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-99, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.471 pool-9-thread-7 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.471 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-99, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.472 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.476 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.476 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.476 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919096476
21/02/21 22:51:36.476 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-100, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.476 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.476 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-100, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.478 pool-9-thread-7 INFO Metadata: [Consumer clientId=consumer-groupId-99, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.480 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.481 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-100, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.483 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.483 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.483 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919096483
21/02/21 22:51:36.484 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-101, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.484 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.484 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-101, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.485 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.489 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-101, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.492 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.492 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.492 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919096492
21/02/21 22:51:36.492 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-102, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.492 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.492 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-102, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.493 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.497 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-102, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.498 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.498 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.498 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919096498
21/02/21 22:51:36.498 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-103, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.499 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.499 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-103, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.501 pool-9-thread-22 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.503 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-103, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.507 pool-9-thread-22 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.507 pool-9-thread-22 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.507 pool-9-thread-22 INFO AppInfoParser: Kafka startTimeMs: 1613919096507
21/02/21 22:51:36.507 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-104, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.507 pool-9-thread-22 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.507 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-104, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.508 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.511 pool-9-thread-22 INFO Metadata: [Consumer clientId=consumer-groupId-104, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.514 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.514 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.514 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919096514
21/02/21 22:51:36.514 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-105, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.514 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.514 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-105, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.515 pool-9-thread-27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.519 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-105, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.521 pool-9-thread-27 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.521 pool-9-thread-27 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.521 pool-9-thread-27 INFO AppInfoParser: Kafka startTimeMs: 1613919096521
21/02/21 22:51:36.521 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-106, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.521 pool-9-thread-27 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.521 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-106, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.522 pool-9-thread-23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.526 pool-9-thread-27 INFO Metadata: [Consumer clientId=consumer-groupId-106, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.527 pool-9-thread-23 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.527 pool-9-thread-23 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.527 pool-9-thread-23 INFO AppInfoParser: Kafka startTimeMs: 1613919096527
21/02/21 22:51:36.528 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-107, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.528 pool-9-thread-23 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.528 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-107, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.529 pool-9-thread-31 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.532 pool-9-thread-23 INFO Metadata: [Consumer clientId=consumer-groupId-107, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.536 pool-9-thread-31 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.536 pool-9-thread-31 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.536 pool-9-thread-31 INFO AppInfoParser: Kafka startTimeMs: 1613919096536
21/02/21 22:51:36.537 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-108, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.537 pool-9-thread-31 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.537 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-108, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.539 pool-9-thread-19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.542 pool-9-thread-31 INFO Metadata: [Consumer clientId=consumer-groupId-108, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.543 pool-9-thread-19 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.543 pool-9-thread-19 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.543 pool-9-thread-19 INFO AppInfoParser: Kafka startTimeMs: 1613919096543
21/02/21 22:51:36.543 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-109, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.544 pool-9-thread-19 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.544 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-109, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.544 pool-9-thread-26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.547 pool-9-thread-19 INFO Metadata: [Consumer clientId=consumer-groupId-109, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.548 pool-9-thread-26 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.548 pool-9-thread-26 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.548 pool-9-thread-26 INFO AppInfoParser: Kafka startTimeMs: 1613919096548
21/02/21 22:51:36.548 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-110, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.548 pool-9-thread-26 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.548 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-110, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.550 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.553 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.553 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.553 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919096553
21/02/21 22:51:36.553 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-111, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.554 pool-9-thread-26 INFO Metadata: [Consumer clientId=consumer-groupId-110, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.554 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.554 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-111, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.559 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-111, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.559 pool-9-thread-15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.564 pool-9-thread-15 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.564 pool-9-thread-15 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.564 pool-9-thread-15 INFO AppInfoParser: Kafka startTimeMs: 1613919096564
21/02/21 22:51:36.564 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-112, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.564 pool-9-thread-15 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.565 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-112, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.568 pool-9-thread-15 INFO Metadata: [Consumer clientId=consumer-groupId-112, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.573 pool-9-thread-25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.577 pool-9-thread-25 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.577 pool-9-thread-25 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.577 pool-9-thread-25 INFO AppInfoParser: Kafka startTimeMs: 1613919096577
21/02/21 22:51:36.577 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-113, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.578 pool-9-thread-25 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.578 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-113, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.578 pool-9-thread-30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.581 pool-9-thread-25 INFO Metadata: [Consumer clientId=consumer-groupId-113, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.582 pool-9-thread-30 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.582 pool-9-thread-30 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.582 pool-9-thread-30 INFO AppInfoParser: Kafka startTimeMs: 1613919096582
21/02/21 22:51:36.582 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-114, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.582 pool-9-thread-30 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.582 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-114, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.584 pool-9-thread-24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.585 pool-9-thread-30 INFO Metadata: [Consumer clientId=consumer-groupId-114, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.590 pool-9-thread-24 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.590 pool-9-thread-24 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.590 pool-9-thread-24 INFO AppInfoParser: Kafka startTimeMs: 1613919096590
21/02/21 22:51:36.590 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-115, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.590 pool-9-thread-24 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.590 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-115, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.590 pool-9-thread-29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.593 pool-9-thread-24 INFO Metadata: [Consumer clientId=consumer-groupId-115, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.595 pool-9-thread-29 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.595 pool-9-thread-29 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.595 pool-9-thread-29 INFO AppInfoParser: Kafka startTimeMs: 1613919096595
21/02/21 22:51:36.595 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-116, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.595 pool-9-thread-29 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.596 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-116, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.596 pool-9-thread-34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.600 pool-9-thread-29 INFO Metadata: [Consumer clientId=consumer-groupId-116, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.600 pool-9-thread-34 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.601 pool-9-thread-34 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.601 pool-9-thread-34 INFO AppInfoParser: Kafka startTimeMs: 1613919096600
21/02/21 22:51:36.601 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-117, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.601 pool-9-thread-34 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.601 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-117, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.602 pool-9-thread-35 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.606 pool-9-thread-34 INFO Metadata: [Consumer clientId=consumer-groupId-117, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.607 pool-9-thread-35 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.607 pool-9-thread-35 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.607 pool-9-thread-35 INFO AppInfoParser: Kafka startTimeMs: 1613919096607
21/02/21 22:51:36.607 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-118, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.607 pool-9-thread-35 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.607 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-118, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.609 pool-9-thread-43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.613 pool-9-thread-35 INFO Metadata: [Consumer clientId=consumer-groupId-118, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.617 pool-9-thread-43 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.617 pool-9-thread-43 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.617 pool-9-thread-43 INFO AppInfoParser: Kafka startTimeMs: 1613919096617
21/02/21 22:51:36.617 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-119, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.617 pool-9-thread-43 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.617 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-119, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.618 pool-9-thread-38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.622 pool-9-thread-38 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.622 pool-9-thread-38 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.622 pool-9-thread-38 INFO AppInfoParser: Kafka startTimeMs: 1613919096622
21/02/21 22:51:36.622 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-120, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.623 pool-9-thread-38 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.623 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-120, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.623 pool-9-thread-43 INFO Metadata: [Consumer clientId=consumer-groupId-119, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.623 pool-9-thread-44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.627 pool-9-thread-44 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.627 pool-9-thread-44 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.627 pool-9-thread-44 INFO AppInfoParser: Kafka startTimeMs: 1613919096627
21/02/21 22:51:36.627 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-121, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.628 pool-9-thread-44 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.628 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-121, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.628 pool-9-thread-38 INFO Metadata: [Consumer clientId=consumer-groupId-120, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.629 pool-9-thread-40 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.634 pool-9-thread-40 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.634 pool-9-thread-44 INFO Metadata: [Consumer clientId=consumer-groupId-121, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.634 pool-9-thread-40 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.634 pool-9-thread-40 INFO AppInfoParser: Kafka startTimeMs: 1613919096634
21/02/21 22:51:36.635 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-122, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.635 pool-9-thread-40 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.635 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-122, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.636 pool-9-thread-45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.640 pool-9-thread-40 INFO Metadata: [Consumer clientId=consumer-groupId-122, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.657 pool-9-thread-45 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.657 pool-9-thread-45 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.657 pool-9-thread-45 INFO AppInfoParser: Kafka startTimeMs: 1613919096657
21/02/21 22:51:36.657 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-123, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.657 pool-9-thread-45 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.658 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-123, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.660 pool-9-thread-21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.663 pool-9-thread-45 INFO Metadata: [Consumer clientId=consumer-groupId-123, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.663 pool-9-thread-21 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.663 pool-9-thread-21 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.663 pool-9-thread-21 INFO AppInfoParser: Kafka startTimeMs: 1613919096663
21/02/21 22:51:36.663 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-124, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.663 pool-9-thread-21 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.663 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-124, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.666 pool-9-thread-48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.667 pool-9-thread-21 INFO Metadata: [Consumer clientId=consumer-groupId-124, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.671 pool-9-thread-48 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.671 pool-9-thread-48 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.672 pool-9-thread-48 INFO AppInfoParser: Kafka startTimeMs: 1613919096671
21/02/21 22:51:36.672 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-125, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.672 pool-9-thread-48 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.672 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-125, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.673 pool-9-thread-37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.675 pool-9-thread-48 INFO Metadata: [Consumer clientId=consumer-groupId-125, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.678 pool-9-thread-37 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.678 pool-9-thread-37 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.678 pool-9-thread-37 INFO AppInfoParser: Kafka startTimeMs: 1613919096678
21/02/21 22:51:36.678 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-126, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.678 pool-9-thread-37 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.678 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-126, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.679 pool-9-thread-42 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.682 pool-9-thread-42 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.682 pool-9-thread-42 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.682 pool-9-thread-42 INFO AppInfoParser: Kafka startTimeMs: 1613919096682
21/02/21 22:51:36.682 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-127, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.683 pool-9-thread-42 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.683 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-127, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.684 pool-9-thread-37 INFO Metadata: [Consumer clientId=consumer-groupId-126, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.684 pool-9-thread-46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.687 pool-9-thread-46 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.687 pool-9-thread-46 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.687 pool-9-thread-46 INFO AppInfoParser: Kafka startTimeMs: 1613919096687
21/02/21 22:51:36.687 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-128, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.687 pool-9-thread-46 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.687 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-128, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.688 pool-9-thread-42 INFO Metadata: [Consumer clientId=consumer-groupId-127, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.688 pool-9-thread-36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.692 pool-9-thread-46 INFO Metadata: [Consumer clientId=consumer-groupId-128, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.694 pool-9-thread-36 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.694 pool-9-thread-36 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.694 pool-9-thread-36 INFO AppInfoParser: Kafka startTimeMs: 1613919096694
21/02/21 22:51:36.694 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-129, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.694 pool-9-thread-36 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.694 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-129, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.696 pool-9-thread-49 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.699 pool-9-thread-36 INFO Metadata: [Consumer clientId=consumer-groupId-129, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.701 pool-9-thread-49 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.701 pool-9-thread-49 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.701 pool-9-thread-49 INFO AppInfoParser: Kafka startTimeMs: 1613919096701
21/02/21 22:51:36.701 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-130, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.702 pool-9-thread-49 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.702 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-130, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.702 pool-9-thread-47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.706 pool-9-thread-47 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.707 pool-9-thread-47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.707 pool-9-thread-47 INFO AppInfoParser: Kafka startTimeMs: 1613919096706
21/02/21 22:51:36.707 pool-9-thread-49 INFO Metadata: [Consumer clientId=consumer-groupId-130, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.707 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-131, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.707 pool-9-thread-47 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.707 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-131, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.707 pool-9-thread-41 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.711 pool-9-thread-41 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.711 pool-9-thread-41 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.711 pool-9-thread-41 INFO AppInfoParser: Kafka startTimeMs: 1613919096711
21/02/21 22:51:36.711 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-132, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.711 pool-9-thread-41 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.711 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-132, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.712 pool-9-thread-47 INFO Metadata: [Consumer clientId=consumer-groupId-131, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.714 pool-9-thread-39 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.717 pool-9-thread-41 INFO Metadata: [Consumer clientId=consumer-groupId-132, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.720 pool-9-thread-39 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.720 pool-9-thread-39 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.720 pool-9-thread-39 INFO AppInfoParser: Kafka startTimeMs: 1613919096720
21/02/21 22:51:36.720 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-133, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.721 pool-9-thread-39 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.721 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-133, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.723 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.726 pool-9-thread-39 INFO Metadata: [Consumer clientId=consumer-groupId-133, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.727 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.727 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.727 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919096726
21/02/21 22:51:36.727 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-134, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.727 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.727 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-134, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.728 pool-9-thread-54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.730 pool-9-thread-54 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.730 pool-9-thread-54 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.730 pool-9-thread-54 INFO AppInfoParser: Kafka startTimeMs: 1613919096730
21/02/21 22:51:36.731 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-135, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.731 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-134, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.731 pool-9-thread-54 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.731 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-135, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.731 pool-9-thread-51 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.735 pool-9-thread-51 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.735 pool-9-thread-51 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.735 pool-9-thread-51 INFO AppInfoParser: Kafka startTimeMs: 1613919096735
21/02/21 22:51:36.735 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-136, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.735 pool-9-thread-51 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.735 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-136, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.735 pool-9-thread-54 INFO Metadata: [Consumer clientId=consumer-groupId-135, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.736 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.740 pool-9-thread-51 INFO Metadata: [Consumer clientId=consumer-groupId-136, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.741 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.741 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.741 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919096741
21/02/21 22:51:36.741 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-137, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.741 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.741 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-137, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.742 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.745 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.745 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.745 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919096745
21/02/21 22:51:36.745 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-138, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.746 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.746 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-138, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.746 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.748 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-137, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.752 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.752 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.752 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919096752
21/02/21 22:51:36.752 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-139, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.752 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-138, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.752 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.752 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-139, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.754 pool-9-thread-59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.756 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-139, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.758 pool-9-thread-59 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.759 pool-9-thread-59 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.759 pool-9-thread-59 INFO AppInfoParser: Kafka startTimeMs: 1613919096758
21/02/21 22:51:36.759 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-140, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.759 pool-9-thread-59 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.759 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-140, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.760 pool-9-thread-53 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.764 pool-9-thread-53 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.764 pool-9-thread-53 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.764 pool-9-thread-53 INFO AppInfoParser: Kafka startTimeMs: 1613919096764
21/02/21 22:51:36.764 pool-9-thread-59 INFO Metadata: [Consumer clientId=consumer-groupId-140, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.764 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-141, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.764 pool-9-thread-53 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.765 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-141, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.766 pool-9-thread-55 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.768 pool-9-thread-53 INFO Metadata: [Consumer clientId=consumer-groupId-141, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.769 pool-9-thread-55 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.769 pool-9-thread-55 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.769 pool-9-thread-55 INFO AppInfoParser: Kafka startTimeMs: 1613919096769
21/02/21 22:51:36.770 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-142, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.770 pool-9-thread-55 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.770 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-142, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.771 pool-9-thread-56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.774 pool-9-thread-55 INFO Metadata: [Consumer clientId=consumer-groupId-142, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.774 pool-9-thread-56 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.775 pool-9-thread-56 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.775 pool-9-thread-56 INFO AppInfoParser: Kafka startTimeMs: 1613919096774
21/02/21 22:51:36.775 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-143, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.775 pool-9-thread-56 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.775 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-143, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.776 pool-9-thread-57 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.779 pool-9-thread-57 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.779 pool-9-thread-57 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.779 pool-9-thread-57 INFO AppInfoParser: Kafka startTimeMs: 1613919096779
21/02/21 22:51:36.779 pool-9-thread-56 INFO Metadata: [Consumer clientId=consumer-groupId-143, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.779 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-144, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.779 pool-9-thread-57 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.780 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-144, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.780 pool-9-thread-60 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.784 pool-9-thread-60 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.784 pool-9-thread-60 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.784 pool-9-thread-60 INFO AppInfoParser: Kafka startTimeMs: 1613919096784
21/02/21 22:51:36.784 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-145, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.785 pool-9-thread-60 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.785 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-145, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.785 pool-9-thread-80 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.785 pool-9-thread-57 INFO Metadata: [Consumer clientId=consumer-groupId-144, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.789 pool-9-thread-60 INFO Metadata: [Consumer clientId=consumer-groupId-145, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.789 pool-9-thread-80 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.789 pool-9-thread-80 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.790 pool-9-thread-80 INFO AppInfoParser: Kafka startTimeMs: 1613919096789
21/02/21 22:51:36.790 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-146, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.790 pool-9-thread-80 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.790 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-146, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.793 pool-9-thread-66 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.794 pool-9-thread-80 INFO Metadata: [Consumer clientId=consumer-groupId-146, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.800 pool-9-thread-66 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.800 pool-9-thread-66 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.800 pool-9-thread-66 INFO AppInfoParser: Kafka startTimeMs: 1613919096800
21/02/21 22:51:36.800 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-147, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.800 pool-9-thread-66 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.800 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-147, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.801 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.804 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.804 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.804 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919096804
21/02/21 22:51:36.805 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-148, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.805 pool-9-thread-66 INFO Metadata: [Consumer clientId=consumer-groupId-147, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.805 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.805 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-148, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.807 pool-9-thread-63 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.810 pool-9-thread-63 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.810 pool-9-thread-63 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.811 pool-9-thread-63 INFO AppInfoParser: Kafka startTimeMs: 1613919096810
21/02/21 22:51:36.811 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-149, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.811 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-148, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.811 pool-9-thread-63 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.811 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-149, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.814 pool-9-thread-75 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.816 pool-9-thread-63 INFO Metadata: [Consumer clientId=consumer-groupId-149, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.818 pool-9-thread-75 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.818 pool-9-thread-75 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.819 pool-9-thread-75 INFO AppInfoParser: Kafka startTimeMs: 1613919096818
21/02/21 22:51:36.819 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-150, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.819 pool-9-thread-75 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.819 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-150, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.822 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.825 pool-9-thread-75 INFO Metadata: [Consumer clientId=consumer-groupId-150, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.825 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.825 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.826 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919096825
21/02/21 22:51:36.826 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-151, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.826 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.826 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-151, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.828 pool-9-thread-78 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.830 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-151, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.833 pool-9-thread-78 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.833 pool-9-thread-78 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.833 pool-9-thread-78 INFO AppInfoParser: Kafka startTimeMs: 1613919096833
21/02/21 22:51:36.833 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-152, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.833 pool-9-thread-78 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.834 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-152, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.834 pool-9-thread-67 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.837 pool-9-thread-78 INFO Metadata: [Consumer clientId=consumer-groupId-152, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.840 pool-9-thread-67 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.840 pool-9-thread-67 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.841 pool-9-thread-67 INFO AppInfoParser: Kafka startTimeMs: 1613919096840
21/02/21 22:51:36.841 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-153, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.841 pool-9-thread-67 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.841 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-153, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.841 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.845 pool-9-thread-67 INFO Metadata: [Consumer clientId=consumer-groupId-153, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.846 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.846 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.846 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919096846
21/02/21 22:51:36.847 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-154, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.847 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.847 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-154, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.848 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.852 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-154, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.853 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.853 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.853 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919096853
21/02/21 22:51:36.853 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-155, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.854 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.854 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-155, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.856 pool-9-thread-74 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.858 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-155, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.860 pool-9-thread-74 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.860 pool-9-thread-74 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.860 pool-9-thread-74 INFO AppInfoParser: Kafka startTimeMs: 1613919096860
21/02/21 22:51:36.860 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-156, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.860 pool-9-thread-74 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.860 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-156, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.862 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.863 pool-9-thread-74 INFO Metadata: [Consumer clientId=consumer-groupId-156, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.866 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.866 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.866 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919096866
21/02/21 22:51:36.866 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-157, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.866 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.866 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-157, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.868 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.871 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.871 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.871 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919096871
21/02/21 22:51:36.871 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-158, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.871 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-157, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.871 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.871 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-158, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.874 pool-9-thread-69 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.876 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-158, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.877 pool-9-thread-69 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.877 pool-9-thread-69 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.878 pool-9-thread-69 INFO AppInfoParser: Kafka startTimeMs: 1613919096877
21/02/21 22:51:36.880 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-159, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.880 pool-9-thread-69 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.880 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-159, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.880 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.883 pool-9-thread-69 INFO Metadata: [Consumer clientId=consumer-groupId-159, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.887 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.888 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.888 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919096887
21/02/21 22:51:36.888 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-160, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.888 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.888 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-160, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.890 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.893 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-160, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.894 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.894 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.894 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919096894
21/02/21 22:51:36.894 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-161, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.894 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.894 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-161, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.898 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.902 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.902 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.902 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919096902
21/02/21 22:51:36.902 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-162, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.902 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.902 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-162, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.906 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-161, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.909 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.910 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-162, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.913 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.913 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.913 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919096913
21/02/21 22:51:36.913 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-163, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.913 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.913 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-163, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.917 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.923 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-163, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.925 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.925 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.925 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919096925
21/02/21 22:51:36.925 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-164, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.925 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.925 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-164, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.926 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.930 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.930 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.930 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919096930
21/02/21 22:51:36.930 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-165, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.930 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.931 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-165, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.932 pool-9-thread-98 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.934 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-164, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.934 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-165, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.938 pool-9-thread-98 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.938 pool-9-thread-98 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.938 pool-9-thread-98 INFO AppInfoParser: Kafka startTimeMs: 1613919096938
21/02/21 22:51:36.938 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-166, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.938 pool-9-thread-98 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.938 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-166, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.943 pool-9-thread-90 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.943 pool-9-thread-98 INFO Metadata: [Consumer clientId=consumer-groupId-166, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.947 pool-9-thread-90 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.947 pool-9-thread-90 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.947 pool-9-thread-90 INFO AppInfoParser: Kafka startTimeMs: 1613919096947
21/02/21 22:51:36.947 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-167, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.947 pool-9-thread-90 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.947 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-167, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.948 pool-9-thread-94 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.952 pool-9-thread-90 INFO Metadata: [Consumer clientId=consumer-groupId-167, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.953 pool-9-thread-94 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.953 pool-9-thread-94 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.953 pool-9-thread-94 INFO AppInfoParser: Kafka startTimeMs: 1613919096953
21/02/21 22:51:36.953 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-168, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.953 pool-9-thread-94 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.953 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-168, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.955 pool-9-thread-95 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.957 pool-9-thread-94 INFO Metadata: [Consumer clientId=consumer-groupId-168, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.959 pool-9-thread-95 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.959 pool-9-thread-95 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.959 pool-9-thread-95 INFO AppInfoParser: Kafka startTimeMs: 1613919096959
21/02/21 22:51:36.960 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-169, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.960 pool-9-thread-95 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.960 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-169, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.961 pool-9-thread-99 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.964 pool-9-thread-95 INFO Metadata: [Consumer clientId=consumer-groupId-169, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.969 pool-9-thread-99 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.969 pool-9-thread-99 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.969 pool-9-thread-99 INFO AppInfoParser: Kafka startTimeMs: 1613919096969
21/02/21 22:51:36.970 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-170, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.970 pool-9-thread-99 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.970 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-170, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.971 pool-9-thread-97 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.984 pool-9-thread-99 INFO Metadata: [Consumer clientId=consumer-groupId-170, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.985 pool-9-thread-97 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.985 pool-9-thread-97 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.985 pool-9-thread-97 INFO AppInfoParser: Kafka startTimeMs: 1613919096985
21/02/21 22:51:36.985 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-171, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.986 pool-9-thread-97 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.986 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-171, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.987 pool-9-thread-100 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.990 pool-9-thread-97 INFO Metadata: [Consumer clientId=consumer-groupId-171, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.991 pool-9-thread-100 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.991 pool-9-thread-100 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.991 pool-9-thread-100 INFO AppInfoParser: Kafka startTimeMs: 1613919096991
21/02/21 22:51:36.991 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-172, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.991 pool-9-thread-100 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.991 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-172, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.992 pool-9-thread-93 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:36.995 pool-9-thread-100 INFO Metadata: [Consumer clientId=consumer-groupId-172, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:36.997 pool-9-thread-93 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:36.997 pool-9-thread-93 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:36.997 pool-9-thread-93 INFO AppInfoParser: Kafka startTimeMs: 1613919096997
21/02/21 22:51:36.997 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-173, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:36.997 pool-9-thread-93 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:36.997 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-173, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:36.998 pool-9-thread-96 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.001 pool-9-thread-93 INFO Metadata: [Consumer clientId=consumer-groupId-173, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.003 pool-9-thread-96 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.003 pool-9-thread-96 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.004 pool-9-thread-96 INFO AppInfoParser: Kafka startTimeMs: 1613919097003
21/02/21 22:51:37.004 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-174, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.004 pool-9-thread-96 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.004 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-174, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.005 pool-9-thread-92 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.008 pool-9-thread-96 INFO Metadata: [Consumer clientId=consumer-groupId-174, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.009 pool-9-thread-92 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.009 pool-9-thread-92 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.009 pool-9-thread-92 INFO AppInfoParser: Kafka startTimeMs: 1613919097009
21/02/21 22:51:37.009 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-175, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.010 pool-9-thread-92 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.010 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-175, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.011 pool-9-thread-88 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.013 pool-9-thread-92 INFO Metadata: [Consumer clientId=consumer-groupId-175, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.016 pool-9-thread-88 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.017 pool-9-thread-88 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.017 pool-9-thread-88 INFO AppInfoParser: Kafka startTimeMs: 1613919097016
21/02/21 22:51:37.017 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-176, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.017 pool-9-thread-88 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.017 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-176, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.018 pool-9-thread-91 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.020 pool-9-thread-88 INFO Metadata: [Consumer clientId=consumer-groupId-176, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.022 pool-9-thread-91 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.022 pool-9-thread-91 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.022 pool-9-thread-91 INFO AppInfoParser: Kafka startTimeMs: 1613919097021
21/02/21 22:51:37.022 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-177, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.022 pool-9-thread-91 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.022 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-177, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.023 pool-9-thread-89 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.025 pool-9-thread-91 INFO Metadata: [Consumer clientId=consumer-groupId-177, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.028 pool-9-thread-89 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.028 pool-9-thread-89 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.028 pool-9-thread-89 INFO AppInfoParser: Kafka startTimeMs: 1613919097028
21/02/21 22:51:37.029 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-178, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.029 pool-9-thread-89 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.029 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-178, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.031 pool-9-thread-86 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.032 pool-9-thread-89 INFO Metadata: [Consumer clientId=consumer-groupId-178, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.034 pool-9-thread-86 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.034 pool-9-thread-86 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.034 pool-9-thread-86 INFO AppInfoParser: Kafka startTimeMs: 1613919097034
21/02/21 22:51:37.034 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-179, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.034 pool-9-thread-86 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.034 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-179, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.035 pool-9-thread-85 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.037 pool-9-thread-86 INFO Metadata: [Consumer clientId=consumer-groupId-179, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.040 pool-9-thread-85 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.040 pool-9-thread-85 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.040 pool-9-thread-85 INFO AppInfoParser: Kafka startTimeMs: 1613919097040
21/02/21 22:51:37.040 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-180, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.040 pool-9-thread-85 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.040 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-180, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.041 pool-9-thread-83 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.044 pool-9-thread-85 INFO Metadata: [Consumer clientId=consumer-groupId-180, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.046 pool-9-thread-83 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.046 pool-9-thread-83 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.046 pool-9-thread-83 INFO AppInfoParser: Kafka startTimeMs: 1613919097046
21/02/21 22:51:37.046 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-181, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.046 pool-9-thread-83 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.047 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-181, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.048 pool-9-thread-87 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.050 pool-9-thread-83 INFO Metadata: [Consumer clientId=consumer-groupId-181, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.053 pool-9-thread-87 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.053 pool-9-thread-87 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.053 pool-9-thread-87 INFO AppInfoParser: Kafka startTimeMs: 1613919097053
21/02/21 22:51:37.053 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-182, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.053 pool-9-thread-87 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.053 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-182, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.054 pool-9-thread-84 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.057 pool-9-thread-87 INFO Metadata: [Consumer clientId=consumer-groupId-182, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.058 pool-9-thread-84 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.058 pool-9-thread-84 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.058 pool-9-thread-84 INFO AppInfoParser: Kafka startTimeMs: 1613919097058
21/02/21 22:51:37.059 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-183, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.059 pool-9-thread-84 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.059 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-183, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.061 pool-9-thread-73 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.062 pool-9-thread-84 INFO Metadata: [Consumer clientId=consumer-groupId-183, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.065 pool-9-thread-73 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.065 pool-9-thread-73 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.065 pool-9-thread-73 INFO AppInfoParser: Kafka startTimeMs: 1613919097065
21/02/21 22:51:37.066 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-184, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.066 pool-9-thread-73 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.066 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-184, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.067 pool-9-thread-76 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.069 pool-9-thread-73 INFO Metadata: [Consumer clientId=consumer-groupId-184, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.071 pool-9-thread-76 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.071 pool-9-thread-76 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.072 pool-9-thread-76 INFO AppInfoParser: Kafka startTimeMs: 1613919097071
21/02/21 22:51:37.072 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-185, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.072 pool-9-thread-76 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.072 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-185, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.073 pool-9-thread-64 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.075 pool-9-thread-76 INFO Metadata: [Consumer clientId=consumer-groupId-185, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.078 pool-9-thread-64 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.078 pool-9-thread-64 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.078 pool-9-thread-64 INFO AppInfoParser: Kafka startTimeMs: 1613919097078
21/02/21 22:51:37.078 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-186, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.079 pool-9-thread-64 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.079 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-186, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.081 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.083 pool-9-thread-64 INFO Metadata: [Consumer clientId=consumer-groupId-186, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.085 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.085 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.085 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919097085
21/02/21 22:51:37.085 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-187, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.085 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.085 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-187, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.088 pool-9-thread-70 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.089 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-187, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.091 pool-9-thread-70 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.092 pool-9-thread-70 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.092 pool-9-thread-70 INFO AppInfoParser: Kafka startTimeMs: 1613919097091
21/02/21 22:51:37.092 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-188, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.092 pool-9-thread-70 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.092 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-188, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.093 pool-9-thread-82 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.096 pool-9-thread-70 INFO Metadata: [Consumer clientId=consumer-groupId-188, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.097 pool-9-thread-82 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.097 pool-9-thread-82 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.097 pool-9-thread-82 INFO AppInfoParser: Kafka startTimeMs: 1613919097097
21/02/21 22:51:37.097 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-189, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.097 pool-9-thread-82 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.097 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-189, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.098 pool-9-thread-77 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.100 pool-9-thread-82 INFO Metadata: [Consumer clientId=consumer-groupId-189, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.102 pool-9-thread-77 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.102 pool-9-thread-77 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.102 pool-9-thread-77 INFO AppInfoParser: Kafka startTimeMs: 1613919097102
21/02/21 22:51:37.102 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-190, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.103 pool-9-thread-77 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.103 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-190, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.105 pool-9-thread-79 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.108 pool-9-thread-77 INFO Metadata: [Consumer clientId=consumer-groupId-190, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.110 pool-9-thread-79 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.110 pool-9-thread-79 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.110 pool-9-thread-79 INFO AppInfoParser: Kafka startTimeMs: 1613919097110
21/02/21 22:51:37.110 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-191, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.110 pool-9-thread-79 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.111 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-191, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.111 pool-9-thread-82 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.114 pool-9-thread-79 INFO Metadata: [Consumer clientId=consumer-groupId-191, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.114 pool-9-thread-82 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.114 pool-9-thread-82 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.114 pool-9-thread-82 INFO AppInfoParser: Kafka startTimeMs: 1613919097114
21/02/21 22:51:37.114 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-192, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.115 pool-9-thread-82 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.115 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-192, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.115 pool-9-thread-70 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.118 pool-9-thread-82 INFO Metadata: [Consumer clientId=consumer-groupId-192, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.118 pool-9-thread-70 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.118 pool-9-thread-70 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.119 pool-9-thread-70 INFO AppInfoParser: Kafka startTimeMs: 1613919097118
21/02/21 22:51:37.119 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-193, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.120 pool-9-thread-70 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.120 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-193, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.123 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.125 pool-9-thread-70 INFO Metadata: [Consumer clientId=consumer-groupId-193, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.127 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.127 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.127 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919097127
21/02/21 22:51:37.127 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-194, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.127 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.127 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-194, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.129 pool-9-thread-64 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.132 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-194, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.133 pool-9-thread-64 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.134 pool-9-thread-64 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.134 pool-9-thread-64 INFO AppInfoParser: Kafka startTimeMs: 1613919097133
21/02/21 22:51:37.134 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-195, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.134 pool-9-thread-64 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.134 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-195, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.135 pool-9-thread-76 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.137 pool-9-thread-64 INFO Metadata: [Consumer clientId=consumer-groupId-195, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.138 pool-9-thread-76 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.138 pool-9-thread-76 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.138 pool-9-thread-76 INFO AppInfoParser: Kafka startTimeMs: 1613919097138
21/02/21 22:51:37.138 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-196, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.139 pool-9-thread-76 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.139 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-196, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.140 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.142 pool-9-thread-76 INFO Metadata: [Consumer clientId=consumer-groupId-196, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.143 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.143 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.143 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919097143
21/02/21 22:51:37.143 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-197, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.144 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.144 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-197, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.145 pool-9-thread-73 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.148 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-197, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.148 pool-9-thread-73 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.148 pool-9-thread-73 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.149 pool-9-thread-73 INFO AppInfoParser: Kafka startTimeMs: 1613919097148
21/02/21 22:51:37.150 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-198, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.150 pool-9-thread-73 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.150 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-198, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.151 pool-9-thread-84 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.153 pool-9-thread-73 INFO Metadata: [Consumer clientId=consumer-groupId-198, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.155 pool-9-thread-84 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.155 pool-9-thread-84 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.155 pool-9-thread-84 INFO AppInfoParser: Kafka startTimeMs: 1613919097155
21/02/21 22:51:37.155 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-199, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.155 pool-9-thread-84 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.156 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-199, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.158 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.160 pool-9-thread-84 INFO Metadata: [Consumer clientId=consumer-groupId-199, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.161 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.161 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.161 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919097161
21/02/21 22:51:37.161 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-200, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.161 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.161 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-200, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.164 pool-9-thread-87 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.165 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-200, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.167 pool-9-thread-87 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.168 pool-9-thread-87 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.169 pool-9-thread-87 INFO AppInfoParser: Kafka startTimeMs: 1613919097167
21/02/21 22:51:37.169 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-201, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.169 pool-9-thread-87 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.169 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-201, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.171 pool-9-thread-83 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.173 pool-9-thread-87 INFO Metadata: [Consumer clientId=consumer-groupId-201, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.175 pool-9-thread-83 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.175 pool-9-thread-83 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.175 pool-9-thread-83 INFO AppInfoParser: Kafka startTimeMs: 1613919097175
21/02/21 22:51:37.175 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-202, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.175 pool-9-thread-83 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.175 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-202, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.176 pool-9-thread-85 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.178 pool-9-thread-85 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.178 pool-9-thread-85 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.178 pool-9-thread-85 INFO AppInfoParser: Kafka startTimeMs: 1613919097178
21/02/21 22:51:37.178 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-203, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.179 pool-9-thread-85 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.179 pool-9-thread-83 INFO Metadata: [Consumer clientId=consumer-groupId-202, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.179 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-203, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.181 pool-9-thread-86 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.183 pool-9-thread-85 INFO Metadata: [Consumer clientId=consumer-groupId-203, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.185 pool-9-thread-86 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.185 pool-9-thread-86 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.185 pool-9-thread-86 INFO AppInfoParser: Kafka startTimeMs: 1613919097185
21/02/21 22:51:37.185 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-204, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.186 pool-9-thread-86 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.186 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-204, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.186 pool-9-thread-89 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.190 pool-9-thread-86 INFO Metadata: [Consumer clientId=consumer-groupId-204, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.191 pool-9-thread-89 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.192 pool-9-thread-89 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.192 pool-9-thread-89 INFO AppInfoParser: Kafka startTimeMs: 1613919097191
21/02/21 22:51:37.192 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-205, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.192 pool-9-thread-89 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.192 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-205, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.193 pool-9-thread-91 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.195 pool-9-thread-89 INFO Metadata: [Consumer clientId=consumer-groupId-205, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.197 pool-9-thread-91 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.197 pool-9-thread-91 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.197 pool-9-thread-91 INFO AppInfoParser: Kafka startTimeMs: 1613919097197
21/02/21 22:51:37.197 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-206, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.197 pool-9-thread-91 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.197 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-206, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.198 pool-9-thread-88 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.201 pool-9-thread-91 INFO Metadata: [Consumer clientId=consumer-groupId-206, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.202 pool-9-thread-88 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.202 pool-9-thread-88 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.202 pool-9-thread-88 INFO AppInfoParser: Kafka startTimeMs: 1613919097202
21/02/21 22:51:37.202 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-207, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.202 pool-9-thread-88 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.202 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-207, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.203 pool-9-thread-92 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.206 pool-9-thread-92 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.206 pool-9-thread-92 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.206 pool-9-thread-92 INFO AppInfoParser: Kafka startTimeMs: 1613919097206
21/02/21 22:51:37.207 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-208, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.207 pool-9-thread-88 INFO Metadata: [Consumer clientId=consumer-groupId-207, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.207 pool-9-thread-92 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.207 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-208, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.208 pool-9-thread-96 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.212 pool-9-thread-96 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.212 pool-9-thread-96 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.212 pool-9-thread-96 INFO AppInfoParser: Kafka startTimeMs: 1613919097212
21/02/21 22:51:37.212 pool-9-thread-92 INFO Metadata: [Consumer clientId=consumer-groupId-208, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.212 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-209, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.212 pool-9-thread-96 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.212 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-209, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.213 pool-9-thread-93 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.216 pool-9-thread-96 INFO Metadata: [Consumer clientId=consumer-groupId-209, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.216 pool-9-thread-93 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.216 pool-9-thread-93 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.216 pool-9-thread-93 INFO AppInfoParser: Kafka startTimeMs: 1613919097216
21/02/21 22:51:37.217 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-210, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.217 pool-9-thread-93 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.217 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-210, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.219 pool-9-thread-100 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.222 pool-9-thread-93 INFO Metadata: [Consumer clientId=consumer-groupId-210, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.224 pool-9-thread-100 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.224 pool-9-thread-100 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.224 pool-9-thread-100 INFO AppInfoParser: Kafka startTimeMs: 1613919097224
21/02/21 22:51:37.224 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-211, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.224 pool-9-thread-100 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.224 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-211, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.225 pool-9-thread-97 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.227 pool-9-thread-100 INFO Metadata: [Consumer clientId=consumer-groupId-211, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.229 pool-9-thread-97 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.229 pool-9-thread-97 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.229 pool-9-thread-97 INFO AppInfoParser: Kafka startTimeMs: 1613919097229
21/02/21 22:51:37.230 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-212, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.230 pool-9-thread-97 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.230 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-212, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.230 pool-9-thread-99 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.233 pool-9-thread-97 INFO Metadata: [Consumer clientId=consumer-groupId-212, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.235 pool-9-thread-99 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.235 pool-9-thread-99 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.235 pool-9-thread-99 INFO AppInfoParser: Kafka startTimeMs: 1613919097235
21/02/21 22:51:37.235 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-213, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.236 pool-9-thread-99 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.236 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-213, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.236 pool-9-thread-95 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.239 pool-9-thread-99 INFO Metadata: [Consumer clientId=consumer-groupId-213, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.241 pool-9-thread-95 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.241 pool-9-thread-95 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.241 pool-9-thread-95 INFO AppInfoParser: Kafka startTimeMs: 1613919097241
21/02/21 22:51:37.241 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-214, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.241 pool-9-thread-95 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.242 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-214, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.242 pool-9-thread-94 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.245 pool-9-thread-95 INFO Metadata: [Consumer clientId=consumer-groupId-214, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.246 pool-9-thread-94 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.246 pool-9-thread-94 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.246 pool-9-thread-94 INFO AppInfoParser: Kafka startTimeMs: 1613919097246
21/02/21 22:51:37.246 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-215, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.246 pool-9-thread-94 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.246 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-215, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.248 pool-9-thread-90 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.249 pool-9-thread-94 INFO Metadata: [Consumer clientId=consumer-groupId-215, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.251 pool-9-thread-90 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.251 pool-9-thread-90 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.251 pool-9-thread-90 INFO AppInfoParser: Kafka startTimeMs: 1613919097251
21/02/21 22:51:37.252 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-216, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.252 pool-9-thread-90 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.252 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-216, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.254 pool-9-thread-98 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.256 pool-9-thread-90 INFO Metadata: [Consumer clientId=consumer-groupId-216, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.258 pool-9-thread-98 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.258 pool-9-thread-98 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.258 pool-9-thread-98 INFO AppInfoParser: Kafka startTimeMs: 1613919097258
21/02/21 22:51:37.258 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-217, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.258 pool-9-thread-98 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.258 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-217, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.259 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.262 pool-9-thread-98 INFO Metadata: [Consumer clientId=consumer-groupId-217, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.262 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.262 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.262 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919097262
21/02/21 22:51:37.263 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-218, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.263 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.263 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-218, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.263 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.265 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-218, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.267 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.267 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.267 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919097267
21/02/21 22:51:37.267 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-219, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.267 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.267 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-219, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.268 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.270 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-219, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.273 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.273 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.273 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919097273
21/02/21 22:51:37.273 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-220, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.273 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.273 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-220, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.274 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.276 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-220, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.279 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.279 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.279 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919097279
21/02/21 22:51:37.279 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-221, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.279 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.279 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-221, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.280 pool-9-thread-69 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.283 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-221, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.284 pool-9-thread-69 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.284 pool-9-thread-69 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.284 pool-9-thread-69 INFO AppInfoParser: Kafka startTimeMs: 1613919097284
21/02/21 22:51:37.284 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-222, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.284 pool-9-thread-69 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.284 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-222, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.285 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.289 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.289 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.289 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919097289
21/02/21 22:51:37.289 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-223, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.289 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.289 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-223, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.290 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.290 pool-9-thread-69 INFO Metadata: [Consumer clientId=consumer-groupId-222, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.293 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.293 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.293 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919097293
21/02/21 22:51:37.293 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-224, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.294 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.294 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-224, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.296 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.298 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-223, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.299 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-224, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.301 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.301 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.301 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919097300
21/02/21 22:51:37.301 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-225, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.302 pool-9-thread-74 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.303 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.303 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-225, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.517 pool-9-thread-74 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.517 pool-9-thread-74 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.517 pool-9-thread-74 INFO AppInfoParser: Kafka startTimeMs: 1613919097517
21/02/21 22:51:37.518 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-226, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.518 pool-9-thread-74 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.518 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-226, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.518 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-225, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.519 pool-9-thread-67 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.521 pool-9-thread-67 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.521 pool-9-thread-67 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.521 pool-9-thread-67 INFO AppInfoParser: Kafka startTimeMs: 1613919097521
21/02/21 22:51:37.521 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-227, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.522 pool-9-thread-74 INFO Metadata: [Consumer clientId=consumer-groupId-226, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.522 pool-9-thread-67 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.522 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-227, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.523 pool-9-thread-78 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.527 pool-9-thread-78 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.527 pool-9-thread-78 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.527 pool-9-thread-78 INFO AppInfoParser: Kafka startTimeMs: 1613919097527
21/02/21 22:51:37.527 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-228, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.527 pool-9-thread-67 INFO Metadata: [Consumer clientId=consumer-groupId-227, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.527 pool-9-thread-78 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.528 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-228, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.528 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.532 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.532 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.532 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919097532
21/02/21 22:51:37.532 pool-9-thread-78 INFO Metadata: [Consumer clientId=consumer-groupId-228, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.532 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-229, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.533 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.533 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-229, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.534 pool-9-thread-75 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.536 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-229, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.537 pool-9-thread-75 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.537 pool-9-thread-75 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.537 pool-9-thread-75 INFO AppInfoParser: Kafka startTimeMs: 1613919097537
21/02/21 22:51:37.538 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-230, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.538 pool-9-thread-75 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.538 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-230, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.540 pool-9-thread-63 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.543 pool-9-thread-75 INFO Metadata: [Consumer clientId=consumer-groupId-230, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.545 pool-9-thread-63 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.545 pool-9-thread-63 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.545 pool-9-thread-63 INFO AppInfoParser: Kafka startTimeMs: 1613919097545
21/02/21 22:51:37.545 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-231, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.545 pool-9-thread-63 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.545 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-231, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.546 pool-9-thread-66 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.550 pool-9-thread-63 INFO Metadata: [Consumer clientId=consumer-groupId-231, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.551 pool-9-thread-66 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.551 pool-9-thread-66 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.551 pool-9-thread-66 INFO AppInfoParser: Kafka startTimeMs: 1613919097551
21/02/21 22:51:37.551 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-232, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.551 pool-9-thread-66 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.552 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-232, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.552 pool-9-thread-80 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.555 pool-9-thread-66 INFO Metadata: [Consumer clientId=consumer-groupId-232, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.555 pool-9-thread-80 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.556 pool-9-thread-80 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.556 pool-9-thread-80 INFO AppInfoParser: Kafka startTimeMs: 1613919097555
21/02/21 22:51:37.556 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-233, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.556 pool-9-thread-80 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.556 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-233, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.556 pool-9-thread-60 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.559 pool-9-thread-80 INFO Metadata: [Consumer clientId=consumer-groupId-233, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.560 pool-9-thread-60 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.560 pool-9-thread-60 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.560 pool-9-thread-60 INFO AppInfoParser: Kafka startTimeMs: 1613919097560
21/02/21 22:51:37.560 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-234, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.560 pool-9-thread-60 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.560 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-234, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.561 pool-9-thread-57 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.564 pool-9-thread-60 INFO Metadata: [Consumer clientId=consumer-groupId-234, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.565 pool-9-thread-57 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.565 pool-9-thread-57 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.565 pool-9-thread-57 INFO AppInfoParser: Kafka startTimeMs: 1613919097565
21/02/21 22:51:37.565 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-235, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.566 pool-9-thread-57 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.566 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-235, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.567 pool-9-thread-56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.571 pool-9-thread-56 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.571 pool-9-thread-56 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.571 pool-9-thread-56 INFO AppInfoParser: Kafka startTimeMs: 1613919097571
21/02/21 22:51:37.571 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-236, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.571 pool-9-thread-56 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.571 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-236, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.572 pool-9-thread-57 INFO Metadata: [Consumer clientId=consumer-groupId-235, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.572 pool-9-thread-55 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.575 pool-9-thread-56 INFO Metadata: [Consumer clientId=consumer-groupId-236, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.576 pool-9-thread-55 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.576 pool-9-thread-55 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.576 pool-9-thread-55 INFO AppInfoParser: Kafka startTimeMs: 1613919097576
21/02/21 22:51:37.576 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-237, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.576 pool-9-thread-55 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.576 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-237, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.577 pool-9-thread-53 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.581 pool-9-thread-55 INFO Metadata: [Consumer clientId=consumer-groupId-237, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.581 pool-9-thread-53 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.582 pool-9-thread-53 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.582 pool-9-thread-53 INFO AppInfoParser: Kafka startTimeMs: 1613919097581
21/02/21 22:51:37.582 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-238, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.582 pool-9-thread-53 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.582 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-238, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.584 pool-9-thread-59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.586 pool-9-thread-53 INFO Metadata: [Consumer clientId=consumer-groupId-238, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.591 pool-9-thread-59 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.591 pool-9-thread-59 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.591 pool-9-thread-59 INFO AppInfoParser: Kafka startTimeMs: 1613919097591
21/02/21 22:51:37.592 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-239, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.592 pool-9-thread-59 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.592 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-239, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.592 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.596 pool-9-thread-59 INFO Metadata: [Consumer clientId=consumer-groupId-239, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.596 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.596 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.596 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919097596
21/02/21 22:51:37.596 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-240, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.596 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.597 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-240, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.597 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.600 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-240, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.602 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.603 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.603 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919097602
21/02/21 22:51:37.603 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-241, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.603 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.603 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-241, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.604 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.608 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.608 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.609 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919097608
21/02/21 22:51:37.609 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-241, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.609 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-242, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.609 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.609 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-242, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.611 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.614 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-242, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.615 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.615 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.615 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919097615
21/02/21 22:51:37.615 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-243, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.616 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.616 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-243, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.616 pool-9-thread-51 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.620 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-243, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.623 pool-9-thread-51 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.623 pool-9-thread-51 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.623 pool-9-thread-51 INFO AppInfoParser: Kafka startTimeMs: 1613919097622
21/02/21 22:51:37.623 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-244, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.623 pool-9-thread-51 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.623 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-244, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.624 pool-9-thread-54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.627 pool-9-thread-51 INFO Metadata: [Consumer clientId=consumer-groupId-244, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.628 pool-9-thread-54 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.628 pool-9-thread-54 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.628 pool-9-thread-54 INFO AppInfoParser: Kafka startTimeMs: 1613919097628
21/02/21 22:51:37.629 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-245, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.629 pool-9-thread-54 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.629 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-245, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.630 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.632 pool-9-thread-54 INFO Metadata: [Consumer clientId=consumer-groupId-245, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.633 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.633 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.633 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919097633
21/02/21 22:51:37.633 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-246, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.633 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.633 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-246, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.634 pool-9-thread-39 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.637 pool-9-thread-39 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.637 pool-9-thread-39 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.638 pool-9-thread-39 INFO AppInfoParser: Kafka startTimeMs: 1613919097637
21/02/21 22:51:37.638 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-246, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.638 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-247, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.638 pool-9-thread-39 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.638 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-247, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.639 pool-9-thread-41 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.642 pool-9-thread-39 INFO Metadata: [Consumer clientId=consumer-groupId-247, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.642 pool-9-thread-41 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.642 pool-9-thread-41 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.642 pool-9-thread-41 INFO AppInfoParser: Kafka startTimeMs: 1613919097642
21/02/21 22:51:37.642 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-248, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.643 pool-9-thread-41 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.643 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-248, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.643 pool-9-thread-47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.646 pool-9-thread-41 INFO Metadata: [Consumer clientId=consumer-groupId-248, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.647 pool-9-thread-47 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.647 pool-9-thread-47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.647 pool-9-thread-47 INFO AppInfoParser: Kafka startTimeMs: 1613919097647
21/02/21 22:51:37.647 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-249, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.647 pool-9-thread-47 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.647 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-249, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.648 pool-9-thread-49 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.651 pool-9-thread-47 INFO Metadata: [Consumer clientId=consumer-groupId-249, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.654 pool-9-thread-49 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.654 pool-9-thread-49 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.654 pool-9-thread-49 INFO AppInfoParser: Kafka startTimeMs: 1613919097654
21/02/21 22:51:37.654 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-250, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.655 pool-9-thread-49 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.655 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-250, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.655 pool-9-thread-36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.659 pool-9-thread-49 INFO Metadata: [Consumer clientId=consumer-groupId-250, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.661 pool-9-thread-36 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.661 pool-9-thread-36 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.661 pool-9-thread-36 INFO AppInfoParser: Kafka startTimeMs: 1613919097661
21/02/21 22:51:37.661 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-251, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.661 pool-9-thread-36 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.661 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-251, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.662 pool-9-thread-46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.664 pool-9-thread-36 INFO Metadata: [Consumer clientId=consumer-groupId-251, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.666 pool-9-thread-46 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.666 pool-9-thread-46 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.666 pool-9-thread-46 INFO AppInfoParser: Kafka startTimeMs: 1613919097666
21/02/21 22:51:37.666 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-252, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.667 pool-9-thread-46 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.667 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-252, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.667 pool-9-thread-42 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.671 pool-9-thread-46 INFO Metadata: [Consumer clientId=consumer-groupId-252, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.672 pool-9-thread-42 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.672 pool-9-thread-42 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.672 pool-9-thread-42 INFO AppInfoParser: Kafka startTimeMs: 1613919097672
21/02/21 22:51:37.672 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-253, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.672 pool-9-thread-42 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.672 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-253, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.673 pool-9-thread-37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.676 pool-9-thread-42 INFO Metadata: [Consumer clientId=consumer-groupId-253, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.676 pool-9-thread-37 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.676 pool-9-thread-37 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.676 pool-9-thread-37 INFO AppInfoParser: Kafka startTimeMs: 1613919097676
21/02/21 22:51:37.676 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-254, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.677 pool-9-thread-37 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.677 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-254, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.678 pool-9-thread-48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.683 pool-9-thread-37 INFO Metadata: [Consumer clientId=consumer-groupId-254, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.685 pool-9-thread-48 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.686 pool-9-thread-48 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.686 pool-9-thread-48 INFO AppInfoParser: Kafka startTimeMs: 1613919097685
21/02/21 22:51:37.686 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-255, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.686 pool-9-thread-48 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.686 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-255, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.688 pool-9-thread-21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.691 pool-9-thread-48 INFO Metadata: [Consumer clientId=consumer-groupId-255, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.694 pool-9-thread-21 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.694 pool-9-thread-21 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.694 pool-9-thread-21 INFO AppInfoParser: Kafka startTimeMs: 1613919097694
21/02/21 22:51:37.694 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-256, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.695 pool-9-thread-21 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.695 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-256, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.695 pool-9-thread-45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.698 pool-9-thread-21 INFO Metadata: [Consumer clientId=consumer-groupId-256, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.699 pool-9-thread-45 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.699 pool-9-thread-45 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.699 pool-9-thread-45 INFO AppInfoParser: Kafka startTimeMs: 1613919097699
21/02/21 22:51:37.699 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-257, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.699 pool-9-thread-45 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.699 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-257, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.700 pool-9-thread-40 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.702 pool-9-thread-45 INFO Metadata: [Consumer clientId=consumer-groupId-257, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.706 pool-9-thread-40 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.706 pool-9-thread-40 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.706 pool-9-thread-40 INFO AppInfoParser: Kafka startTimeMs: 1613919097706
21/02/21 22:51:37.706 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-258, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.706 pool-9-thread-40 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.706 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-258, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.707 pool-9-thread-44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.710 pool-9-thread-44 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.710 pool-9-thread-44 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.710 pool-9-thread-44 INFO AppInfoParser: Kafka startTimeMs: 1613919097710
21/02/21 22:51:37.711 pool-9-thread-40 INFO Metadata: [Consumer clientId=consumer-groupId-258, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.711 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-259, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.711 pool-9-thread-44 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.711 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-259, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.711 pool-9-thread-38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.714 pool-9-thread-44 INFO Metadata: [Consumer clientId=consumer-groupId-259, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.714 pool-9-thread-38 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.715 pool-9-thread-38 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.715 pool-9-thread-38 INFO AppInfoParser: Kafka startTimeMs: 1613919097714
21/02/21 22:51:37.715 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-260, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.715 pool-9-thread-38 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.715 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-260, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.717 pool-9-thread-43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.718 pool-9-thread-38 INFO Metadata: [Consumer clientId=consumer-groupId-260, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.720 pool-9-thread-43 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.721 pool-9-thread-43 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.721 pool-9-thread-43 INFO AppInfoParser: Kafka startTimeMs: 1613919097720
21/02/21 22:51:37.721 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-261, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.721 pool-9-thread-43 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.721 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-261, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.722 pool-9-thread-35 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.726 pool-9-thread-43 INFO Metadata: [Consumer clientId=consumer-groupId-261, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.728 pool-9-thread-35 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.728 pool-9-thread-35 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.728 pool-9-thread-35 INFO AppInfoParser: Kafka startTimeMs: 1613919097728
21/02/21 22:51:37.728 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-262, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.729 pool-9-thread-35 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.729 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-262, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.729 pool-9-thread-34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.732 pool-9-thread-35 INFO Metadata: [Consumer clientId=consumer-groupId-262, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.734 pool-9-thread-34 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.734 pool-9-thread-34 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.734 pool-9-thread-34 INFO AppInfoParser: Kafka startTimeMs: 1613919097734
21/02/21 22:51:37.734 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-263, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.735 pool-9-thread-34 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.735 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-263, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.735 pool-9-thread-29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.738 pool-9-thread-34 INFO Metadata: [Consumer clientId=consumer-groupId-263, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.738 pool-9-thread-29 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.738 pool-9-thread-29 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.738 pool-9-thread-29 INFO AppInfoParser: Kafka startTimeMs: 1613919097738
21/02/21 22:51:37.738 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-264, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.739 pool-9-thread-29 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.739 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-264, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.739 pool-9-thread-24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.742 pool-9-thread-29 INFO Metadata: [Consumer clientId=consumer-groupId-264, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.745 pool-9-thread-24 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.745 pool-9-thread-24 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.745 pool-9-thread-24 INFO AppInfoParser: Kafka startTimeMs: 1613919097745
21/02/21 22:51:37.745 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-265, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.745 pool-9-thread-24 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.745 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-265, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.745 pool-9-thread-30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.748 pool-9-thread-24 INFO Metadata: [Consumer clientId=consumer-groupId-265, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.749 pool-9-thread-30 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.749 pool-9-thread-30 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.749 pool-9-thread-30 INFO AppInfoParser: Kafka startTimeMs: 1613919097749
21/02/21 22:51:37.749 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-266, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.750 pool-9-thread-30 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.750 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-266, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.751 pool-9-thread-25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.753 pool-9-thread-30 INFO Metadata: [Consumer clientId=consumer-groupId-266, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.755 pool-9-thread-25 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.755 pool-9-thread-25 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.755 pool-9-thread-25 INFO AppInfoParser: Kafka startTimeMs: 1613919097755
21/02/21 22:51:37.755 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-267, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.756 pool-9-thread-25 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.756 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-267, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.756 pool-9-thread-15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.759 pool-9-thread-25 INFO Metadata: [Consumer clientId=consumer-groupId-267, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.762 pool-9-thread-15 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.762 pool-9-thread-15 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.762 pool-9-thread-15 INFO AppInfoParser: Kafka startTimeMs: 1613919097762
21/02/21 22:51:37.762 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-268, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.762 pool-9-thread-15 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.762 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-268, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.763 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.765 pool-9-thread-15 INFO Metadata: [Consumer clientId=consumer-groupId-268, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.767 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.767 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.767 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919097767
21/02/21 22:51:37.767 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-269, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.767 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.767 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-269, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.768 pool-9-thread-26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.771 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-269, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.773 pool-9-thread-26 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.773 pool-9-thread-26 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.773 pool-9-thread-26 INFO AppInfoParser: Kafka startTimeMs: 1613919097773
21/02/21 22:51:37.773 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-270, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.774 pool-9-thread-26 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.774 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-270, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.774 pool-9-thread-19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.777 pool-9-thread-26 INFO Metadata: [Consumer clientId=consumer-groupId-270, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.778 pool-9-thread-19 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.778 pool-9-thread-19 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.779 pool-9-thread-19 INFO AppInfoParser: Kafka startTimeMs: 1613919097778
21/02/21 22:51:37.779 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-271, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.779 pool-9-thread-19 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.779 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-271, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.779 pool-9-thread-31 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.782 pool-9-thread-19 INFO Metadata: [Consumer clientId=consumer-groupId-271, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.784 pool-9-thread-31 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.784 pool-9-thread-31 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.784 pool-9-thread-31 INFO AppInfoParser: Kafka startTimeMs: 1613919097784
21/02/21 22:51:37.784 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-272, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.785 pool-9-thread-31 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.785 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-272, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.785 pool-9-thread-23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.787 pool-9-thread-31 INFO Metadata: [Consumer clientId=consumer-groupId-272, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.790 pool-9-thread-23 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.790 pool-9-thread-23 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.790 pool-9-thread-23 INFO AppInfoParser: Kafka startTimeMs: 1613919097790
21/02/21 22:51:37.791 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-273, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.791 pool-9-thread-23 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.791 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-273, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.791 pool-9-thread-27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.794 pool-9-thread-23 INFO Metadata: [Consumer clientId=consumer-groupId-273, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.796 pool-9-thread-27 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.796 pool-9-thread-27 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.796 pool-9-thread-27 INFO AppInfoParser: Kafka startTimeMs: 1613919097796
21/02/21 22:51:37.796 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-274, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.796 pool-9-thread-27 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.796 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-274, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.797 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.799 pool-9-thread-27 INFO Metadata: [Consumer clientId=consumer-groupId-274, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.802 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.802 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.802 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919097802
21/02/21 22:51:37.802 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-275, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.803 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.803 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-275, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.803 pool-9-thread-22 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.807 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-275, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.808 pool-9-thread-22 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.808 pool-9-thread-22 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.808 pool-9-thread-22 INFO AppInfoParser: Kafka startTimeMs: 1613919097808
21/02/21 22:51:37.808 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-276, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.809 pool-9-thread-22 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.809 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-276, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.810 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.813 pool-9-thread-22 INFO Metadata: [Consumer clientId=consumer-groupId-276, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.814 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.815 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.815 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919097814
21/02/21 22:51:37.815 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-277, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.815 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.815 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-277, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.815 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.817 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-277, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.818 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.818 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.818 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919097818
21/02/21 22:51:37.819 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-278, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.820 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.820 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-278, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.821 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.823 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-278, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.824 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.824 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.824 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919097824
21/02/21 22:51:37.824 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-279, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.824 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.824 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-279, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.825 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.827 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-279, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.830 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.830 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.830 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919097830
21/02/21 22:51:37.830 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-280, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.831 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.831 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-280, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.831 pool-9-thread-7 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.834 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-280, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.836 pool-9-thread-7 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.836 pool-9-thread-7 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.836 pool-9-thread-7 INFO AppInfoParser: Kafka startTimeMs: 1613919097836
21/02/21 22:51:37.836 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-281, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.836 pool-9-thread-7 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.836 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-281, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.836 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.838 pool-9-thread-7 INFO Metadata: [Consumer clientId=consumer-groupId-281, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.840 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.840 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.840 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919097840
21/02/21 22:51:37.841 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-282, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.841 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.841 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-282, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.841 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.845 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-282, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.845 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.845 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.845 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919097845
21/02/21 22:51:37.845 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-283, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.846 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.846 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-283, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.846 pool-9-thread-10 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.849 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-283, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.851 pool-9-thread-10 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.851 pool-9-thread-10 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.851 pool-9-thread-10 INFO AppInfoParser: Kafka startTimeMs: 1613919097851
21/02/21 22:51:37.851 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-284, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.851 pool-9-thread-10 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.851 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-284, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.851 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.853 pool-9-thread-10 INFO Metadata: [Consumer clientId=consumer-groupId-284, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.856 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.856 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.856 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919097856
21/02/21 22:51:37.856 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-285, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.856 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.856 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-285, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.856 pool-9-thread-16 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.860 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-285, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.861 pool-9-thread-16 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.862 pool-9-thread-16 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.862 pool-9-thread-16 INFO AppInfoParser: Kafka startTimeMs: 1613919097861
21/02/21 22:51:37.862 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-286, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.862 pool-9-thread-16 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.862 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-286, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.863 pool-9-thread-12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.865 pool-9-thread-16 INFO Metadata: [Consumer clientId=consumer-groupId-286, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.866 pool-9-thread-12 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.866 pool-9-thread-12 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.866 pool-9-thread-12 INFO AppInfoParser: Kafka startTimeMs: 1613919097866
21/02/21 22:51:37.867 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-287, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.867 pool-9-thread-12 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.867 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-287, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.867 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.871 pool-9-thread-12 INFO Metadata: [Consumer clientId=consumer-groupId-287, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.873 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.873 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.873 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919097873
21/02/21 22:51:37.873 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-288, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.873 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.873 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-288, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.874 pool-9-thread-17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.876 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-288, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.879 pool-9-thread-17 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.879 pool-9-thread-17 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.879 pool-9-thread-17 INFO AppInfoParser: Kafka startTimeMs: 1613919097879
21/02/21 22:51:37.879 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-289, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.880 pool-9-thread-17 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.880 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-289, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.881 pool-9-thread-13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.883 pool-9-thread-17 INFO Metadata: [Consumer clientId=consumer-groupId-289, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.885 pool-9-thread-13 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.885 pool-9-thread-13 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.885 pool-9-thread-13 INFO AppInfoParser: Kafka startTimeMs: 1613919097885
21/02/21 22:51:37.885 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-290, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.885 pool-9-thread-13 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.886 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-290, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.886 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.888 pool-9-thread-13 INFO Metadata: [Consumer clientId=consumer-groupId-290, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.892 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.892 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.892 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919097892
21/02/21 22:51:37.892 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-291, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.893 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.893 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-291, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.894 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.895 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-291, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.897 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.897 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.897 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919097897
21/02/21 22:51:37.897 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-292, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.897 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.897 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-292, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.899 pool-9-thread-9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.902 pool-9-thread-9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.902 pool-9-thread-9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.902 pool-9-thread-9 INFO AppInfoParser: Kafka startTimeMs: 1613919097902
21/02/21 22:51:37.902 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-293, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.902 pool-9-thread-9 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.902 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-293, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.903 pool-9-thread-4 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.906 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-292, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.906 pool-9-thread-9 INFO Metadata: [Consumer clientId=consumer-groupId-293, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.907 pool-9-thread-4 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.907 pool-9-thread-4 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.907 pool-9-thread-4 INFO AppInfoParser: Kafka startTimeMs: 1613919097907
21/02/21 22:51:37.907 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-294, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.907 pool-9-thread-4 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.907 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-294, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.909 pool-9-thread-5 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.912 pool-9-thread-4 INFO Metadata: [Consumer clientId=consumer-groupId-294, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.915 pool-9-thread-5 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.915 pool-9-thread-5 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.915 pool-9-thread-5 INFO AppInfoParser: Kafka startTimeMs: 1613919097915
21/02/21 22:51:37.915 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-295, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.915 pool-9-thread-5 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.915 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-295, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.916 pool-9-thread-9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.919 pool-9-thread-5 INFO Metadata: [Consumer clientId=consumer-groupId-295, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.919 pool-9-thread-9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.919 pool-9-thread-9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.919 pool-9-thread-9 INFO AppInfoParser: Kafka startTimeMs: 1613919097919
21/02/21 22:51:37.919 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-296, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.920 pool-9-thread-9 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.920 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-296, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.921 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.924 pool-9-thread-9 INFO Metadata: [Consumer clientId=consumer-groupId-296, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.924 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.924 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.924 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919097924
21/02/21 22:51:37.924 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-297, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.924 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.924 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-297, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.925 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.927 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-297, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.927 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.927 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.927 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919097927
21/02/21 22:51:37.927 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-298, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.927 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.927 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-298, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.930 pool-9-thread-13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.932 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-298, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.933 pool-9-thread-13 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.933 pool-9-thread-13 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.933 pool-9-thread-13 INFO AppInfoParser: Kafka startTimeMs: 1613919097933
21/02/21 22:51:37.933 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-299, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.934 pool-9-thread-13 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.934 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-299, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.934 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.937 pool-9-thread-13 INFO Metadata: [Consumer clientId=consumer-groupId-299, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.938 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.938 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.938 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919097938
21/02/21 22:51:37.938 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-300, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.938 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.938 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-300, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.939 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.941 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-300, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.942 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.942 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.942 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919097942
21/02/21 22:51:37.943 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-301, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.943 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.943 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-301, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.943 pool-9-thread-17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.946 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-301, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.949 pool-9-thread-17 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.949 pool-9-thread-17 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.949 pool-9-thread-17 INFO AppInfoParser: Kafka startTimeMs: 1613919097948
21/02/21 22:51:37.949 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-302, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.949 pool-9-thread-17 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.949 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-302, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.950 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.952 pool-9-thread-17 INFO Metadata: [Consumer clientId=consumer-groupId-302, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.954 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.954 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.954 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919097954
21/02/21 22:51:37.954 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-303, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.955 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.955 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-303, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.956 pool-9-thread-12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.959 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-303, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.961 pool-9-thread-12 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.961 pool-9-thread-12 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.961 pool-9-thread-12 INFO AppInfoParser: Kafka startTimeMs: 1613919097961
21/02/21 22:51:37.961 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-304, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.961 pool-9-thread-12 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.961 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-304, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.962 pool-9-thread-16 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.964 pool-9-thread-12 INFO Metadata: [Consumer clientId=consumer-groupId-304, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.966 pool-9-thread-16 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.966 pool-9-thread-16 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.967 pool-9-thread-16 INFO AppInfoParser: Kafka startTimeMs: 1613919097966
21/02/21 22:51:37.967 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-305, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.967 pool-9-thread-16 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.967 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-305, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.967 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.970 pool-9-thread-16 INFO Metadata: [Consumer clientId=consumer-groupId-305, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.972 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.972 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.972 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919097972
21/02/21 22:51:37.972 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-306, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.972 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.972 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-306, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.973 pool-9-thread-10 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.975 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-306, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.978 pool-9-thread-10 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.978 pool-9-thread-10 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.978 pool-9-thread-10 INFO AppInfoParser: Kafka startTimeMs: 1613919097978
21/02/21 22:51:37.978 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-307, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.979 pool-9-thread-10 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.979 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-307, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.979 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.982 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.982 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.982 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919097982
21/02/21 22:51:37.982 pool-9-thread-10 INFO Metadata: [Consumer clientId=consumer-groupId-307, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.982 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-308, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.982 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.982 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-308, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.983 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.985 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-308, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.985 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.985 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.985 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919097985
21/02/21 22:51:37.985 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-309, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.986 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.986 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-309, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.987 pool-9-thread-7 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.988 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-309, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.991 pool-9-thread-7 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.991 pool-9-thread-7 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.991 pool-9-thread-7 INFO AppInfoParser: Kafka startTimeMs: 1613919097991
21/02/21 22:51:37.991 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-310, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.991 pool-9-thread-7 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.991 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-310, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.992 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.995 pool-9-thread-7 INFO Metadata: [Consumer clientId=consumer-groupId-310, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:37.995 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:37.995 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:37.995 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919097995
21/02/21 22:51:37.995 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-311, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:37.996 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:37.996 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-311, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:37.996 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:37.997 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-311, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.000 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.000 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.000 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919098000
21/02/21 22:51:38.000 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-312, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.000 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.001 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-312, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.002 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.003 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-312, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.004 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.004 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.004 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919098004
21/02/21 22:51:38.004 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-313, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.004 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.004 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-313, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.005 pool-9-thread-22 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.008 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-313, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.008 pool-9-thread-22 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.008 pool-9-thread-22 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.008 pool-9-thread-22 INFO AppInfoParser: Kafka startTimeMs: 1613919098008
21/02/21 22:51:38.009 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-314, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.009 pool-9-thread-22 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.009 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-314, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.009 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.012 pool-9-thread-22 INFO Metadata: [Consumer clientId=consumer-groupId-314, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.014 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.014 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.015 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919098014
21/02/21 22:51:38.015 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-315, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.015 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.015 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-315, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.015 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.018 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-315, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.018 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.018 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.018 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919098018
21/02/21 22:51:38.018 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-316, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.018 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.019 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-316, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.021 pool-9-thread-27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.023 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-316, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.025 pool-9-thread-27 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.025 pool-9-thread-27 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.025 pool-9-thread-27 INFO AppInfoParser: Kafka startTimeMs: 1613919098025
21/02/21 22:51:38.025 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-317, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.025 pool-9-thread-27 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.025 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-317, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.026 pool-9-thread-23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.029 pool-9-thread-27 INFO Metadata: [Consumer clientId=consumer-groupId-317, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.031 pool-9-thread-23 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.031 pool-9-thread-23 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.031 pool-9-thread-23 INFO AppInfoParser: Kafka startTimeMs: 1613919098031
21/02/21 22:51:38.031 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-318, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.031 pool-9-thread-23 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.031 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-318, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.032 pool-9-thread-31 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.034 pool-9-thread-23 INFO Metadata: [Consumer clientId=consumer-groupId-318, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.035 pool-9-thread-31 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.035 pool-9-thread-31 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.035 pool-9-thread-31 INFO AppInfoParser: Kafka startTimeMs: 1613919098035
21/02/21 22:51:38.035 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-319, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.036 pool-9-thread-31 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.036 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-319, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.036 pool-9-thread-19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.038 pool-9-thread-31 INFO Metadata: [Consumer clientId=consumer-groupId-319, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.042 pool-9-thread-19 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.042 pool-9-thread-19 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.042 pool-9-thread-19 INFO AppInfoParser: Kafka startTimeMs: 1613919098042
21/02/21 22:51:38.042 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-320, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.042 pool-9-thread-19 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.042 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-320, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.043 pool-9-thread-26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.045 pool-9-thread-19 INFO Metadata: [Consumer clientId=consumer-groupId-320, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.047 pool-9-thread-26 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.047 pool-9-thread-26 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.047 pool-9-thread-26 INFO AppInfoParser: Kafka startTimeMs: 1613919098047
21/02/21 22:51:38.047 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-321, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.047 pool-9-thread-26 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.047 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-321, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.048 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.050 pool-9-thread-26 INFO Metadata: [Consumer clientId=consumer-groupId-321, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.053 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.053 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.053 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919098053
21/02/21 22:51:38.053 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-322, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.053 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.053 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-322, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.055 pool-9-thread-15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.056 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-322, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.059 pool-9-thread-15 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.059 pool-9-thread-15 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.059 pool-9-thread-15 INFO AppInfoParser: Kafka startTimeMs: 1613919098059
21/02/21 22:51:38.059 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-323, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.059 pool-9-thread-15 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.059 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-323, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.060 pool-9-thread-25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.062 pool-9-thread-15 INFO Metadata: [Consumer clientId=consumer-groupId-323, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.063 pool-9-thread-25 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.063 pool-9-thread-25 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.063 pool-9-thread-25 INFO AppInfoParser: Kafka startTimeMs: 1613919098063
21/02/21 22:51:38.063 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-324, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.063 pool-9-thread-25 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.063 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-324, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.065 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.066 pool-9-thread-25 INFO Metadata: [Consumer clientId=consumer-groupId-324, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.068 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.068 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.068 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919098068
21/02/21 22:51:38.069 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-325, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.069 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.069 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-325, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.070 pool-9-thread-30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.072 pool-9-thread-30 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.072 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-325, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.073 pool-9-thread-30 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.073 pool-9-thread-30 INFO AppInfoParser: Kafka startTimeMs: 1613919098072
21/02/21 22:51:38.074 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-326, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.074 pool-9-thread-30 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.074 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-326, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.075 pool-9-thread-24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.077 pool-9-thread-30 INFO Metadata: [Consumer clientId=consumer-groupId-326, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.080 pool-9-thread-24 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.080 pool-9-thread-24 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.080 pool-9-thread-24 INFO AppInfoParser: Kafka startTimeMs: 1613919098080
21/02/21 22:51:38.081 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-327, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.081 pool-9-thread-24 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.081 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-327, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.081 pool-9-thread-29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.084 pool-9-thread-24 INFO Metadata: [Consumer clientId=consumer-groupId-327, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.085 pool-9-thread-29 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.085 pool-9-thread-29 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.085 pool-9-thread-29 INFO AppInfoParser: Kafka startTimeMs: 1613919098085
21/02/21 22:51:38.085 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-328, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.085 pool-9-thread-29 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.085 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-328, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.086 pool-9-thread-34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.088 pool-9-thread-29 INFO Metadata: [Consumer clientId=consumer-groupId-328, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.089 pool-9-thread-34 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.089 pool-9-thread-34 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.090 pool-9-thread-34 INFO AppInfoParser: Kafka startTimeMs: 1613919098089
21/02/21 22:51:38.090 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-329, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.090 pool-9-thread-34 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.090 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-329, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.090 pool-9-thread-35 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.092 pool-9-thread-34 INFO Metadata: [Consumer clientId=consumer-groupId-329, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.093 pool-9-thread-35 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.093 pool-9-thread-35 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.093 pool-9-thread-35 INFO AppInfoParser: Kafka startTimeMs: 1613919098093
21/02/21 22:51:38.093 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-330, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.094 pool-9-thread-35 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.094 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-330, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.094 pool-9-thread-43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.096 pool-9-thread-35 INFO Metadata: [Consumer clientId=consumer-groupId-330, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.097 pool-9-thread-43 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.098 pool-9-thread-43 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.098 pool-9-thread-43 INFO AppInfoParser: Kafka startTimeMs: 1613919098097
21/02/21 22:51:38.098 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-331, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.098 pool-9-thread-43 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.098 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-331, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.098 pool-9-thread-38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.101 pool-9-thread-43 INFO Metadata: [Consumer clientId=consumer-groupId-331, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.102 pool-9-thread-38 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.103 pool-9-thread-38 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.103 pool-9-thread-38 INFO AppInfoParser: Kafka startTimeMs: 1613919098102
21/02/21 22:51:38.103 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-332, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.103 pool-9-thread-38 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.103 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-332, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.104 pool-9-thread-44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.107 pool-9-thread-38 INFO Metadata: [Consumer clientId=consumer-groupId-332, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.108 pool-9-thread-44 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.108 pool-9-thread-44 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.108 pool-9-thread-44 INFO AppInfoParser: Kafka startTimeMs: 1613919098108
21/02/21 22:51:38.108 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-333, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.109 pool-9-thread-44 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.109 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-333, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.110 pool-9-thread-40 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.111 pool-9-thread-44 INFO Metadata: [Consumer clientId=consumer-groupId-333, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.114 pool-9-thread-40 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.114 pool-9-thread-40 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.114 pool-9-thread-40 INFO AppInfoParser: Kafka startTimeMs: 1613919098114
21/02/21 22:51:38.114 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-334, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.114 pool-9-thread-40 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.114 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-334, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.115 pool-9-thread-45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.117 pool-9-thread-40 INFO Metadata: [Consumer clientId=consumer-groupId-334, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.118 pool-9-thread-45 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.118 pool-9-thread-45 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.118 pool-9-thread-45 INFO AppInfoParser: Kafka startTimeMs: 1613919098118
21/02/21 22:51:38.119 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-335, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.119 pool-9-thread-45 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.119 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-335, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.119 pool-9-thread-21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.122 pool-9-thread-21 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.122 pool-9-thread-21 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.122 pool-9-thread-21 INFO AppInfoParser: Kafka startTimeMs: 1613919098122
21/02/21 22:51:38.122 pool-9-thread-45 INFO Metadata: [Consumer clientId=consumer-groupId-335, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.122 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-336, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.123 pool-9-thread-21 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.123 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-336, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.123 pool-9-thread-48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.126 pool-9-thread-21 INFO Metadata: [Consumer clientId=consumer-groupId-336, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.127 pool-9-thread-48 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.127 pool-9-thread-48 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.127 pool-9-thread-48 INFO AppInfoParser: Kafka startTimeMs: 1613919098127
21/02/21 22:51:38.127 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-337, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.127 pool-9-thread-48 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.127 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-337, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.128 pool-9-thread-37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.130 pool-9-thread-48 INFO Metadata: [Consumer clientId=consumer-groupId-337, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.133 pool-9-thread-37 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.133 pool-9-thread-37 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.133 pool-9-thread-37 INFO AppInfoParser: Kafka startTimeMs: 1613919098133
21/02/21 22:51:38.133 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-338, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.133 pool-9-thread-37 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.133 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-338, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.134 pool-9-thread-42 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.136 pool-9-thread-37 INFO Metadata: [Consumer clientId=consumer-groupId-338, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.137 pool-9-thread-42 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.137 pool-9-thread-42 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.137 pool-9-thread-42 INFO AppInfoParser: Kafka startTimeMs: 1613919098137
21/02/21 22:51:38.137 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-339, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.137 pool-9-thread-42 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.138 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-339, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.138 pool-9-thread-46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.140 pool-9-thread-42 INFO Metadata: [Consumer clientId=consumer-groupId-339, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.142 pool-9-thread-46 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.143 pool-9-thread-46 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.143 pool-9-thread-46 INFO AppInfoParser: Kafka startTimeMs: 1613919098142
21/02/21 22:51:38.143 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-340, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.143 pool-9-thread-46 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.143 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-340, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.144 pool-9-thread-36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.146 pool-9-thread-46 INFO Metadata: [Consumer clientId=consumer-groupId-340, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.148 pool-9-thread-36 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.148 pool-9-thread-36 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.148 pool-9-thread-36 INFO AppInfoParser: Kafka startTimeMs: 1613919098148
21/02/21 22:51:38.148 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-341, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.148 pool-9-thread-36 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.148 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-341, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.149 pool-9-thread-49 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.151 pool-9-thread-36 INFO Metadata: [Consumer clientId=consumer-groupId-341, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.152 pool-9-thread-49 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.152 pool-9-thread-49 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.152 pool-9-thread-49 INFO AppInfoParser: Kafka startTimeMs: 1613919098152
21/02/21 22:51:38.152 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-342, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.152 pool-9-thread-49 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.152 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-342, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.153 pool-9-thread-47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.155 pool-9-thread-49 INFO Metadata: [Consumer clientId=consumer-groupId-342, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.157 pool-9-thread-47 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.158 pool-9-thread-47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.158 pool-9-thread-47 INFO AppInfoParser: Kafka startTimeMs: 1613919098157
21/02/21 22:51:38.158 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-343, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.158 pool-9-thread-47 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.158 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-343, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.159 pool-9-thread-41 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.161 pool-9-thread-47 INFO Metadata: [Consumer clientId=consumer-groupId-343, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.162 pool-9-thread-41 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.162 pool-9-thread-41 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.162 pool-9-thread-41 INFO AppInfoParser: Kafka startTimeMs: 1613919098162
21/02/21 22:51:38.162 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-344, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.162 pool-9-thread-41 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.162 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-344, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.163 pool-9-thread-39 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.166 pool-9-thread-39 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.166 pool-9-thread-39 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.166 pool-9-thread-39 INFO AppInfoParser: Kafka startTimeMs: 1613919098166
21/02/21 22:51:38.166 pool-9-thread-41 INFO Metadata: [Consumer clientId=consumer-groupId-344, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.166 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-345, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.166 pool-9-thread-39 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.166 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-345, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.168 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.169 pool-9-thread-39 INFO Metadata: [Consumer clientId=consumer-groupId-345, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.172 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.172 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.172 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919098172
21/02/21 22:51:38.172 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-346, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.172 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.172 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-346, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.173 pool-9-thread-54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.176 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-346, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.177 pool-9-thread-54 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.177 pool-9-thread-54 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.177 pool-9-thread-54 INFO AppInfoParser: Kafka startTimeMs: 1613919098177
21/02/21 22:51:38.177 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-347, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.178 pool-9-thread-54 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.178 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-347, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.178 pool-9-thread-51 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.182 pool-9-thread-54 INFO Metadata: [Consumer clientId=consumer-groupId-347, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.183 pool-9-thread-51 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.183 pool-9-thread-51 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.183 pool-9-thread-51 INFO AppInfoParser: Kafka startTimeMs: 1613919098183
21/02/21 22:51:38.183 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-348, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.183 pool-9-thread-51 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.183 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-348, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.184 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.186 pool-9-thread-51 INFO Metadata: [Consumer clientId=consumer-groupId-348, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.189 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.189 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.189 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919098189
21/02/21 22:51:38.189 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-349, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.189 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.189 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-349, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.190 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.191 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-349, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.192 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.192 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.192 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919098192
21/02/21 22:51:38.192 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-350, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.192 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.192 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-350, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.193 pool-9-thread-59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.195 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-350, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.197 pool-9-thread-59 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.197 pool-9-thread-59 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.197 pool-9-thread-59 INFO AppInfoParser: Kafka startTimeMs: 1613919098197
21/02/21 22:51:38.197 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-351, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.197 pool-9-thread-59 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.197 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-351, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.197 pool-9-thread-53 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.200 pool-9-thread-59 INFO Metadata: [Consumer clientId=consumer-groupId-351, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.202 pool-9-thread-53 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.202 pool-9-thread-53 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.202 pool-9-thread-53 INFO AppInfoParser: Kafka startTimeMs: 1613919098202
21/02/21 22:51:38.202 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-352, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.202 pool-9-thread-53 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.202 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-352, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.203 pool-9-thread-55 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.205 pool-9-thread-53 INFO Metadata: [Consumer clientId=consumer-groupId-352, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.208 pool-9-thread-55 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.208 pool-9-thread-55 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.208 pool-9-thread-55 INFO AppInfoParser: Kafka startTimeMs: 1613919098208
21/02/21 22:51:38.208 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-353, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.208 pool-9-thread-55 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.208 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-353, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.209 pool-9-thread-56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.211 pool-9-thread-56 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.211 pool-9-thread-56 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.211 pool-9-thread-56 INFO AppInfoParser: Kafka startTimeMs: 1613919098211
21/02/21 22:51:38.211 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-354, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.212 pool-9-thread-56 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.212 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-354, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.212 pool-9-thread-55 INFO Metadata: [Consumer clientId=consumer-groupId-353, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.213 pool-9-thread-57 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.215 pool-9-thread-56 INFO Metadata: [Consumer clientId=consumer-groupId-354, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.217 pool-9-thread-57 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.217 pool-9-thread-57 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.217 pool-9-thread-57 INFO AppInfoParser: Kafka startTimeMs: 1613919098217
21/02/21 22:51:38.218 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-355, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.218 pool-9-thread-57 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.218 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-355, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.218 pool-9-thread-60 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.221 pool-9-thread-57 INFO Metadata: [Consumer clientId=consumer-groupId-355, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.223 pool-9-thread-60 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.224 pool-9-thread-60 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.224 pool-9-thread-60 INFO AppInfoParser: Kafka startTimeMs: 1613919098223
21/02/21 22:51:38.224 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-356, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.224 pool-9-thread-60 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.224 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-356, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.224 pool-9-thread-80 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.227 pool-9-thread-60 INFO Metadata: [Consumer clientId=consumer-groupId-356, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.230 pool-9-thread-80 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.230 pool-9-thread-80 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.230 pool-9-thread-80 INFO AppInfoParser: Kafka startTimeMs: 1613919098230
21/02/21 22:51:38.230 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-357, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.230 pool-9-thread-80 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.230 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-357, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.230 pool-9-thread-66 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.232 pool-9-thread-80 INFO Metadata: [Consumer clientId=consumer-groupId-357, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.233 pool-9-thread-66 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.233 pool-9-thread-66 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.233 pool-9-thread-66 INFO AppInfoParser: Kafka startTimeMs: 1613919098233
21/02/21 22:51:38.233 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-358, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.234 pool-9-thread-66 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.234 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-358, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.234 pool-9-thread-63 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.236 pool-9-thread-66 INFO Metadata: [Consumer clientId=consumer-groupId-358, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.238 pool-9-thread-63 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.238 pool-9-thread-63 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.238 pool-9-thread-63 INFO AppInfoParser: Kafka startTimeMs: 1613919098238
21/02/21 22:51:38.238 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-359, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.238 pool-9-thread-63 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.238 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-359, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.239 pool-9-thread-75 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.240 pool-9-thread-63 INFO Metadata: [Consumer clientId=consumer-groupId-359, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.244 pool-9-thread-75 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.244 pool-9-thread-75 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.244 pool-9-thread-75 INFO AppInfoParser: Kafka startTimeMs: 1613919098244
21/02/21 22:51:38.244 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-360, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.244 pool-9-thread-75 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.244 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-360, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.244 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.246 pool-9-thread-75 INFO Metadata: [Consumer clientId=consumer-groupId-360, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.249 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.249 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.249 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919098249
21/02/21 22:51:38.250 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-361, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.250 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.250 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-361, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.250 pool-9-thread-78 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.252 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-361, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.255 pool-9-thread-78 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.255 pool-9-thread-78 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.255 pool-9-thread-78 INFO AppInfoParser: Kafka startTimeMs: 1613919098255
21/02/21 22:51:38.255 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-362, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.255 pool-9-thread-78 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.255 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-362, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.256 pool-9-thread-67 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.258 pool-9-thread-78 INFO Metadata: [Consumer clientId=consumer-groupId-362, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.260 pool-9-thread-67 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.260 pool-9-thread-67 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.260 pool-9-thread-67 INFO AppInfoParser: Kafka startTimeMs: 1613919098260
21/02/21 22:51:38.261 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-363, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.261 pool-9-thread-67 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.261 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-363, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.261 pool-9-thread-74 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.264 pool-9-thread-67 INFO Metadata: [Consumer clientId=consumer-groupId-363, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.266 pool-9-thread-74 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.266 pool-9-thread-74 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.266 pool-9-thread-74 INFO AppInfoParser: Kafka startTimeMs: 1613919098266
21/02/21 22:51:38.266 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-364, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.266 pool-9-thread-74 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.266 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-364, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.266 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.269 pool-9-thread-74 INFO Metadata: [Consumer clientId=consumer-groupId-364, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.271 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.271 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.271 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919098271
21/02/21 22:51:38.271 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-365, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.271 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.271 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-365, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.272 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.274 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-365, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.276 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.276 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.276 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919098276
21/02/21 22:51:38.276 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-366, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.276 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.276 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-366, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.276 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.279 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-366, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.281 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.281 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.281 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919098281
21/02/21 22:51:38.281 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-367, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.281 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.281 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-367, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.282 pool-9-thread-69 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.284 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-367, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.286 pool-9-thread-69 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.286 pool-9-thread-69 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.286 pool-9-thread-69 INFO AppInfoParser: Kafka startTimeMs: 1613919098286
21/02/21 22:51:38.286 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-368, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.286 pool-9-thread-69 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.286 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-368, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.286 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.288 pool-9-thread-69 INFO Metadata: [Consumer clientId=consumer-groupId-368, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.292 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.292 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.292 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919098292
21/02/21 22:51:38.292 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-369, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.292 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.292 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-369, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.293 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.295 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-369, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.297 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.297 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.297 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919098297
21/02/21 22:51:38.298 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-370, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.298 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.298 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-370, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.298 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.304 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.304 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.304 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919098304
21/02/21 22:51:38.304 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-371, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.304 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.304 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-371, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.305 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-370, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.309 pool-9-thread-98 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.311 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-371, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.314 pool-9-thread-98 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.314 pool-9-thread-98 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.314 pool-9-thread-98 INFO AppInfoParser: Kafka startTimeMs: 1613919098314
21/02/21 22:51:38.314 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-372, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.315 pool-9-thread-98 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.315 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-372, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.316 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.320 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.320 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.320 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919098320
21/02/21 22:51:38.321 pool-9-thread-98 INFO Metadata: [Consumer clientId=consumer-groupId-372, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.321 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-373, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.321 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.321 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-373, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.322 pool-9-thread-90 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.323 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-373, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.328 pool-9-thread-90 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.329 pool-9-thread-90 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.329 pool-9-thread-90 INFO AppInfoParser: Kafka startTimeMs: 1613919098328
21/02/21 22:51:38.329 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-374, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.329 pool-9-thread-90 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.329 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-374, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.330 pool-9-thread-94 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.332 pool-9-thread-90 INFO Metadata: [Consumer clientId=consumer-groupId-374, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.334 pool-9-thread-94 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.334 pool-9-thread-94 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.334 pool-9-thread-94 INFO AppInfoParser: Kafka startTimeMs: 1613919098334
21/02/21 22:51:38.334 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-375, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.335 pool-9-thread-94 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.335 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-375, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.335 pool-9-thread-95 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.338 pool-9-thread-94 INFO Metadata: [Consumer clientId=consumer-groupId-375, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.341 pool-9-thread-95 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.341 pool-9-thread-95 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.341 pool-9-thread-95 INFO AppInfoParser: Kafka startTimeMs: 1613919098341
21/02/21 22:51:38.341 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-376, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.341 pool-9-thread-95 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.342 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-376, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.344 pool-9-thread-99 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.346 pool-9-thread-95 INFO Metadata: [Consumer clientId=consumer-groupId-376, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.348 pool-9-thread-99 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.349 pool-9-thread-99 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.349 pool-9-thread-99 INFO AppInfoParser: Kafka startTimeMs: 1613919098348
21/02/21 22:51:38.349 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-377, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.349 pool-9-thread-99 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.349 pool-9-thread-97 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.350 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-377, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.354 pool-9-thread-97 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.354 pool-9-thread-99 INFO Metadata: [Consumer clientId=consumer-groupId-377, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.354 pool-9-thread-97 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.354 pool-9-thread-97 INFO AppInfoParser: Kafka startTimeMs: 1613919098354
21/02/21 22:51:38.354 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-378, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.354 pool-9-thread-97 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.354 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-378, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.355 pool-9-thread-100 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.358 pool-9-thread-100 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.358 pool-9-thread-100 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.358 pool-9-thread-100 INFO AppInfoParser: Kafka startTimeMs: 1613919098358
21/02/21 22:51:38.358 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-379, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.358 pool-9-thread-100 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.358 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-379, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.358 pool-9-thread-97 INFO Metadata: [Consumer clientId=consumer-groupId-378, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.360 pool-9-thread-93 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.362 pool-9-thread-100 INFO Metadata: [Consumer clientId=consumer-groupId-379, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.367 pool-9-thread-93 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.367 pool-9-thread-93 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.367 pool-9-thread-93 INFO AppInfoParser: Kafka startTimeMs: 1613919098365
21/02/21 22:51:38.368 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-380, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.368 pool-9-thread-93 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.368 pool-9-thread-93 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-380, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.368 pool-9-thread-96 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.370 pool-9-thread-93 INFO Metadata: [Consumer clientId=consumer-groupId-380, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.373 pool-9-thread-96 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.373 pool-9-thread-96 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.374 pool-9-thread-96 INFO AppInfoParser: Kafka startTimeMs: 1613919098373
21/02/21 22:51:38.374 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-381, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.374 pool-9-thread-96 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.374 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-381, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.375 pool-9-thread-92 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.376 pool-9-thread-96 INFO Metadata: [Consumer clientId=consumer-groupId-381, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.377 pool-9-thread-92 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.377 pool-9-thread-92 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.377 pool-9-thread-92 INFO AppInfoParser: Kafka startTimeMs: 1613919098377
21/02/21 22:51:38.378 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-382, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.378 pool-9-thread-92 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.378 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-382, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.379 pool-9-thread-88 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.383 pool-9-thread-92 INFO Metadata: [Consumer clientId=consumer-groupId-382, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.384 pool-9-thread-88 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.384 pool-9-thread-88 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.384 pool-9-thread-88 INFO AppInfoParser: Kafka startTimeMs: 1613919098383
21/02/21 22:51:38.384 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-383, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.384 pool-9-thread-88 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.384 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-383, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.385 pool-9-thread-91 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.387 pool-9-thread-88 INFO Metadata: [Consumer clientId=consumer-groupId-383, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.391 pool-9-thread-91 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.391 pool-9-thread-91 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.391 pool-9-thread-91 INFO AppInfoParser: Kafka startTimeMs: 1613919098391
21/02/21 22:51:38.391 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-384, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.391 pool-9-thread-91 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.392 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-384, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.392 pool-9-thread-92 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.394 pool-9-thread-91 INFO Metadata: [Consumer clientId=consumer-groupId-384, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.395 pool-9-thread-92 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.395 pool-9-thread-92 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.395 pool-9-thread-92 INFO AppInfoParser: Kafka startTimeMs: 1613919098394
21/02/21 22:51:38.395 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-385, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.395 pool-9-thread-92 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.395 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-385, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.395 pool-9-thread-89 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.398 pool-9-thread-92 INFO Metadata: [Consumer clientId=consumer-groupId-385, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.400 pool-9-thread-89 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.400 pool-9-thread-89 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.400 pool-9-thread-89 INFO AppInfoParser: Kafka startTimeMs: 1613919098400
21/02/21 22:51:38.400 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-386, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.400 pool-9-thread-89 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.400 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-386, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.402 pool-9-thread-86 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.405 pool-9-thread-89 INFO Metadata: [Consumer clientId=consumer-groupId-386, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.406 pool-9-thread-86 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.406 pool-9-thread-86 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.406 pool-9-thread-86 INFO AppInfoParser: Kafka startTimeMs: 1613919098406
21/02/21 22:51:38.406 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-387, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.407 pool-9-thread-86 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.407 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-387, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.408 pool-9-thread-85 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.411 pool-9-thread-85 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.412 pool-9-thread-85 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.412 pool-9-thread-85 INFO AppInfoParser: Kafka startTimeMs: 1613919098411
21/02/21 22:51:38.412 pool-9-thread-86 INFO Metadata: [Consumer clientId=consumer-groupId-387, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.412 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-388, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.412 pool-9-thread-85 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.412 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-388, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.413 pool-9-thread-83 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.416 pool-9-thread-85 INFO Metadata: [Consumer clientId=consumer-groupId-388, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.416 pool-9-thread-83 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.416 pool-9-thread-83 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.416 pool-9-thread-83 INFO AppInfoParser: Kafka startTimeMs: 1613919098416
21/02/21 22:51:38.416 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-389, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.417 pool-9-thread-83 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.418 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-389, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.420 pool-9-thread-87 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.422 pool-9-thread-83 INFO Metadata: [Consumer clientId=consumer-groupId-389, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.425 pool-9-thread-87 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.425 pool-9-thread-87 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.425 pool-9-thread-87 INFO AppInfoParser: Kafka startTimeMs: 1613919098425
21/02/21 22:51:38.425 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-390, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.425 pool-9-thread-87 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.425 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-390, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.426 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.429 pool-9-thread-87 INFO Metadata: [Consumer clientId=consumer-groupId-390, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.430 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.430 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.431 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919098430
21/02/21 22:51:38.431 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-391, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.431 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.431 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-391, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.431 pool-9-thread-84 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.433 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-391, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.436 pool-9-thread-84 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.436 pool-9-thread-84 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.436 pool-9-thread-84 INFO AppInfoParser: Kafka startTimeMs: 1613919098436
21/02/21 22:51:38.436 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-392, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.436 pool-9-thread-84 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.436 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-392, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.437 pool-9-thread-73 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.440 pool-9-thread-84 INFO Metadata: [Consumer clientId=consumer-groupId-392, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.442 pool-9-thread-73 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.442 pool-9-thread-73 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.442 pool-9-thread-73 INFO AppInfoParser: Kafka startTimeMs: 1613919098442
21/02/21 22:51:38.442 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-393, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.442 pool-9-thread-73 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.442 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-393, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.443 pool-9-thread-76 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.444 pool-9-thread-73 INFO Metadata: [Consumer clientId=consumer-groupId-393, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.447 pool-9-thread-76 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.447 pool-9-thread-76 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.447 pool-9-thread-76 INFO AppInfoParser: Kafka startTimeMs: 1613919098447
21/02/21 22:51:38.447 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-394, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.447 pool-9-thread-76 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.447 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-394, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.448 pool-9-thread-64 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.451 pool-9-thread-76 INFO Metadata: [Consumer clientId=consumer-groupId-394, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.452 pool-9-thread-64 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.452 pool-9-thread-64 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.452 pool-9-thread-64 INFO AppInfoParser: Kafka startTimeMs: 1613919098452
21/02/21 22:51:38.452 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-395, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.453 pool-9-thread-64 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.453 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-395, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.453 pool-9-thread-70 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.455 pool-9-thread-64 INFO Metadata: [Consumer clientId=consumer-groupId-395, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.457 pool-9-thread-70 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.458 pool-9-thread-70 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.458 pool-9-thread-70 INFO AppInfoParser: Kafka startTimeMs: 1613919098457
21/02/21 22:51:38.458 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-396, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.458 pool-9-thread-70 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.458 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-396, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.458 pool-9-thread-82 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.460 pool-9-thread-70 INFO Metadata: [Consumer clientId=consumer-groupId-396, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.462 pool-9-thread-82 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.463 pool-9-thread-82 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.463 pool-9-thread-82 INFO AppInfoParser: Kafka startTimeMs: 1613919098462
21/02/21 22:51:38.463 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-397, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.463 pool-9-thread-82 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.463 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-397, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.463 pool-9-thread-79 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.466 pool-9-thread-82 INFO Metadata: [Consumer clientId=consumer-groupId-397, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.467 pool-9-thread-79 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.467 pool-9-thread-79 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.467 pool-9-thread-79 INFO AppInfoParser: Kafka startTimeMs: 1613919098467
21/02/21 22:51:38.467 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-398, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.467 pool-9-thread-79 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.467 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-398, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.468 pool-9-thread-77 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.470 pool-9-thread-79 INFO Metadata: [Consumer clientId=consumer-groupId-398, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.472 pool-9-thread-77 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.472 pool-9-thread-77 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.472 pool-9-thread-77 INFO AppInfoParser: Kafka startTimeMs: 1613919098472
21/02/21 22:51:38.472 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-399, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.472 pool-9-thread-77 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.472 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-399, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.473 pool-9-thread-70 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.475 pool-9-thread-77 INFO Metadata: [Consumer clientId=consumer-groupId-399, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.476 pool-9-thread-70 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.476 pool-9-thread-70 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.476 pool-9-thread-70 INFO AppInfoParser: Kafka startTimeMs: 1613919098476
21/02/21 22:51:38.476 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-400, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.476 pool-9-thread-70 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.476 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-400, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.477 pool-9-thread-64 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.480 pool-9-thread-70 INFO Metadata: [Consumer clientId=consumer-groupId-400, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.481 pool-9-thread-64 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.481 pool-9-thread-64 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.481 pool-9-thread-64 INFO AppInfoParser: Kafka startTimeMs: 1613919098481
21/02/21 22:51:38.481 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-401, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.481 pool-9-thread-64 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.481 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-401, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.482 pool-9-thread-76 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.484 pool-9-thread-64 INFO Metadata: [Consumer clientId=consumer-groupId-401, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.486 pool-9-thread-76 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.486 pool-9-thread-76 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.486 pool-9-thread-76 INFO AppInfoParser: Kafka startTimeMs: 1613919098486
21/02/21 22:51:38.486 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-402, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.487 pool-9-thread-76 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.487 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-402, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.487 pool-9-thread-73 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.491 pool-9-thread-76 INFO Metadata: [Consumer clientId=consumer-groupId-402, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.492 pool-9-thread-73 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.493 pool-9-thread-73 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.493 pool-9-thread-73 INFO AppInfoParser: Kafka startTimeMs: 1613919098492
21/02/21 22:51:38.493 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-403, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.493 pool-9-thread-73 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.493 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-403, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.501 pool-9-thread-84 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.502 pool-9-thread-73 INFO Metadata: [Consumer clientId=consumer-groupId-403, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.505 pool-9-thread-84 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.505 pool-9-thread-84 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.505 pool-9-thread-84 INFO AppInfoParser: Kafka startTimeMs: 1613919098505
21/02/21 22:51:38.505 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-404, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.505 pool-9-thread-84 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.506 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-404, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.506 pool-9-thread-71 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.509 pool-9-thread-84 INFO Metadata: [Consumer clientId=consumer-groupId-404, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.510 pool-9-thread-71 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.510 pool-9-thread-71 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.510 pool-9-thread-71 INFO AppInfoParser: Kafka startTimeMs: 1613919098510
21/02/21 22:51:38.511 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-405, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.511 pool-9-thread-71 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.511 pool-9-thread-71 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-405, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.511 pool-9-thread-87 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.514 pool-9-thread-71 INFO Metadata: [Consumer clientId=consumer-groupId-405, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.517 pool-9-thread-87 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.517 pool-9-thread-87 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.517 pool-9-thread-87 INFO AppInfoParser: Kafka startTimeMs: 1613919098517
21/02/21 22:51:38.517 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-406, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.517 pool-9-thread-87 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.517 pool-9-thread-87 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-406, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.518 pool-9-thread-83 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.520 pool-9-thread-87 INFO Metadata: [Consumer clientId=consumer-groupId-406, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.523 pool-9-thread-83 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.523 pool-9-thread-83 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.523 pool-9-thread-83 INFO AppInfoParser: Kafka startTimeMs: 1613919098523
21/02/21 22:51:38.523 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-407, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.523 pool-9-thread-83 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.523 pool-9-thread-83 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-407, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.524 pool-9-thread-85 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.526 pool-9-thread-83 INFO Metadata: [Consumer clientId=consumer-groupId-407, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.529 pool-9-thread-85 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.529 pool-9-thread-85 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.529 pool-9-thread-85 INFO AppInfoParser: Kafka startTimeMs: 1613919098529
21/02/21 22:51:38.529 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-408, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.529 pool-9-thread-85 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.529 pool-9-thread-85 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-408, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.530 pool-9-thread-86 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.532 pool-9-thread-85 INFO Metadata: [Consumer clientId=consumer-groupId-408, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.533 pool-9-thread-86 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.533 pool-9-thread-86 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.533 pool-9-thread-86 INFO AppInfoParser: Kafka startTimeMs: 1613919098533
21/02/21 22:51:38.533 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-409, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.533 pool-9-thread-86 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.533 pool-9-thread-86 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-409, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.534 pool-9-thread-89 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.536 pool-9-thread-86 INFO Metadata: [Consumer clientId=consumer-groupId-409, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.537 pool-9-thread-89 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.537 pool-9-thread-89 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.537 pool-9-thread-89 INFO AppInfoParser: Kafka startTimeMs: 1613919098537
21/02/21 22:51:38.538 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-410, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.538 pool-9-thread-89 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.538 pool-9-thread-89 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-410, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.539 pool-9-thread-92 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.542 pool-9-thread-89 INFO Metadata: [Consumer clientId=consumer-groupId-410, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.542 pool-9-thread-92 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.542 pool-9-thread-92 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.543 pool-9-thread-92 INFO AppInfoParser: Kafka startTimeMs: 1613919098542
21/02/21 22:51:38.543 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-411, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.543 pool-9-thread-92 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.543 pool-9-thread-92 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-411, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.544 pool-9-thread-91 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.546 pool-9-thread-92 INFO Metadata: [Consumer clientId=consumer-groupId-411, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.546 pool-9-thread-91 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.546 pool-9-thread-91 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.546 pool-9-thread-91 INFO AppInfoParser: Kafka startTimeMs: 1613919098546
21/02/21 22:51:38.546 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-412, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.547 pool-9-thread-91 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.547 pool-9-thread-91 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-412, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.549 pool-9-thread-88 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.550 pool-9-thread-91 INFO Metadata: [Consumer clientId=consumer-groupId-412, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.552 pool-9-thread-88 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.552 pool-9-thread-88 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.552 pool-9-thread-88 INFO AppInfoParser: Kafka startTimeMs: 1613919098552
21/02/21 22:51:38.552 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-413, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.553 pool-9-thread-88 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.553 pool-9-thread-88 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-413, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.553 pool-9-thread-96 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.556 pool-9-thread-88 INFO Metadata: [Consumer clientId=consumer-groupId-413, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.558 pool-9-thread-96 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.559 pool-9-thread-96 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.559 pool-9-thread-96 INFO AppInfoParser: Kafka startTimeMs: 1613919098558
21/02/21 22:51:38.559 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-414, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.559 pool-9-thread-96 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.559 pool-9-thread-96 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-414, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.561 pool-9-thread-100 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.562 pool-9-thread-96 INFO Metadata: [Consumer clientId=consumer-groupId-414, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.564 pool-9-thread-100 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.564 pool-9-thread-100 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.564 pool-9-thread-100 INFO AppInfoParser: Kafka startTimeMs: 1613919098564
21/02/21 22:51:38.564 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-415, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.564 pool-9-thread-100 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.565 pool-9-thread-100 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-415, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.565 pool-9-thread-97 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.567 pool-9-thread-100 INFO Metadata: [Consumer clientId=consumer-groupId-415, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.570 pool-9-thread-97 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.570 pool-9-thread-97 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.570 pool-9-thread-97 INFO AppInfoParser: Kafka startTimeMs: 1613919098570
21/02/21 22:51:38.570 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-416, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.570 pool-9-thread-97 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.570 pool-9-thread-97 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-416, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.571 pool-9-thread-99 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.574 pool-9-thread-97 INFO Metadata: [Consumer clientId=consumer-groupId-416, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.576 pool-9-thread-99 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.576 pool-9-thread-99 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.576 pool-9-thread-99 INFO AppInfoParser: Kafka startTimeMs: 1613919098576
21/02/21 22:51:38.576 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-417, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.576 pool-9-thread-99 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.576 pool-9-thread-99 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-417, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.576 pool-9-thread-95 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.579 pool-9-thread-99 INFO Metadata: [Consumer clientId=consumer-groupId-417, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.581 pool-9-thread-95 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.582 pool-9-thread-95 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.582 pool-9-thread-95 INFO AppInfoParser: Kafka startTimeMs: 1613919098581
21/02/21 22:51:38.582 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-418, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.582 pool-9-thread-95 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.582 pool-9-thread-95 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-418, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.582 pool-9-thread-94 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.584 pool-9-thread-95 INFO Metadata: [Consumer clientId=consumer-groupId-418, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.586 pool-9-thread-94 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.586 pool-9-thread-94 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.586 pool-9-thread-94 INFO AppInfoParser: Kafka startTimeMs: 1613919098586
21/02/21 22:51:38.586 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-419, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.587 pool-9-thread-94 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.587 pool-9-thread-94 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-419, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.587 pool-9-thread-90 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.589 pool-9-thread-94 INFO Metadata: [Consumer clientId=consumer-groupId-419, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.592 pool-9-thread-90 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.592 pool-9-thread-90 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.592 pool-9-thread-90 INFO AppInfoParser: Kafka startTimeMs: 1613919098592
21/02/21 22:51:38.592 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-420, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.592 pool-9-thread-90 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.592 pool-9-thread-90 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-420, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.593 pool-9-thread-62 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.595 pool-9-thread-90 INFO Metadata: [Consumer clientId=consumer-groupId-420, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.598 pool-9-thread-62 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.598 pool-9-thread-62 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.598 pool-9-thread-62 INFO AppInfoParser: Kafka startTimeMs: 1613919098598
21/02/21 22:51:38.598 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-421, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.598 pool-9-thread-62 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.598 pool-9-thread-62 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-421, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.598 pool-9-thread-98 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.601 pool-9-thread-62 INFO Metadata: [Consumer clientId=consumer-groupId-421, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.602 pool-9-thread-98 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.602 pool-9-thread-98 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.602 pool-9-thread-98 INFO AppInfoParser: Kafka startTimeMs: 1613919098602
21/02/21 22:51:38.602 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-422, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.603 pool-9-thread-98 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.603 pool-9-thread-98 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-422, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.603 pool-9-thread-65 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.605 pool-9-thread-98 INFO Metadata: [Consumer clientId=consumer-groupId-422, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.606 pool-9-thread-65 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.606 pool-9-thread-65 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.606 pool-9-thread-65 INFO AppInfoParser: Kafka startTimeMs: 1613919098606
21/02/21 22:51:38.607 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-423, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.607 pool-9-thread-65 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.607 pool-9-thread-65 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-423, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.608 pool-9-thread-61 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.610 pool-9-thread-65 INFO Metadata: [Consumer clientId=consumer-groupId-423, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.610 pool-9-thread-61 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.610 pool-9-thread-61 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.610 pool-9-thread-61 INFO AppInfoParser: Kafka startTimeMs: 1613919098610
21/02/21 22:51:38.610 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-424, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.610 pool-9-thread-61 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.611 pool-9-thread-61 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-424, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.611 pool-9-thread-69 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.614 pool-9-thread-61 INFO Metadata: [Consumer clientId=consumer-groupId-424, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.616 pool-9-thread-69 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.616 pool-9-thread-69 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.616 pool-9-thread-69 INFO AppInfoParser: Kafka startTimeMs: 1613919098616
21/02/21 22:51:38.616 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-425, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.616 pool-9-thread-69 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.616 pool-9-thread-69 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-425, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.616 pool-9-thread-1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.618 pool-9-thread-69 INFO Metadata: [Consumer clientId=consumer-groupId-425, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.622 pool-9-thread-1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.622 pool-9-thread-1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.622 pool-9-thread-1 INFO AppInfoParser: Kafka startTimeMs: 1613919098622
21/02/21 22:51:38.622 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-426, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.622 pool-9-thread-1 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.622 pool-9-thread-1 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-426, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.624 pool-9-thread-68 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.626 pool-9-thread-1 INFO Metadata: [Consumer clientId=consumer-groupId-426, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.626 pool-9-thread-68 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.626 pool-9-thread-68 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.626 pool-9-thread-68 INFO AppInfoParser: Kafka startTimeMs: 1613919098626
21/02/21 22:51:38.626 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-427, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.627 pool-9-thread-68 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.628 pool-9-thread-68 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-427, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.628 pool-9-thread-81 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.630 pool-9-thread-68 INFO Metadata: [Consumer clientId=consumer-groupId-427, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.633 pool-9-thread-81 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.633 pool-9-thread-81 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.633 pool-9-thread-81 INFO AppInfoParser: Kafka startTimeMs: 1613919098633
21/02/21 22:51:38.634 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-428, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.634 pool-9-thread-81 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.634 pool-9-thread-81 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-428, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.634 pool-9-thread-74 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.636 pool-9-thread-81 INFO Metadata: [Consumer clientId=consumer-groupId-428, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.637 pool-9-thread-74 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.637 pool-9-thread-74 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.637 pool-9-thread-74 INFO AppInfoParser: Kafka startTimeMs: 1613919098637
21/02/21 22:51:38.637 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-429, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.637 pool-9-thread-74 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.638 pool-9-thread-74 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-429, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.638 pool-9-thread-67 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.640 pool-9-thread-74 INFO Metadata: [Consumer clientId=consumer-groupId-429, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.642 pool-9-thread-67 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.642 pool-9-thread-67 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.642 pool-9-thread-67 INFO AppInfoParser: Kafka startTimeMs: 1613919098642
21/02/21 22:51:38.642 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-430, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.642 pool-9-thread-67 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.642 pool-9-thread-67 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-430, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.643 pool-9-thread-78 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.645 pool-9-thread-67 INFO Metadata: [Consumer clientId=consumer-groupId-430, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.647 pool-9-thread-78 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.648 pool-9-thread-78 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.648 pool-9-thread-78 INFO AppInfoParser: Kafka startTimeMs: 1613919098647
21/02/21 22:51:38.648 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-431, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.648 pool-9-thread-78 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.648 pool-9-thread-78 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-431, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.648 pool-9-thread-72 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.651 pool-9-thread-78 INFO Metadata: [Consumer clientId=consumer-groupId-431, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.652 pool-9-thread-72 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.652 pool-9-thread-72 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.652 pool-9-thread-72 INFO AppInfoParser: Kafka startTimeMs: 1613919098652
21/02/21 22:51:38.652 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-432, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.653 pool-9-thread-72 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.653 pool-9-thread-72 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-432, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.653 pool-9-thread-75 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.655 pool-9-thread-72 INFO Metadata: [Consumer clientId=consumer-groupId-432, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.658 pool-9-thread-75 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.658 pool-9-thread-75 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.658 pool-9-thread-75 INFO AppInfoParser: Kafka startTimeMs: 1613919098658
21/02/21 22:51:38.658 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-433, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.658 pool-9-thread-75 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.658 pool-9-thread-75 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-433, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.658 pool-9-thread-63 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.661 pool-9-thread-75 INFO Metadata: [Consumer clientId=consumer-groupId-433, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.663 pool-9-thread-63 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.663 pool-9-thread-63 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.663 pool-9-thread-63 INFO AppInfoParser: Kafka startTimeMs: 1613919098663
21/02/21 22:51:38.663 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-434, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.663 pool-9-thread-63 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.663 pool-9-thread-63 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-434, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.663 pool-9-thread-66 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.665 pool-9-thread-63 INFO Metadata: [Consumer clientId=consumer-groupId-434, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.669 pool-9-thread-66 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.669 pool-9-thread-66 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.669 pool-9-thread-66 INFO AppInfoParser: Kafka startTimeMs: 1613919098669
21/02/21 22:51:38.669 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-435, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.669 pool-9-thread-66 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.669 pool-9-thread-66 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-435, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.670 pool-9-thread-80 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.672 pool-9-thread-66 INFO Metadata: [Consumer clientId=consumer-groupId-435, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.674 pool-9-thread-80 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.674 pool-9-thread-80 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.674 pool-9-thread-80 INFO AppInfoParser: Kafka startTimeMs: 1613919098674
21/02/21 22:51:38.674 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-436, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.674 pool-9-thread-80 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.674 pool-9-thread-80 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-436, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.675 pool-9-thread-60 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.677 pool-9-thread-80 INFO Metadata: [Consumer clientId=consumer-groupId-436, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.680 pool-9-thread-60 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.680 pool-9-thread-60 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.680 pool-9-thread-60 INFO AppInfoParser: Kafka startTimeMs: 1613919098680
21/02/21 22:51:38.681 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-437, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.681 pool-9-thread-60 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.681 pool-9-thread-60 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-437, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.681 pool-9-thread-57 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.684 pool-9-thread-60 INFO Metadata: [Consumer clientId=consumer-groupId-437, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.685 pool-9-thread-57 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.685 pool-9-thread-57 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.685 pool-9-thread-57 INFO AppInfoParser: Kafka startTimeMs: 1613919098685
21/02/21 22:51:38.686 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-438, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.686 pool-9-thread-57 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.686 pool-9-thread-57 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-438, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.687 pool-9-thread-56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.688 pool-9-thread-57 INFO Metadata: [Consumer clientId=consumer-groupId-438, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.690 pool-9-thread-56 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.691 pool-9-thread-56 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.691 pool-9-thread-56 INFO AppInfoParser: Kafka startTimeMs: 1613919098690
21/02/21 22:51:38.691 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-439, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.691 pool-9-thread-56 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.691 pool-9-thread-56 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-439, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.691 pool-9-thread-55 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.694 pool-9-thread-56 INFO Metadata: [Consumer clientId=consumer-groupId-439, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.694 pool-9-thread-55 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.695 pool-9-thread-55 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.695 pool-9-thread-55 INFO AppInfoParser: Kafka startTimeMs: 1613919098694
21/02/21 22:51:38.696 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-440, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.696 pool-9-thread-55 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.696 pool-9-thread-55 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-440, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.696 pool-9-thread-53 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.698 pool-9-thread-55 INFO Metadata: [Consumer clientId=consumer-groupId-440, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.701 pool-9-thread-53 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.702 pool-9-thread-53 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.702 pool-9-thread-53 INFO AppInfoParser: Kafka startTimeMs: 1613919098701
21/02/21 22:51:38.702 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-441, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.702 pool-9-thread-53 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.702 pool-9-thread-53 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-441, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.704 pool-9-thread-59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.706 pool-9-thread-53 INFO Metadata: [Consumer clientId=consumer-groupId-441, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.709 pool-9-thread-59 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.709 pool-9-thread-59 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.709 pool-9-thread-59 INFO AppInfoParser: Kafka startTimeMs: 1613919098709
21/02/21 22:51:38.709 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-442, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.709 pool-9-thread-59 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.709 pool-9-thread-59 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-442, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.710 pool-9-thread-50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.712 pool-9-thread-59 INFO Metadata: [Consumer clientId=consumer-groupId-442, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.715 pool-9-thread-50 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.715 pool-9-thread-50 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.715 pool-9-thread-50 INFO AppInfoParser: Kafka startTimeMs: 1613919098715
21/02/21 22:51:38.715 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-443, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.715 pool-9-thread-50 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.715 pool-9-thread-50 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-443, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.716 pool-9-thread-52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.718 pool-9-thread-50 INFO Metadata: [Consumer clientId=consumer-groupId-443, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.720 pool-9-thread-52 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.720 pool-9-thread-52 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.720 pool-9-thread-52 INFO AppInfoParser: Kafka startTimeMs: 1613919098720
21/02/21 22:51:38.721 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-444, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.721 pool-9-thread-52 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.721 pool-9-thread-52 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-444, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.721 pool-9-thread-51 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.723 pool-9-thread-52 INFO Metadata: [Consumer clientId=consumer-groupId-444, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.726 pool-9-thread-51 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.726 pool-9-thread-51 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.726 pool-9-thread-51 INFO AppInfoParser: Kafka startTimeMs: 1613919098726
21/02/21 22:51:38.726 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-445, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.727 pool-9-thread-51 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.727 pool-9-thread-51 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-445, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.727 pool-9-thread-54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.729 pool-9-thread-51 INFO Metadata: [Consumer clientId=consumer-groupId-445, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.730 pool-9-thread-54 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.730 pool-9-thread-54 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.730 pool-9-thread-54 INFO AppInfoParser: Kafka startTimeMs: 1613919098730
21/02/21 22:51:38.730 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-446, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.730 pool-9-thread-54 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.730 pool-9-thread-54 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-446, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.731 pool-9-thread-58 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.732 pool-9-thread-54 INFO Metadata: [Consumer clientId=consumer-groupId-446, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.736 pool-9-thread-58 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.736 pool-9-thread-58 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.737 pool-9-thread-58 INFO AppInfoParser: Kafka startTimeMs: 1613919098736
21/02/21 22:51:38.737 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-447, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.737 pool-9-thread-58 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.737 pool-9-thread-58 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-447, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.737 pool-9-thread-39 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.740 pool-9-thread-58 INFO Metadata: [Consumer clientId=consumer-groupId-447, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.742 pool-9-thread-39 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.742 pool-9-thread-39 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.742 pool-9-thread-39 INFO AppInfoParser: Kafka startTimeMs: 1613919098742
21/02/21 22:51:38.742 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-448, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.742 pool-9-thread-39 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.742 pool-9-thread-39 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-448, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.742 pool-9-thread-41 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.746 pool-9-thread-39 INFO Metadata: [Consumer clientId=consumer-groupId-448, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.746 pool-9-thread-41 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.746 pool-9-thread-41 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.746 pool-9-thread-41 INFO AppInfoParser: Kafka startTimeMs: 1613919098746
21/02/21 22:51:38.746 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-449, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.746 pool-9-thread-41 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.746 pool-9-thread-41 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-449, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.747 pool-9-thread-47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.749 pool-9-thread-41 INFO Metadata: [Consumer clientId=consumer-groupId-449, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.751 pool-9-thread-47 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.751 pool-9-thread-47 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.751 pool-9-thread-47 INFO AppInfoParser: Kafka startTimeMs: 1613919098751
21/02/21 22:51:38.752 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-450, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.752 pool-9-thread-47 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.752 pool-9-thread-47 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-450, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.752 pool-9-thread-49 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.754 pool-9-thread-47 INFO Metadata: [Consumer clientId=consumer-groupId-450, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.756 pool-9-thread-49 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.756 pool-9-thread-49 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.756 pool-9-thread-49 INFO AppInfoParser: Kafka startTimeMs: 1613919098756
21/02/21 22:51:38.756 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-451, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.756 pool-9-thread-49 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.756 pool-9-thread-49 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-451, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.757 pool-9-thread-36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.760 pool-9-thread-49 INFO Metadata: [Consumer clientId=consumer-groupId-451, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.762 pool-9-thread-36 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.762 pool-9-thread-36 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.762 pool-9-thread-36 INFO AppInfoParser: Kafka startTimeMs: 1613919098762
21/02/21 22:51:38.762 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-452, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.763 pool-9-thread-36 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.763 pool-9-thread-36 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-452, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.763 pool-9-thread-46 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.766 pool-9-thread-36 INFO Metadata: [Consumer clientId=consumer-groupId-452, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.767 pool-9-thread-46 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.767 pool-9-thread-46 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.767 pool-9-thread-46 INFO AppInfoParser: Kafka startTimeMs: 1613919098767
21/02/21 22:51:38.767 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-453, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.767 pool-9-thread-46 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.767 pool-9-thread-46 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-453, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.768 pool-9-thread-42 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.770 pool-9-thread-46 INFO Metadata: [Consumer clientId=consumer-groupId-453, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.771 pool-9-thread-42 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.771 pool-9-thread-42 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.771 pool-9-thread-42 INFO AppInfoParser: Kafka startTimeMs: 1613919098771
21/02/21 22:51:38.771 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-454, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.771 pool-9-thread-42 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.771 pool-9-thread-42 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-454, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.771 pool-9-thread-37 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.774 pool-9-thread-42 INFO Metadata: [Consumer clientId=consumer-groupId-454, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.776 pool-9-thread-37 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.776 pool-9-thread-37 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.776 pool-9-thread-37 INFO AppInfoParser: Kafka startTimeMs: 1613919098776
21/02/21 22:51:38.776 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-455, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.776 pool-9-thread-37 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.776 pool-9-thread-37 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-455, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.777 pool-9-thread-48 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.779 pool-9-thread-37 INFO Metadata: [Consumer clientId=consumer-groupId-455, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.782 pool-9-thread-48 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.782 pool-9-thread-48 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.782 pool-9-thread-48 INFO AppInfoParser: Kafka startTimeMs: 1613919098782
21/02/21 22:51:38.782 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-456, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.782 pool-9-thread-48 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.783 pool-9-thread-48 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-456, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.784 pool-9-thread-21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.787 pool-9-thread-21 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.787 pool-9-thread-21 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.787 pool-9-thread-21 INFO AppInfoParser: Kafka startTimeMs: 1613919098787
21/02/21 22:51:38.787 pool-9-thread-48 INFO Metadata: [Consumer clientId=consumer-groupId-456, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.787 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-457, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.787 pool-9-thread-21 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.787 pool-9-thread-21 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-457, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.788 pool-9-thread-45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.791 pool-9-thread-45 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.791 pool-9-thread-45 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.791 pool-9-thread-45 INFO AppInfoParser: Kafka startTimeMs: 1613919098791
21/02/21 22:51:38.791 pool-9-thread-21 INFO Metadata: [Consumer clientId=consumer-groupId-457, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.791 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-458, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.792 pool-9-thread-45 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.792 pool-9-thread-45 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-458, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.793 pool-9-thread-40 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.795 pool-9-thread-45 INFO Metadata: [Consumer clientId=consumer-groupId-458, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.796 pool-9-thread-40 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.796 pool-9-thread-40 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.796 pool-9-thread-40 INFO AppInfoParser: Kafka startTimeMs: 1613919098796
21/02/21 22:51:38.796 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-459, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.797 pool-9-thread-40 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.797 pool-9-thread-40 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-459, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.799 pool-9-thread-44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.800 pool-9-thread-40 INFO Metadata: [Consumer clientId=consumer-groupId-459, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.804 pool-9-thread-44 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.804 pool-9-thread-44 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.804 pool-9-thread-44 INFO AppInfoParser: Kafka startTimeMs: 1613919098804
21/02/21 22:51:38.804 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-460, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.804 pool-9-thread-44 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.804 pool-9-thread-44 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-460, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.805 pool-9-thread-38 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.808 pool-9-thread-38 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.808 pool-9-thread-38 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.808 pool-9-thread-38 INFO AppInfoParser: Kafka startTimeMs: 1613919098808
21/02/21 22:51:38.808 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-461, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.808 pool-9-thread-44 INFO Metadata: [Consumer clientId=consumer-groupId-460, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.809 pool-9-thread-38 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.809 pool-9-thread-38 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-461, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.810 pool-9-thread-43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.812 pool-9-thread-38 INFO Metadata: [Consumer clientId=consumer-groupId-461, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.813 pool-9-thread-43 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.813 pool-9-thread-43 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.813 pool-9-thread-43 INFO AppInfoParser: Kafka startTimeMs: 1613919098813
21/02/21 22:51:38.813 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-462, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.813 pool-9-thread-43 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.813 pool-9-thread-43 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-462, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.814 pool-9-thread-35 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.816 pool-9-thread-43 INFO Metadata: [Consumer clientId=consumer-groupId-462, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.818 pool-9-thread-35 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.818 pool-9-thread-35 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.818 pool-9-thread-35 INFO AppInfoParser: Kafka startTimeMs: 1613919098818
21/02/21 22:51:38.818 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-463, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.818 pool-9-thread-35 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.818 pool-9-thread-35 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-463, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.820 pool-9-thread-34 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.822 pool-9-thread-35 INFO Metadata: [Consumer clientId=consumer-groupId-463, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.823 pool-9-thread-34 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.823 pool-9-thread-34 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.823 pool-9-thread-34 INFO AppInfoParser: Kafka startTimeMs: 1613919098823
21/02/21 22:51:38.823 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-464, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.823 pool-9-thread-34 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.823 pool-9-thread-34 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-464, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.824 pool-9-thread-29 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.826 pool-9-thread-34 INFO Metadata: [Consumer clientId=consumer-groupId-464, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.828 pool-9-thread-29 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.828 pool-9-thread-29 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.828 pool-9-thread-29 INFO AppInfoParser: Kafka startTimeMs: 1613919098828
21/02/21 22:51:38.828 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-465, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.828 pool-9-thread-29 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.828 pool-9-thread-29 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-465, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.829 pool-9-thread-24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.831 pool-9-thread-29 INFO Metadata: [Consumer clientId=consumer-groupId-465, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.833 pool-9-thread-24 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.833 pool-9-thread-24 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.833 pool-9-thread-24 INFO AppInfoParser: Kafka startTimeMs: 1613919098833
21/02/21 22:51:38.833 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-466, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.834 pool-9-thread-24 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.834 pool-9-thread-24 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-466, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.834 pool-9-thread-30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.836 pool-9-thread-24 INFO Metadata: [Consumer clientId=consumer-groupId-466, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.838 pool-9-thread-30 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.838 pool-9-thread-30 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.838 pool-9-thread-30 INFO AppInfoParser: Kafka startTimeMs: 1613919098838
21/02/21 22:51:38.838 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-467, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.839 pool-9-thread-30 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.839 pool-9-thread-30 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-467, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.839 pool-9-thread-11 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.842 pool-9-thread-30 INFO Metadata: [Consumer clientId=consumer-groupId-467, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.844 pool-9-thread-11 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.844 pool-9-thread-11 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.844 pool-9-thread-11 INFO AppInfoParser: Kafka startTimeMs: 1613919098844
21/02/21 22:51:38.844 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-468, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.845 pool-9-thread-11 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.845 pool-9-thread-11 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-468, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.845 pool-9-thread-25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.848 pool-9-thread-11 INFO Metadata: [Consumer clientId=consumer-groupId-468, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.850 pool-9-thread-25 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.850 pool-9-thread-25 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.850 pool-9-thread-25 INFO AppInfoParser: Kafka startTimeMs: 1613919098850
21/02/21 22:51:38.851 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-469, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.851 pool-9-thread-25 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.851 pool-9-thread-25 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-469, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.851 pool-9-thread-15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.853 pool-9-thread-25 INFO Metadata: [Consumer clientId=consumer-groupId-469, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.854 pool-9-thread-15 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.854 pool-9-thread-15 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.854 pool-9-thread-15 INFO AppInfoParser: Kafka startTimeMs: 1613919098854
21/02/21 22:51:38.854 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-470, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.854 pool-9-thread-15 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.854 pool-9-thread-15 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-470, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.855 pool-9-thread-26 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.857 pool-9-thread-15 INFO Metadata: [Consumer clientId=consumer-groupId-470, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.858 pool-9-thread-26 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.858 pool-9-thread-26 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.858 pool-9-thread-26 INFO AppInfoParser: Kafka startTimeMs: 1613919098858
21/02/21 22:51:38.858 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-471, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.858 pool-9-thread-26 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.858 pool-9-thread-26 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-471, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.859 pool-9-thread-19 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.861 pool-9-thread-26 INFO Metadata: [Consumer clientId=consumer-groupId-471, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.863 pool-9-thread-19 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.863 pool-9-thread-19 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.863 pool-9-thread-19 INFO AppInfoParser: Kafka startTimeMs: 1613919098863
21/02/21 22:51:38.863 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-472, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.864 pool-9-thread-19 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.864 pool-9-thread-19 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-472, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.864 pool-9-thread-31 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.866 pool-9-thread-19 INFO Metadata: [Consumer clientId=consumer-groupId-472, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.867 pool-9-thread-31 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.868 pool-9-thread-31 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.868 pool-9-thread-31 INFO AppInfoParser: Kafka startTimeMs: 1613919098867
21/02/21 22:51:38.868 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-473, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.868 pool-9-thread-31 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.868 pool-9-thread-31 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-473, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.868 pool-9-thread-23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.871 pool-9-thread-31 INFO Metadata: [Consumer clientId=consumer-groupId-473, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.872 pool-9-thread-23 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.872 pool-9-thread-23 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.872 pool-9-thread-23 INFO AppInfoParser: Kafka startTimeMs: 1613919098872
21/02/21 22:51:38.872 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-474, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.873 pool-9-thread-23 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.873 pool-9-thread-23 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-474, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.873 pool-9-thread-27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.875 pool-9-thread-23 INFO Metadata: [Consumer clientId=consumer-groupId-474, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.878 pool-9-thread-27 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.878 pool-9-thread-27 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.878 pool-9-thread-27 INFO AppInfoParser: Kafka startTimeMs: 1613919098878
21/02/21 22:51:38.878 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-475, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.879 pool-9-thread-27 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.879 pool-9-thread-27 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-475, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.879 pool-9-thread-28 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.882 pool-9-thread-27 INFO Metadata: [Consumer clientId=consumer-groupId-475, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.884 pool-9-thread-28 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.884 pool-9-thread-28 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.884 pool-9-thread-28 INFO AppInfoParser: Kafka startTimeMs: 1613919098884
21/02/21 22:51:38.884 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-476, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.884 pool-9-thread-28 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.884 pool-9-thread-28 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-476, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.885 pool-9-thread-32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.887 pool-9-thread-28 INFO Metadata: [Consumer clientId=consumer-groupId-476, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.890 pool-9-thread-32 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.890 pool-9-thread-32 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.890 pool-9-thread-32 INFO AppInfoParser: Kafka startTimeMs: 1613919098890
21/02/21 22:51:38.890 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-477, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.890 pool-9-thread-32 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.890 pool-9-thread-32 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-477, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.891 pool-9-thread-22 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.893 pool-9-thread-32 INFO Metadata: [Consumer clientId=consumer-groupId-477, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.895 pool-9-thread-22 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.895 pool-9-thread-22 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.895 pool-9-thread-22 INFO AppInfoParser: Kafka startTimeMs: 1613919098895
21/02/21 22:51:38.895 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-478, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.895 pool-9-thread-22 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.895 pool-9-thread-22 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-478, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.896 pool-9-thread-18 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.898 pool-9-thread-22 INFO Metadata: [Consumer clientId=consumer-groupId-478, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.902 pool-9-thread-18 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.902 pool-9-thread-18 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.902 pool-9-thread-18 INFO AppInfoParser: Kafka startTimeMs: 1613919098902
21/02/21 22:51:38.902 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-479, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.902 pool-9-thread-18 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.902 pool-9-thread-18 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-479, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.903 pool-9-thread-14 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.909 pool-9-thread-18 INFO Metadata: [Consumer clientId=consumer-groupId-479, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.909 pool-9-thread-14 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.909 pool-9-thread-14 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.909 pool-9-thread-14 INFO AppInfoParser: Kafka startTimeMs: 1613919098909
21/02/21 22:51:38.910 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-480, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.911 pool-9-thread-14 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.911 pool-9-thread-14 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-480, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.911 pool-9-thread-7 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.913 pool-9-thread-7 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.913 pool-9-thread-7 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.913 pool-9-thread-7 INFO AppInfoParser: Kafka startTimeMs: 1613919098913
21/02/21 22:51:38.913 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-481, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.913 pool-9-thread-7 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.913 pool-9-thread-7 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-481, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.915 pool-9-thread-14 INFO Metadata: [Consumer clientId=consumer-groupId-480, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.918 pool-9-thread-7 INFO Metadata: [Consumer clientId=consumer-groupId-481, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.919 pool-9-thread-33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.923 pool-9-thread-33 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.923 pool-9-thread-33 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.923 pool-9-thread-33 INFO AppInfoParser: Kafka startTimeMs: 1613919098923
21/02/21 22:51:38.923 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-482, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.924 pool-9-thread-33 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.924 pool-9-thread-33 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-482, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.925 pool-9-thread-20 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.928 pool-9-thread-20 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.928 pool-9-thread-20 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.928 pool-9-thread-20 INFO AppInfoParser: Kafka startTimeMs: 1613919098928
21/02/21 22:51:38.928 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-483, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.928 pool-9-thread-20 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.928 pool-9-thread-20 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-483, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.929 pool-9-thread-10 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.929 pool-9-thread-33 INFO Metadata: [Consumer clientId=consumer-groupId-482, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.932 pool-9-thread-20 INFO Metadata: [Consumer clientId=consumer-groupId-483, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.932 pool-9-thread-10 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.932 pool-9-thread-10 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.932 pool-9-thread-10 INFO AppInfoParser: Kafka startTimeMs: 1613919098932
21/02/21 22:51:38.932 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-484, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.932 pool-9-thread-10 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.932 pool-9-thread-10 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-484, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.934 pool-9-thread-2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.935 pool-9-thread-10 INFO Metadata: [Consumer clientId=consumer-groupId-484, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.938 pool-9-thread-2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.939 pool-9-thread-2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.939 pool-9-thread-2 INFO AppInfoParser: Kafka startTimeMs: 1613919098938
21/02/21 22:51:38.939 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-485, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.939 pool-9-thread-2 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.939 pool-9-thread-2 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-485, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.939 pool-9-thread-16 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.942 pool-9-thread-2 INFO Metadata: [Consumer clientId=consumer-groupId-485, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.944 pool-9-thread-16 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.946 pool-9-thread-16 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.946 pool-9-thread-16 INFO AppInfoParser: Kafka startTimeMs: 1613919098944
21/02/21 22:51:38.946 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-486, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.946 pool-9-thread-16 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.946 pool-9-thread-16 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-486, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.947 pool-9-thread-12 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.949 pool-9-thread-16 INFO Metadata: [Consumer clientId=consumer-groupId-486, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.950 pool-9-thread-12 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.950 pool-9-thread-12 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.950 pool-9-thread-12 INFO AppInfoParser: Kafka startTimeMs: 1613919098950
21/02/21 22:51:38.950 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-487, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.950 pool-9-thread-12 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.951 pool-9-thread-12 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-487, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.951 pool-9-thread-3 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.953 pool-9-thread-12 INFO Metadata: [Consumer clientId=consumer-groupId-487, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.954 pool-9-thread-3 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.954 pool-9-thread-3 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.954 pool-9-thread-3 INFO AppInfoParser: Kafka startTimeMs: 1613919098954
21/02/21 22:51:38.955 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-488, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.955 pool-9-thread-3 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.955 pool-9-thread-3 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-488, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.955 pool-9-thread-17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.958 pool-9-thread-3 INFO Metadata: [Consumer clientId=consumer-groupId-488, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.960 pool-9-thread-17 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.960 pool-9-thread-17 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.960 pool-9-thread-17 INFO AppInfoParser: Kafka startTimeMs: 1613919098960
21/02/21 22:51:38.960 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-489, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.961 pool-9-thread-17 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.961 pool-9-thread-17 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-489, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.961 pool-9-thread-6 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.963 pool-9-thread-17 INFO Metadata: [Consumer clientId=consumer-groupId-489, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.965 pool-9-thread-6 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.965 pool-9-thread-6 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.965 pool-9-thread-6 INFO AppInfoParser: Kafka startTimeMs: 1613919098965
21/02/21 22:51:38.965 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-490, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.965 pool-9-thread-6 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.965 pool-9-thread-6 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-490, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.966 pool-9-thread-8 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.967 pool-9-thread-6 INFO Metadata: [Consumer clientId=consumer-groupId-490, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.969 pool-9-thread-8 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.969 pool-9-thread-8 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.969 pool-9-thread-8 INFO AppInfoParser: Kafka startTimeMs: 1613919098969
21/02/21 22:51:38.969 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-491, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.969 pool-9-thread-8 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.969 pool-9-thread-8 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-491, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.969 pool-9-thread-13 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.971 pool-9-thread-8 INFO Metadata: [Consumer clientId=consumer-groupId-491, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.972 pool-9-thread-13 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.972 pool-9-thread-13 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.972 pool-9-thread-13 INFO AppInfoParser: Kafka startTimeMs: 1613919098972
21/02/21 22:51:38.973 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-492, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.973 pool-9-thread-13 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.973 pool-9-thread-13 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-492, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.974 pool-9-thread-9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.976 pool-9-thread-13 INFO Metadata: [Consumer clientId=consumer-groupId-492, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.978 pool-9-thread-9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.978 pool-9-thread-9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.979 pool-9-thread-9 INFO AppInfoParser: Kafka startTimeMs: 1613919098978
21/02/21 22:51:38.979 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-493, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.979 pool-9-thread-9 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.979 pool-9-thread-9 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-493, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.979 pool-9-thread-5 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.980 pool-9-thread-9 INFO Metadata: [Consumer clientId=consumer-groupId-493, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.981 pool-9-thread-5 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.981 pool-9-thread-5 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.981 pool-9-thread-5 INFO AppInfoParser: Kafka startTimeMs: 1613919098981
21/02/21 22:51:38.981 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-494, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.981 pool-9-thread-5 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.981 pool-9-thread-5 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-494, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.982 pool-9-thread-4 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.985 pool-9-thread-5 INFO Metadata: [Consumer clientId=consumer-groupId-494, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.985 pool-9-thread-4 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.985 pool-9-thread-4 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.985 pool-9-thread-4 INFO AppInfoParser: Kafka startTimeMs: 1613919098985
21/02/21 22:51:38.985 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-495, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.985 pool-9-thread-4 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.985 pool-9-thread-4 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-495, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.988 pool-9-thread-4 INFO Metadata: [Consumer clientId=consumer-groupId-495, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.989 pool-9-thread-84 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.992 pool-9-thread-84 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.992 pool-9-thread-84 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.992 pool-9-thread-84 INFO AppInfoParser: Kafka startTimeMs: 1613919098992
21/02/21 22:51:38.992 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-496, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.993 pool-9-thread-84 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.993 pool-9-thread-84 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-496, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.993 pool-9-thread-73 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.995 pool-9-thread-84 INFO Metadata: [Consumer clientId=consumer-groupId-496, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:38.996 pool-9-thread-73 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:38.996 pool-9-thread-73 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:38.996 pool-9-thread-73 INFO AppInfoParser: Kafka startTimeMs: 1613919098996
21/02/21 22:51:38.996 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-497, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:38.996 pool-9-thread-73 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:38.997 pool-9-thread-73 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-497, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:38.997 pool-9-thread-76 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:38.998 pool-9-thread-73 INFO Metadata: [Consumer clientId=consumer-groupId-497, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.000 pool-9-thread-76 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.000 pool-9-thread-76 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.000 pool-9-thread-76 INFO AppInfoParser: Kafka startTimeMs: 1613919099000
21/02/21 22:51:39.000 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-498, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.000 pool-9-thread-76 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.000 pool-9-thread-76 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-498, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.000 pool-9-thread-64 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:39.002 pool-9-thread-76 INFO Metadata: [Consumer clientId=consumer-groupId-498, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.006 pool-9-thread-64 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.006 pool-9-thread-64 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.006 pool-9-thread-64 INFO AppInfoParser: Kafka startTimeMs: 1613919099006
21/02/21 22:51:39.006 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-499, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.006 pool-9-thread-64 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.006 pool-9-thread-64 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-499, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.007 pool-9-thread-70 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:39.008 pool-9-thread-64 INFO Metadata: [Consumer clientId=consumer-groupId-499, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.008 pool-9-thread-70 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.008 pool-9-thread-70 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.008 pool-9-thread-70 INFO AppInfoParser: Kafka startTimeMs: 1613919099008
21/02/21 22:51:39.009 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-500, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.009 pool-9-thread-70 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.009 pool-9-thread-70 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-500, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.009 pool-9-thread-77 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:39.012 pool-9-thread-70 INFO Metadata: [Consumer clientId=consumer-groupId-500, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.012 pool-9-thread-77 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.012 pool-9-thread-77 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.012 pool-9-thread-77 INFO AppInfoParser: Kafka startTimeMs: 1613919099012
21/02/21 22:51:39.012 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-501, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.012 pool-9-thread-77 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.012 pool-9-thread-77 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-501, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.013 pool-9-thread-79 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:39.015 pool-9-thread-77 INFO Metadata: [Consumer clientId=consumer-groupId-501, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.016 pool-9-thread-79 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.016 pool-9-thread-79 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.016 pool-9-thread-79 INFO AppInfoParser: Kafka startTimeMs: 1613919099016
21/02/21 22:51:39.017 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-502, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.017 pool-9-thread-79 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.017 pool-9-thread-79 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-502, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.018 pool-9-thread-82 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:56738]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

21/02/21 22:51:39.020 pool-9-thread-79 INFO Metadata: [Consumer clientId=consumer-groupId-502, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.020 pool-9-thread-82 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:39.020 pool-9-thread-82 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:39.020 pool-9-thread-82 INFO AppInfoParser: Kafka startTimeMs: 1613919099020
21/02/21 22:51:39.020 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-503, groupId=groupId] Subscribed to partition(s): topic1286212732-0
21/02/21 22:51:39.020 pool-9-thread-82 INFO InternalKafkaConsumer: Initial fetch for groupId topic1286212732-0 0
21/02/21 22:51:39.020 pool-9-thread-82 INFO KafkaConsumer: [Consumer clientId=consumer-groupId-503, groupId=groupId] Seeking to offset 0 for partition topic1286212732-0
21/02/21 22:51:39.024 pool-9-thread-82 INFO Metadata: [Consumer clientId=consumer-groupId-503, groupId=groupId] Cluster ID: xpuAH50kSf-lHJbF1ZbxLg
21/02/21 22:51:39.030 ScalaTest-main-running-KafkaDataConsumerSuite INFO KafkaDataConsumerSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaDataConsumerSuite: 'concurrent use of KafkaDataConsumer' =====

21/02/21 22:51:39.043 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shutting down
21/02/21 22:51:39.045 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Starting controlled shutdown
21/02/21 22:51:39.055 controller-event-thread INFO KafkaController: [Controller id=0] Shutting down broker 0
21/02/21 22:51:39.065 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Controlled shutdown succeeded
21/02/21 22:51:39.069 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutting down
21/02/21 22:51:39.070 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutdown completed
21/02/21 22:51:39.070 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Stopped
21/02/21 22:51:39.070 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopping socket server request processors
21/02/21 22:51:39.081 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopped socket server request processors
21/02/21 22:51:39.082 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shutting down
21/02/21 22:51:39.086 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shut down completely
21/02/21 22:51:39.090 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutting down
21/02/21 22:51:39.123 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Stopped
21/02/21 22:51:39.123 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutdown completed
21/02/21 22:51:39.124 ScalaTest-main-running-DiscoverySuite INFO KafkaApis: [KafkaApi-0] Shutdown complete.
21/02/21 22:51:39.124 controller-event-thread INFO KafkaController: [Controller id=0] Processing automatic preferred replica leader election
21/02/21 22:51:39.125 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutting down
21/02/21 22:51:39.323 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Stopped
21/02/21 22:51:39.323 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutdown completed
21/02/21 22:51:39.327 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutting down.
21/02/21 22:51:39.329 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
21/02/21 22:51:39.329 ScalaTest-main-running-DiscoverySuite INFO TransactionStateManager: [Transaction State Manager 0]: Shutdown complete
21/02/21 22:51:39.330 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutting down
21/02/21 22:51:39.333 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Stopped
21/02/21 22:51:39.333 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutdown completed
21/02/21 22:51:39.335 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutdown complete.
21/02/21 22:51:39.336 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutting down.
21/02/21 22:51:39.336 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutting down
21/02/21 22:51:39.535 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Stopped
21/02/21 22:51:39.535 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutdown completed
21/02/21 22:51:39.536 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutting down
21/02/21 22:51:39.737 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Stopped
21/02/21 22:51:39.737 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutdown completed
21/02/21 22:51:39.738 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutdown complete.
21/02/21 22:51:39.740 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shutting down
21/02/21 22:51:39.741 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutting down
21/02/21 22:51:39.741 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Stopped
21/02/21 22:51:39.741 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutdown completed
21/02/21 22:51:39.742 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutting down
21/02/21 22:51:39.745 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutdown completed
21/02/21 22:51:39.746 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutting down
21/02/21 22:51:39.746 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutdown completed
21/02/21 22:51:39.746 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutting down
21/02/21 22:51:39.936 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Stopped
21/02/21 22:51:39.936 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutdown completed
21/02/21 22:51:39.936 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutting down
21/02/21 22:51:40.136 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Stopped
21/02/21 22:51:40.136 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutdown completed
21/02/21 22:51:40.136 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutting down
21/02/21 22:51:40.327 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Stopped
21/02/21 22:51:40.327 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutdown completed
21/02/21 22:51:40.327 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutting down
21/02/21 22:51:40.526 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Stopped
21/02/21 22:51:40.526 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutdown completed
21/02/21 22:51:40.535 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shut down completely
21/02/21 22:51:40.535 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutting down.
21/02/21 22:51:40.537 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:51:40.537 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:51:40.537 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:51:40.537 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:51:40.542 pool-10-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topic1286212732-0] Writing producer snapshot at offset 1000
21/02/21 22:51:40.599 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutdown complete.
21/02/21 22:51:40.599 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutting down
21/02/21 22:51:40.600 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Stopped
21/02/21 22:51:40.600 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutdown completed
21/02/21 22:51:40.604 ScalaTest-main-running-DiscoverySuite INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Stopped partition state machine
21/02/21 22:51:40.605 ScalaTest-main-running-DiscoverySuite INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Stopped replica state machine
21/02/21 22:51:40.606 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutting down
21/02/21 22:51:40.606 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutdown completed
21/02/21 22:51:40.606 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Stopped
21/02/21 22:51:40.609 ScalaTest-main-running-DiscoverySuite INFO KafkaController: [Controller id=0] Resigned
21/02/21 22:51:40.609 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closing.
21/02/21 22:51:40.610 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c512fba00001
21/02/21 22:51:40.612 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c512fba00001 closed
21/02/21 22:51:40.613 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c512fba00001
21/02/21 22:51:40.614 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56734 which had sessionid 0x177c512fba00001
21/02/21 22:51:40.614 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closed.
21/02/21 22:51:40.615 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutting down
21/02/21 22:51:40.818 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Stopped
21/02/21 22:51:40.818 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutdown completed
21/02/21 22:51:40.818 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutting down
21/02/21 22:51:41.818 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Stopped
21/02/21 22:51:41.818 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutdown completed
21/02/21 22:51:41.818 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutting down
21/02/21 22:51:42.819 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Stopped
21/02/21 22:51:42.819 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutdown completed
21/02/21 22:51:42.820 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutting down socket server
21/02/21 22:51:42.852 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutdown completed
21/02/21 22:51:42.857 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shut down completed
21/02/21 22:51:42.874 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closing.
21/02/21 22:51:42.874 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c512fba00000
21/02/21 22:51:42.878 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c512fba00000 closed
21/02/21 22:51:42.878 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c512fba00000
21/02/21 22:51:42.878 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closed.
21/02/21 22:51:42.878 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:56731 which had sessionid 0x177c512fba00000
21/02/21 22:51:42.879 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method
21/02/21 22:51:42.879 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: shutting down
21/02/21 22:51:42.879 ScalaTest-main-running-DiscoverySuite INFO SessionTrackerImpl: Shutting down
21/02/21 22:51:42.879 ScalaTest-main-running-DiscoverySuite INFO PrepRequestProcessor: Shutting down
21/02/21 22:51:42.879 ScalaTest-main-running-DiscoverySuite INFO SyncRequestProcessor: Shutting down
21/02/21 22:51:42.879 ProcessThread(sid:0 cport:56728): INFO PrepRequestProcessor: PrepRequestProcessor exited loop!
21/02/21 22:51:42.879 SyncThread:0 INFO SyncRequestProcessor: SyncRequestProcessor exited!
21/02/21 22:51:42.880 ScalaTest-main-running-DiscoverySuite INFO FinalRequestProcessor: shutdown of request processor complete
21/02/21 22:51:42.885 ScalaTest-main-running-DiscoverySuite WARN KafkaTestUtils: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-fa748bea-6e08-420f-9dd0-681fe31ab7c1\version-2\log.1
21/02/21 22:51:42.893 ScalaTest-main-running-DiscoverySuite WARN KafkaDataConsumerSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.streaming.kafka010.KafkaDataConsumerSuite, thread names: metrics-meter-tick-thread-1, metrics-meter-tick-thread-2, SessionTracker, ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:56728) =====

21/02/21 22:51:42.906 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-18745cb3-16c8-4724-bded-25c36f16ea30\version-2 snapdir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-461295f0-b735-4d73-9c93-fc22d9aeef45\version-2
21/02/21 22:51:42.907 ScalaTest-main-running-DiscoverySuite INFO NIOServerCnxnFactory: binding to port /127.0.0.1:0
21/02/21 22:51:42.911 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Initializing a new session to 127.0.0.1:58769.
21/02/21 22:51:42.911 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:58769 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@303f1234
21/02/21 22:51:42.912 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Waiting until connected.
21/02/21 22:51:42.912 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:58769. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:42.913 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:58769, initiating session
21/02/21 22:51:42.913 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:58772
21/02/21 22:51:42.913 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:58772
21/02/21 22:51:42.913 SyncThread:0 INFO FileTxnLog: Creating new log file: log.1
21/02/21 22:51:42.917 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c51327bc0000 with negotiated timeout 10000 for client /127.0.0.1:58772
21/02/21 22:51:42.917 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:58769, sessionid = 0x177c51327bc0000, negotiated timeout = 10000
21/02/21 22:51:42.918 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Connected.
21/02/21 22:51:42.921 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: starting
21/02/21 22:51:42.921 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Connecting to zookeeper on 127.0.0.1:58769
21/02/21 22:51:42.921 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:58769.
21/02/21 22:51:42.921 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:58769 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@24d61e4
21/02/21 22:51:42.923 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Waiting until connected.
21/02/21 22:51:42.923 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:58769. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:42.924 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:58769, initiating session
21/02/21 22:51:42.924 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:58775
21/02/21 22:51:42.924 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:58775
21/02/21 22:51:42.926 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c51327bc0001 with negotiated timeout 6000 for client /127.0.0.1:58775
21/02/21 22:51:42.926 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:58769, sessionid = 0x177c51327bc0001, negotiated timeout = 6000
21/02/21 22:51:42.927 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Connected.
21/02/21 22:51:42.932 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:create cxid:0x2 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
21/02/21 22:51:42.942 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:create cxid:0x6 zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
21/02/21 22:51:42.950 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:create cxid:0x9 zxid:0xb txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
21/02/21 22:51:42.978 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:create cxid:0x15 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
21/02/21 22:51:42.985 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Cluster ID = 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:42.985 ScalaTest-main-running-DiscoverySuite WARN BrokerMetadataCheckpoint: No meta.properties file under dir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\meta.properties
21/02/21 22:51:42.988 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:58769
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:42.991 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:58769
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:42.994 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Starting
21/02/21 22:51:42.995 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Starting
21/02/21 22:51:42.995 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Starting
21/02/21 22:51:43.000 ScalaTest-main-running-DiscoverySuite INFO LogManager: Loading logs.
21/02/21 22:51:43.000 SessionTracker INFO SessionTrackerImpl: SessionTrackerImpl exited loop!
21/02/21 22:51:43.001 ScalaTest-main-running-DiscoverySuite INFO LogManager: Logs loading complete in 1 ms.
21/02/21 22:51:43.002 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log cleanup with a period of 300000 ms.
21/02/21 22:51:43.003 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log flusher with a default period of 9223372036854775807 ms.
21/02/21 22:51:43.003 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Starting the log cleaner
21/02/21 22:51:43.041 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:51:43.269 ScalaTest-main-running-DiscoverySuite INFO Acceptor: Awaiting socket connections on 127.0.0.1:58778.
21/02/21 22:51:43.274 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(127.0.0.1,0,ListenerName(PLAINTEXT),PLAINTEXT)
21/02/21 22:51:43.274 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started 1 acceptor threads for data-plane
21/02/21 22:51:43.275 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Starting
21/02/21 22:51:43.276 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Starting
21/02/21 22:51:43.277 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Starting
21/02/21 22:51:43.278 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Starting
21/02/21 22:51:43.283 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Starting
21/02/21 22:51:43.284 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Creating /brokers/ids/0 (is it secure? false)
21/02/21 22:51:43.289 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Stat of the created znode at /brokers/ids/0 is: 25,25,1613919103285,1613919103285,1,0,0,105769802328178689,190,0,25

21/02/21 22:51:43.289 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(127.0.0.1,58778,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25
21/02/21 22:51:43.297 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Starting
21/02/21 22:51:43.298 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Starting
21/02/21 22:51:43.299 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Starting
21/02/21 22:51:43.300 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Starting
21/02/21 22:51:43.301 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Starting up.
21/02/21 22:51:43.301 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Startup complete.
21/02/21 22:51:43.302 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds.
21/02/21 22:51:43.303 controller-event-thread INFO KafkaZkClient: Successfully created /controller_epoch with initial epoch 0
21/02/21 22:51:43.306 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
21/02/21 22:51:43.309 controller-event-thread INFO KafkaController: [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
21/02/21 22:51:43.309 controller-event-thread INFO KafkaController: [Controller id=0] Registering handlers
21/02/21 22:51:43.311 controller-event-thread INFO KafkaController: [Controller id=0] Deleting log dir event notifications
21/02/21 22:51:43.311 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Starting up.
21/02/21 22:51:43.311 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Startup complete.
21/02/21 22:51:43.312 controller-event-thread INFO KafkaController: [Controller id=0] Deleting isr change notifications
21/02/21 22:51:43.312 controller-event-thread INFO KafkaController: [Controller id=0] Initializing controller context
21/02/21 22:51:43.313 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Starting
21/02/21 22:51:43.314 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Starting
21/02/21 22:51:43.316 controller-event-thread INFO KafkaController: [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
21/02/21 22:51:43.316 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Starting
21/02/21 22:51:43.319 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started data-plane processors for 1 acceptors
21/02/21 22:51:43.320 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:43.320 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:43.320 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka startTimeMs: 1613919103320
21/02/21 22:51:43.320 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] started
21/02/21 22:51:43.320 ScalaTest-main-running-DiscoverySuite INFO Utils: Successfully started service 'KafkaBroker' on port 58778.
21/02/21 22:51:43.321 controller-event-thread INFO KafkaController: [Controller id=0] Currently active brokers in the cluster: Set(0)
21/02/21 22:51:43.321 controller-event-thread INFO KafkaController: [Controller id=0] Currently shutting brokers in the cluster: Set()
21/02/21 22:51:43.321 controller-event-thread INFO KafkaController: [Controller id=0] Current list of topics in the cluster: Set()
21/02/21 22:51:43.321 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Starting
21/02/21 22:51:43.321 controller-event-thread INFO KafkaController: [Controller id=0] Fetching topic deletions in progress
21/02/21 22:51:43.322 controller-event-thread INFO KafkaController: [Controller id=0] List of topics to be deleted: 
21/02/21 22:51:43.322 controller-event-thread INFO KafkaController: [Controller id=0] List of topics ineligible for deletion: 
21/02/21 22:51:43.322 controller-event-thread INFO KafkaController: [Controller id=0] Initializing topic deletion manager
21/02/21 22:51:43.322 controller-event-thread INFO TopicDeletionManager: [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
21/02/21 22:51:43.322 controller-event-thread INFO KafkaController: [Controller id=0] Sending update metadata request
21/02/21 22:51:43.323 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'basic stream receiving with multiple topics and smallest starting offset' =====

21/02/21 22:51:43.323 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Initializing replica state
21/02/21 22:51:43.323 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering online replica state changes
21/02/21 22:51:43.323 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
21/02/21 22:51:43.323 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Initializing partition state
21/02/21 22:51:43.324 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Triggering online partition state changes
21/02/21 22:51:43.324 controller-event-thread INFO KafkaController: [Controller id=0] Ready to serve as the new controller with epoch 1
21/02/21 22:51:43.325 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Controller 0 connected to 127.0.0.1:58778 (id: 0 rack: null) for sending state change requests
21/02/21 22:51:43.326 controller-event-thread INFO KafkaController: [Controller id=0] Partitions undergoing preferred replica election: 
21/02/21 22:51:43.326 controller-event-thread INFO KafkaController: [Controller id=0] Partitions that completed preferred replica election: 
21/02/21 22:51:43.326 controller-event-thread INFO KafkaController: [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
21/02/21 22:51:43.326 controller-event-thread INFO KafkaController: [Controller id=0] Resuming preferred replica election for partitions: 
21/02/21 22:51:43.326 controller-event-thread INFO KafkaController: [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
21/02/21 22:51:43.327 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:multi cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
21/02/21 22:51:43.327 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic basic1 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:43.328 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x4 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/basic1 Error:KeeperErrorCode = NoNode for /config/topics/basic1
21/02/21 22:51:43.328 controller-event-thread INFO KafkaController: [Controller id=0] Starting the controller scheduler
21/02/21 22:51:43.338 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(basic1)], deleted topics: [Set()], new partition replica assignment [Map(basic1-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:43.338 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for basic1-0
21/02/21 22:51:43.348 data-plane-kafka-request-handler-0 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(basic1-0)
21/02/21 22:51:43.355 data-plane-kafka-request-handler-0 INFO Log: [Log partition=basic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:43.357 data-plane-kafka-request-handler-0 INFO Log: [Log partition=basic1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
21/02/21 22:51:43.358 data-plane-kafka-request-handler-0 INFO LogManager: Created log for partition basic1-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\basic1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:43.359 data-plane-kafka-request-handler-0 INFO Partition: [Partition basic1-0 broker=0] No checkpointed highwatermark is found for partition basic1-0
21/02/21 22:51:43.359 data-plane-kafka-request-handler-0 INFO Partition: [Partition basic1-0 broker=0] Log loaded for partition basic1-0 with initial high watermark 0
21/02/21 22:51:43.359 data-plane-kafka-request-handler-0 INFO Partition: [Partition basic1-0 broker=0] basic1-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:43.441 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:43.445 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:43.446 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:43.446 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919103445
21/02/21 22:51:43.457 kafka-producer-network-thread | producer-2 INFO Metadata: [Producer clientId=producer-2] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:43.461 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:43.488 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic basic2 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:43.489 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0xb zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/basic2 Error:KeeperErrorCode = NoNode for /config/topics/basic2
21/02/21 22:51:43.506 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(basic2)], deleted topics: [Set()], new partition replica assignment [Map(basic2-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:43.507 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for basic2-0
21/02/21 22:51:43.526 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(basic2-0)
21/02/21 22:51:43.533 data-plane-kafka-request-handler-2 INFO Log: [Log partition=basic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:43.535 data-plane-kafka-request-handler-2 INFO Log: [Log partition=basic2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
21/02/21 22:51:43.536 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition basic2-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\basic2-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:43.536 data-plane-kafka-request-handler-2 INFO Partition: [Partition basic2-0 broker=0] No checkpointed highwatermark is found for partition basic2-0
21/02/21 22:51:43.537 data-plane-kafka-request-handler-2 INFO Partition: [Partition basic2-0 broker=0] Log loaded for partition basic2-0 with initial high watermark 0
21/02/21 22:51:43.537 data-plane-kafka-request-handler-2 INFO Partition: [Partition basic2-0 broker=0] basic2-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:43.605 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:43.610 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:43.610 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:43.610 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919103610
21/02/21 22:51:43.619 kafka-producer-network-thread | producer-3 INFO Metadata: [Producer clientId=producer-3] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:43.622 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:43.648 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic basic3 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:43.650 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x12 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics/basic3 Error:KeeperErrorCode = NoNode for /config/topics/basic3
21/02/21 22:51:43.667 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(basic3)], deleted topics: [Set()], new partition replica assignment [Map(basic3-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:43.668 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for basic3-0
21/02/21 22:51:43.694 data-plane-kafka-request-handler-6 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(basic3-0)
21/02/21 22:51:43.713 data-plane-kafka-request-handler-6 INFO Log: [Log partition=basic3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:43.723 data-plane-kafka-request-handler-6 INFO Log: [Log partition=basic3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms
21/02/21 22:51:43.724 data-plane-kafka-request-handler-6 INFO LogManager: Created log for partition basic3-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\basic3-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:43.724 data-plane-kafka-request-handler-6 INFO Partition: [Partition basic3-0 broker=0] No checkpointed highwatermark is found for partition basic3-0
21/02/21 22:51:43.724 data-plane-kafka-request-handler-6 INFO Partition: [Partition basic3-0 broker=0] Log loaded for partition basic3-0 with initial high watermark 0
21/02/21 22:51:43.724 data-plane-kafka-request-handler-6 INFO Partition: [Partition basic3-0 broker=0] basic3-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:43.765 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:43.767 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:43.767 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:43.767 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919103767
21/02/21 22:51:43.770 kafka-producer-network-thread | producer-4 INFO Metadata: [Producer clientId=producer-4] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:43.771 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:43.839 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:43.918 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:43.918 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:43.919 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:43.919 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:44.002 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:44.002 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:44.003 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:44.003 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:44.003 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:46.172 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 58822.
21/02/21 22:51:46.207 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:46.248 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:46.280 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:46.281 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:46.286 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:46.301 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-6095d3db-47a8-489a-a855-1c46e2a40b13
21/02/21 22:51:46.330 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:46.348 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:46.557 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:46.603 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58841.
21/02/21 22:51:46.604 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:58841
21/02/21 22:51:46.605 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:46.616 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58841, None)
21/02/21 22:51:46.621 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:58841 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 58841, None)
21/02/21 22:51:46.626 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58841, None)
21/02/21 22:51:46.627 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 58841, None)
21/02/21 22:51:46.699 ScalaTest-main-running-DirectKafkaStreamSuite INFO log: Logging initialized @28326ms to org.sparkproject.jetty.util.log.Slf4jLog
21/02/21 22:51:47.156 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:47.157 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:47.158 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-2118558971-1613919103780
21/02/21 22:51:47.158 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:47.331 streaming-start INFO DirectKafkaInputDStream: Slide time = 1000 ms
21/02/21 22:51:47.331 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Remember interval = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@125204a
21/02/21 22:51:47.332 streaming-start INFO TransformedDStream: Slide time = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO TransformedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:47.332 streaming-start INFO TransformedDStream: Checkpoint interval = null
21/02/21 22:51:47.332 streaming-start INFO TransformedDStream: Remember interval = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO TransformedDStream: Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@636cf574
21/02/21 22:51:47.332 streaming-start INFO ForEachDStream: Slide time = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:47.332 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:47.332 streaming-start INFO ForEachDStream: Remember interval = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5bcbf67d
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Slide time = 1000 ms
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:47.332 streaming-start INFO DirectKafkaInputDStream: Remember interval = 1000 ms
21/02/21 22:51:47.333 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@125204a
21/02/21 22:51:47.333 streaming-start INFO ForEachDStream: Slide time = 1000 ms
21/02/21 22:51:47.333 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:47.333 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:47.333 streaming-start INFO ForEachDStream: Remember interval = 1000 ms
21/02/21 22:51:47.333 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2a076b84
21/02/21 22:51:47.421 scala-execution-context-global-260 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-2118558971-1613919103780
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:47.424 scala-execution-context-global-260 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:47.424 scala-execution-context-global-260 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:47.425 scala-execution-context-global-260 INFO AppInfoParser: Kafka startTimeMs: 1613919107424
21/02/21 22:51:47.425 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Subscribed to topic(s): basic1, basic2, basic3
21/02/21 22:51:47.431 scala-execution-context-global-260 INFO Metadata: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:47.437 data-plane-kafka-request-handler-6 INFO AdminZkClient: Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:47.438 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:setData cxid:0x51 zxid:0x30 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
21/02/21 22:51:47.448 data-plane-kafka-request-handler-6 INFO KafkaApis: [KafkaApi-0] Auto creation of topic __consumer_offsets with 1 partitions and replication factor 1 is successful
21/02/21 22:51:47.449 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:47.449 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for __consumer_offsets-0
21/02/21 22:51:47.461 data-plane-kafka-request-handler-5 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-0)
21/02/21 22:51:47.468 data-plane-kafka-request-handler-5 INFO Log: [Log partition=__consumer_offsets-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:47.471 data-plane-kafka-request-handler-5 INFO Log: [Log partition=__consumer_offsets-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:47.471 data-plane-kafka-request-handler-5 INFO LogManager: Created log for partition __consumer_offsets-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:47.472 data-plane-kafka-request-handler-5 INFO Partition: [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
21/02/21 22:51:47.472 data-plane-kafka-request-handler-5 INFO Partition: [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
21/02/21 22:51:47.472 data-plane-kafka-request-handler-5 INFO Partition: [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:47.481 data-plane-kafka-request-handler-5 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0
21/02/21 22:51:47.488 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds.
21/02/21 22:51:47.538 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:47.543 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] (Re-)joining group
21/02/21 22:51:47.575 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] (Re-)joining group
21/02/21 22:51:47.584 data-plane-kafka-request-handler-6 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-2118558971-1613919103780 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-2118558971-1613919103780-504-94498c8b-7151-4374-9553-eddc2099eea4 with group instanceid None)
21/02/21 22:51:47.606 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-2118558971-1613919103780 generation 1 (__consumer_offsets-0)
21/02/21 22:51:47.613 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Finished assignment for group at generation 1: {consumer-test-consumer-2118558971-1613919103780-504-94498c8b-7151-4374-9553-eddc2099eea4=Assignment(partitions=[basic3-0, basic2-0, basic1-0])}
21/02/21 22:51:47.618 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-2118558971-1613919103780 for generation 1
21/02/21 22:51:47.639 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Successfully joined group with generation 1
21/02/21 22:51:47.642 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Adding newly assigned partitions: basic3-0, basic2-0, basic1-0
21/02/21 22:51:47.652 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Found no committed offset for partition basic3-0
21/02/21 22:51:47.653 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Found no committed offset for partition basic2-0
21/02/21 22:51:47.653 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Found no committed offset for partition basic1-0
21/02/21 22:51:47.659 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to offset 2 for partition basic3-0
21/02/21 22:51:47.671 scala-execution-context-global-260 INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic2-0 to offset 0.
21/02/21 22:51:47.671 scala-execution-context-global-260 INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic1-0 to offset 0.
21/02/21 22:51:47.672 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919108000
21/02/21 22:51:47.673 streaming-start INFO JobGenerator: Started JobGenerator at 1613919108000 ms
21/02/21 22:51:47.676 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:47.684 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:48.023 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic3-0
21/02/21 22:51:48.023 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic2-0
21/02/21 22:51:48.023 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic1-0
21/02/21 22:51:48.027 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic3-0 to offset 16.
21/02/21 22:51:48.027 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic2-0 to offset 16.
21/02/21 22:51:48.027 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic1-0 to offset 16.
21/02/21 22:51:48.109 JobGenerator INFO JobScheduler: Added jobs for time 1613919108000 ms
21/02/21 22:51:48.113 JobScheduler INFO JobScheduler: Starting job streaming job 1613919108000 ms.0 from job set of time 1613919108000 ms
21/02/21 22:51:48.114 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic3 0 2 16
21/02/21 22:51:48.114 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic1 0 0 16
21/02/21 22:51:48.114 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic2 0 0 16
21/02/21 22:51:48.221 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:133
21/02/21 22:51:48.238 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at DirectKafkaStreamSuite.scala:133) with 3 output partitions
21/02/21 22:51:48.238 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:133)
21/02/21 22:51:48.239 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:48.240 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:48.247 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:133), which has no missing parents
21/02/21 22:51:48.326 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.6 KiB, free 2.1 GiB)
21/02/21 22:51:48.331 controller-event-thread INFO KafkaController: [Controller id=0] Processing automatic preferred replica leader election
21/02/21 22:51:48.396 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2.1 GiB)
21/02/21 22:51:48.400 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:58841 (size: 3.2 KiB, free: 2.1 GiB)
21/02/21 22:51:48.402 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:48.422 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:133) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:48.423 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
21/02/21 22:51:48.484 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:48.488 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:48.489 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:48.505 Executor task launch worker for task 1 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
21/02/21 22:51:48.505 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:48.505 Executor task launch worker for task 2 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
21/02/21 22:51:48.994 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic basic2, partition 0 offsets 0 -> 16
21/02/21 22:51:48.994 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic basic1, partition 0 offsets 0 -> 16
21/02/21 22:51:48.994 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic basic3, partition 0 offsets 2 -> 16
21/02/21 22:51:48.997 Executor task launch worker for task 2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-2118558971-1613919103780
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:49.000 Executor task launch worker for task 2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:49.000 Executor task launch worker for task 2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:49.000 Executor task launch worker for task 2 INFO AppInfoParser: Kafka startTimeMs: 1613919109000
21/02/21 22:51:49.001 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-505, groupId=spark-executor-test-consumer-2118558971-1613919103780] Subscribed to partition(s): basic2-0
21/02/21 22:51:49.001 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic3-0
21/02/21 22:51:49.001 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic1-0
21/02/21 22:51:49.001 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Seeking to LATEST offset of partition basic2-0
21/02/21 22:51:49.001 Executor task launch worker for task 2 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic2-0 0
21/02/21 22:51:49.001 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-505, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 0 for partition basic2-0
21/02/21 22:51:49.001 Executor task launch worker for task 1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-2118558971-1613919103780
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:49.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic3-0 to offset 16.
21/02/21 22:51:49.003 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic2-0 to offset 16.
21/02/21 22:51:49.003 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Resetting offset for partition basic1-0 to offset 16.
21/02/21 22:51:49.004 Executor task launch worker for task 1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:49.004 Executor task launch worker for task 1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:49.004 Executor task launch worker for task 1 INFO AppInfoParser: Kafka startTimeMs: 1613919109004
21/02/21 22:51:49.004 Executor task launch worker for task 2 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-505, groupId=spark-executor-test-consumer-2118558971-1613919103780] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:49.004 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-506, groupId=spark-executor-test-consumer-2118558971-1613919103780] Subscribed to partition(s): basic1-0
21/02/21 22:51:49.005 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic1-0 0
21/02/21 22:51:49.005 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-506, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 0 for partition basic1-0
21/02/21 22:51:49.005 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-2118558971-1613919103780
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:49.008 Executor task launch worker for task 1 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-506, groupId=spark-executor-test-consumer-2118558971-1613919103780] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:49.009 JobGenerator INFO JobScheduler: Added jobs for time 1613919109000 ms
21/02/21 22:51:49.010 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:49.010 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:49.010 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919109010
21/02/21 22:51:49.010 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-507, groupId=spark-executor-test-consumer-2118558971-1613919103780] Subscribed to partition(s): basic3-0
21/02/21 22:51:49.011 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic3-0 2
21/02/21 22:51:49.011 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-507, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 2 for partition basic3-0
21/02/21 22:51:49.015 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-507, groupId=spark-executor-test-consumer-2118558971-1613919103780] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:49.052 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1057 bytes result sent to driver
21/02/21 22:51:49.052 Executor task launch worker for task 2 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1014 bytes result sent to driver
21/02/21 22:51:49.052 Executor task launch worker for task 1 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1057 bytes result sent to driver
21/02/21 22:51:49.063 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 590 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:49.066 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 576 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:49.066 task-result-getter-1 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 577 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:49.066 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:49.074 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:133) finished in 0.804 s
21/02/21 22:51:49.080 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:49.080 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:49.083 streaming-job-executor-0 INFO DAGScheduler: Job 0 finished: collect at DirectKafkaStreamSuite.scala:133, took 0.861513 s
21/02/21 22:51:49.088 JobScheduler INFO JobScheduler: Finished job streaming job 1613919108000 ms.0 from job set of time 1613919108000 ms
21/02/21 22:51:49.088 JobScheduler INFO JobScheduler: Starting job streaming job 1613919108000 ms.1 from job set of time 1613919108000 ms
21/02/21 22:51:49.105 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:151
21/02/21 22:51:49.106 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at DirectKafkaStreamSuite.scala:151) with 3 output partitions
21/02/21 22:51:49.106 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:151)
21/02/21 22:51:49.106 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:49.106 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:49.107 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at map at DirectKafkaStreamSuite.scala:151), which has no missing parents
21/02/21 22:51:49.112 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.0 KiB, free 2.1 GiB)
21/02/21 22:51:49.113 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 2.1 GiB)
21/02/21 22:51:49.115 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:58841 (size: 3.0 KiB, free: 2.1 GiB)
21/02/21 22:51:49.115 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:49.116 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at map at DirectKafkaStreamSuite.scala:151) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:49.116 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
21/02/21 22:51:49.119 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.119 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.120 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.120 Executor task launch worker for task 5 INFO Executor: Running task 2.0 in stage 1.0 (TID 5)
21/02/21 22:51:49.120 Executor task launch worker for task 3 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)
21/02/21 22:51:49.122 Executor task launch worker for task 4 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)
21/02/21 22:51:49.126 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic basic3, partition 0 offsets 2 -> 16
21/02/21 22:51:49.126 Executor task launch worker for task 5 INFO KafkaRDD: Computing topic basic2, partition 0 offsets 0 -> 16
21/02/21 22:51:49.127 Executor task launch worker for task 3 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic3-0 2
21/02/21 22:51:49.127 Executor task launch worker for task 5 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic2-0 0
21/02/21 22:51:49.127 Executor task launch worker for task 3 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-507, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 2 for partition basic3-0
21/02/21 22:51:49.127 Executor task launch worker for task 4 INFO KafkaRDD: Computing topic basic1, partition 0 offsets 0 -> 16
21/02/21 22:51:49.127 Executor task launch worker for task 5 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-505, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 0 for partition basic2-0
21/02/21 22:51:49.127 Executor task launch worker for task 4 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-2118558971-1613919103780 basic1-0 0
21/02/21 22:51:49.127 Executor task launch worker for task 4 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-2118558971-1613919103780-506, groupId=spark-executor-test-consumer-2118558971-1613919103780] Seeking to offset 0 for partition basic1-0
21/02/21 22:51:49.664 Executor task launch worker for task 4 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 985 bytes result sent to driver
21/02/21 22:51:49.670 Executor task launch worker for task 3 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 963 bytes result sent to driver
21/02/21 22:51:49.672 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 553 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:49.672 Executor task launch worker for task 5 INFO Executor: Finished task 2.0 in stage 1.0 (TID 5). 1028 bytes result sent to driver
21/02/21 22:51:49.673 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 555 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:49.682 task-result-getter-2 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 563 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:49.683 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:51:49.684 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:151) finished in 0.575 s
21/02/21 22:51:49.685 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:49.685 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:51:49.686 streaming-job-executor-0 INFO DAGScheduler: Job 1 finished: collect at DirectKafkaStreamSuite.scala:151, took 0.580098 s
21/02/21 22:51:49.688 JobScheduler INFO JobScheduler: Finished job streaming job 1613919108000 ms.1 from job set of time 1613919108000 ms
21/02/21 22:51:49.688 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic3 0 16 16
21/02/21 22:51:49.688 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic1 0 16 16
21/02/21 22:51:49.689 streaming-job-executor-0 INFO DirectKafkaStreamSuite: basic2 0 16 16
21/02/21 22:51:49.691 JobScheduler INFO JobScheduler: Total delay: 1.688 s for time 1613919108000 ms (execution: 1.576 s)
21/02/21 22:51:49.692 JobScheduler INFO JobScheduler: Starting job streaming job 1613919109000 ms.0 from job set of time 1613919109000 ms
21/02/21 22:51:49.707 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:49.721 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:49.728 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:49.732 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:49.736 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:133
21/02/21 22:51:49.736 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time 1613919109000
21/02/21 22:51:49.738 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at DirectKafkaStreamSuite.scala:133) with 3 output partitions
21/02/21 22:51:49.738 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at DirectKafkaStreamSuite.scala:133)
21/02/21 22:51:49.738 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:49.739 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:49.741 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:133), which has no missing parents
21/02/21 22:51:49.752 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.6 KiB, free 2.1 GiB)
21/02/21 22:51:49.760 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2.1 GiB)
21/02/21 22:51:49.763 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-JPLSL4N:58841 (size: 3.2 KiB, free: 2.1 GiB)
21/02/21 22:51:49.765 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:49.767 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:133) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:49.768 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks
21/02/21 22:51:49.771 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.772 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 7, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.773 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 8, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.775 Executor task launch worker for task 7 INFO Executor: Running task 1.0 in stage 2.0 (TID 7)
21/02/21 22:51:49.775 Executor task launch worker for task 6 INFO Executor: Running task 0.0 in stage 2.0 (TID 6)
21/02/21 22:51:49.776 Executor task launch worker for task 8 INFO Executor: Running task 2.0 in stage 2.0 (TID 8)
21/02/21 22:51:49.789 Executor task launch worker for task 6 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic3 0
21/02/21 22:51:49.789 Executor task launch worker for task 8 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic2 0
21/02/21 22:51:49.790 Executor task launch worker for task 7 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic1 0
21/02/21 22:51:49.793 Executor task launch worker for task 6 INFO Executor: Finished task 0.0 in stage 2.0 (TID 6). 928 bytes result sent to driver
21/02/21 22:51:49.793 Executor task launch worker for task 7 INFO Executor: Finished task 1.0 in stage 2.0 (TID 7). 928 bytes result sent to driver
21/02/21 22:51:49.793 Executor task launch worker for task 8 INFO Executor: Finished task 2.0 in stage 2.0 (TID 8). 928 bytes result sent to driver
21/02/21 22:51:49.795 task-result-getter-3 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 7) in 24 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:49.795 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 25 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:49.796 task-result-getter-0 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 8) in 24 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:49.796 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/02/21 22:51:49.798 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at DirectKafkaStreamSuite.scala:133) finished in 0.052 s
21/02/21 22:51:49.798 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:49.798 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/02/21 22:51:49.799 streaming-job-executor-0 INFO DAGScheduler: Job 2 finished: collect at DirectKafkaStreamSuite.scala:133, took 0.063108 s
21/02/21 22:51:49.800 JobScheduler INFO JobScheduler: Finished job streaming job 1613919109000 ms.0 from job set of time 1613919109000 ms
21/02/21 22:51:49.801 JobScheduler INFO JobScheduler: Starting job streaming job 1613919109000 ms.1 from job set of time 1613919109000 ms
21/02/21 22:51:49.804 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Revoke previously assigned partitions basic3-0, basic2-0, basic1-0
21/02/21 22:51:49.805 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-2118558971-1613919103780-504, groupId=test-consumer-2118558971-1613919103780] Member consumer-test-consumer-2118558971-1613919103780-504-94498c8b-7151-4374-9553-eddc2099eea4 sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:49.811 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-2118558971-1613919103780-504-94498c8b-7151-4374-9553-eddc2099eea4] in group test-consumer-2118558971-1613919103780 has left, removing it from the group
21/02/21 22:51:49.812 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-2118558971-1613919103780 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-2118558971-1613919103780-504-94498c8b-7151-4374-9553-eddc2099eea4 on LeaveGroup)
21/02/21 22:51:49.813 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-2118558971-1613919103780 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:49.818 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:151
21/02/21 22:51:49.819 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at DirectKafkaStreamSuite.scala:151) with 3 output partitions
21/02/21 22:51:49.819 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at DirectKafkaStreamSuite.scala:151)
21/02/21 22:51:49.819 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:49.820 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:49.820 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at map at DirectKafkaStreamSuite.scala:151), which has no missing parents
21/02/21 22:51:49.824 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:49.825 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.0 KiB, free 2.1 GiB)
21/02/21 22:51:49.828 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 2.1 GiB)
21/02/21 22:51:49.829 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-JPLSL4N:58841 (size: 3.0 KiB, free: 2.1 GiB)
21/02/21 22:51:49.830 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:49.831 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at DirectKafkaStreamSuite.scala:151) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:49.831 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks
21/02/21 22:51:49.832 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.832 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.833 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7226 bytes)
21/02/21 22:51:49.833 Executor task launch worker for task 10 INFO Executor: Running task 1.0 in stage 3.0 (TID 10)
21/02/21 22:51:49.833 Executor task launch worker for task 11 INFO Executor: Running task 2.0 in stage 3.0 (TID 11)
21/02/21 22:51:49.833 Executor task launch worker for task 9 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)
21/02/21 22:51:49.838 Executor task launch worker for task 9 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic3 0
21/02/21 22:51:49.838 Executor task launch worker for task 10 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic1 0
21/02/21 22:51:49.838 Executor task launch worker for task 11 INFO KafkaRDD: Beginning offset 16 is the same as ending offset skipping basic2 0
21/02/21 22:51:49.839 Executor task launch worker for task 10 INFO Executor: Finished task 1.0 in stage 3.0 (TID 10). 790 bytes result sent to driver
21/02/21 22:51:49.839 Executor task launch worker for task 11 INFO Executor: Finished task 2.0 in stage 3.0 (TID 11). 790 bytes result sent to driver
21/02/21 22:51:49.840 Executor task launch worker for task 9 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 790 bytes result sent to driver
21/02/21 22:51:49.841 task-result-getter-3 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 9 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:49.841 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 9 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:49.841 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 9 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:49.841 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/21 22:51:49.842 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at DirectKafkaStreamSuite.scala:151) finished in 0.018 s
21/02/21 22:51:49.842 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:49.842 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/21 22:51:49.843 streaming-job-executor-0 INFO DAGScheduler: Job 3 finished: collect at DirectKafkaStreamSuite.scala:151, took 0.023638 s
21/02/21 22:51:49.843 JobScheduler INFO JobScheduler: Finished job streaming job 1613919109000 ms.1 from job set of time 1613919109000 ms
21/02/21 22:51:49.844 JobScheduler INFO JobScheduler: Total delay: 0.843 s for time 1613919109000 ms (execution: 0.151 s)
21/02/21 22:51:49.848 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:49.851 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:49.873 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:49.897 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:49.897 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:49.906 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:49.910 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:49.916 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:49.916 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'basic stream receiving with multiple topics and smallest starting offset' =====

21/02/21 22:51:49.918 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:49.918 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:49.919 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'pattern based subscription' =====

21/02/21 22:51:49.923 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic pat1 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:49.923 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x19 zxid:0x36 txntype:-1 reqpath:n/a Error Path:/config/topics/pat1 Error:KeeperErrorCode = NoNode for /config/topics/pat1
21/02/21 22:51:49.933 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(pat1)], deleted topics: [Set()], new partition replica assignment [Map(pat1-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:49.933 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for pat1-0
21/02/21 22:51:49.943 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pat1-0)
21/02/21 22:51:49.949 data-plane-kafka-request-handler-2 INFO Log: [Log partition=pat1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:49.951 data-plane-kafka-request-handler-2 INFO Log: [Log partition=pat1-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
21/02/21 22:51:49.952 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition pat1-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\pat1-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:49.953 data-plane-kafka-request-handler-2 INFO Partition: [Partition pat1-0 broker=0] No checkpointed highwatermark is found for partition pat1-0
21/02/21 22:51:49.953 data-plane-kafka-request-handler-2 INFO Partition: [Partition pat1-0 broker=0] Log loaded for partition pat1-0 with initial high watermark 0
21/02/21 22:51:49.953 data-plane-kafka-request-handler-2 INFO Partition: [Partition pat1-0 broker=0] pat1-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:50.035 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:50.040 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:50.040 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:50.040 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919110040
21/02/21 22:51:50.048 kafka-producer-network-thread | producer-5 INFO Metadata: [Producer clientId=producer-5] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:50.050 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:50.072 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic pat2 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:50.073 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x20 zxid:0x3c txntype:-1 reqpath:n/a Error Path:/config/topics/pat2 Error:KeeperErrorCode = NoNode for /config/topics/pat2
21/02/21 22:51:50.086 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(pat2)], deleted topics: [Set()], new partition replica assignment [Map(pat2-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:50.086 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for pat2-0
21/02/21 22:51:50.104 data-plane-kafka-request-handler-1 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pat2-0)
21/02/21 22:51:50.117 data-plane-kafka-request-handler-1 INFO Log: [Log partition=pat2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:50.125 data-plane-kafka-request-handler-1 INFO Log: [Log partition=pat2-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms
21/02/21 22:51:50.127 data-plane-kafka-request-handler-1 INFO LogManager: Created log for partition pat2-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\pat2-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:50.128 data-plane-kafka-request-handler-1 INFO Partition: [Partition pat2-0 broker=0] No checkpointed highwatermark is found for partition pat2-0
21/02/21 22:51:50.128 data-plane-kafka-request-handler-1 INFO Partition: [Partition pat2-0 broker=0] Log loaded for partition pat2-0 with initial high watermark 0
21/02/21 22:51:50.128 data-plane-kafka-request-handler-1 INFO Partition: [Partition pat2-0 broker=0] pat2-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:50.185 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:50.190 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:50.190 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:50.190 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919110190
21/02/21 22:51:50.196 kafka-producer-network-thread | producer-6 INFO Metadata: [Producer clientId=producer-6] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:50.197 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:50.217 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic pat3 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:50.218 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x27 zxid:0x42 txntype:-1 reqpath:n/a Error Path:/config/topics/pat3 Error:KeeperErrorCode = NoNode for /config/topics/pat3
21/02/21 22:51:50.231 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(pat3)], deleted topics: [Set()], new partition replica assignment [Map(pat3-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:50.231 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for pat3-0
21/02/21 22:51:50.247 data-plane-kafka-request-handler-7 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pat3-0)
21/02/21 22:51:50.260 data-plane-kafka-request-handler-7 INFO Log: [Log partition=pat3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:50.266 data-plane-kafka-request-handler-7 INFO Log: [Log partition=pat3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms
21/02/21 22:51:50.266 data-plane-kafka-request-handler-7 INFO LogManager: Created log for partition pat3-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\pat3-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:50.268 data-plane-kafka-request-handler-7 INFO Partition: [Partition pat3-0 broker=0] No checkpointed highwatermark is found for partition pat3-0
21/02/21 22:51:50.268 data-plane-kafka-request-handler-7 INFO Partition: [Partition pat3-0 broker=0] Log loaded for partition pat3-0 with initial high watermark 0
21/02/21 22:51:50.268 data-plane-kafka-request-handler-7 INFO Partition: [Partition pat3-0 broker=0] pat3-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:50.331 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:50.335 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:50.335 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:50.335 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919110335
21/02/21 22:51:50.343 kafka-producer-network-thread | producer-7 INFO Metadata: [Producer clientId=producer-7] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:50.345 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:50.366 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic advanced3 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:50.367 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x2e zxid:0x48 txntype:-1 reqpath:n/a Error Path:/config/topics/advanced3 Error:KeeperErrorCode = NoNode for /config/topics/advanced3
21/02/21 22:51:50.383 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(advanced3)], deleted topics: [Set()], new partition replica assignment [Map(advanced3-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:50.383 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for advanced3-0
21/02/21 22:51:50.398 data-plane-kafka-request-handler-3 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(advanced3-0)
21/02/21 22:51:50.408 data-plane-kafka-request-handler-3 INFO Log: [Log partition=advanced3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:50.412 data-plane-kafka-request-handler-3 INFO Log: [Log partition=advanced3-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms
21/02/21 22:51:50.414 data-plane-kafka-request-handler-3 INFO LogManager: Created log for partition advanced3-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\advanced3-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:50.414 data-plane-kafka-request-handler-3 INFO Partition: [Partition advanced3-0 broker=0] No checkpointed highwatermark is found for partition advanced3-0
21/02/21 22:51:50.415 data-plane-kafka-request-handler-3 INFO Partition: [Partition advanced3-0 broker=0] Log loaded for partition advanced3-0 with initial high watermark 0
21/02/21 22:51:50.415 data-plane-kafka-request-handler-3 INFO Partition: [Partition advanced3-0 broker=0] advanced3-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:50.481 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:50.483 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:50.484 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:50.484 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919110483
21/02/21 22:51:50.487 kafka-producer-network-thread | producer-8 INFO Metadata: [Producer clientId=producer-8] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:50.488 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:50.498 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:50.500 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:50.564 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 58893.
21/02/21 22:51:50.567 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:50.568 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:50.569 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:50.569 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:50.569 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:50.573 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-036fd254-9163-4609-a336-b7daffc704ff
21/02/21 22:51:50.573 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:50.575 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:50.634 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:50.645 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58912.
21/02/21 22:51:50.645 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:58912
21/02/21 22:51:50.645 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:50.645 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58912, None)
21/02/21 22:51:50.646 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:58912 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 58912, None)
21/02/21 22:51:50.646 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58912, None)
21/02/21 22:51:50.647 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 58912, None)
21/02/21 22:51:50.658 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:50.658 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:50.658 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-998335880-1613919110498
21/02/21 22:51:50.659 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:50.677 streaming-start INFO DirectKafkaInputDStream: Slide time = 1000 ms
21/02/21 22:51:50.677 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:50.677 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:50.677 streaming-start INFO DirectKafkaInputDStream: Remember interval = 1000 ms
21/02/21 22:51:50.677 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7c595763
21/02/21 22:51:50.677 streaming-start INFO TransformedDStream: Slide time = 1000 ms
21/02/21 22:51:50.677 streaming-start INFO TransformedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:50.677 streaming-start INFO TransformedDStream: Checkpoint interval = null
21/02/21 22:51:50.677 streaming-start INFO TransformedDStream: Remember interval = 1000 ms
21/02/21 22:51:50.677 streaming-start INFO TransformedDStream: Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@729ea698
21/02/21 22:51:50.677 streaming-start INFO ForEachDStream: Slide time = 1000 ms
21/02/21 22:51:50.677 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:50.677 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Remember interval = 1000 ms
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@76a13c13
21/02/21 22:51:50.678 streaming-start INFO DirectKafkaInputDStream: Slide time = 1000 ms
21/02/21 22:51:50.678 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:50.678 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:50.678 streaming-start INFO DirectKafkaInputDStream: Remember interval = 1000 ms
21/02/21 22:51:50.678 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7c595763
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Slide time = 1000 ms
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Remember interval = 1000 ms
21/02/21 22:51:50.678 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@33a3aca3
21/02/21 22:51:50.679 scala-execution-context-global-260 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-998335880-1613919110498
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:50.681 scala-execution-context-global-260 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:50.681 scala-execution-context-global-260 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:50.682 scala-execution-context-global-260 INFO AppInfoParser: Kafka startTimeMs: 1613919110681
21/02/21 22:51:50.682 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Subscribed to pattern: 'pat\d'
21/02/21 22:51:50.686 scala-execution-context-global-260 INFO Metadata: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:50.686 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:50.687 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] (Re-)joining group
21/02/21 22:51:50.692 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] (Re-)joining group
21/02/21 22:51:50.693 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-998335880-1613919110498 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-998335880-1613919110498-508-d51233d5-ba16-49dc-aa4d-3ba757d3626a with group instanceid None)
21/02/21 22:51:50.705 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-998335880-1613919110498 generation 1 (__consumer_offsets-0)
21/02/21 22:51:50.707 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Finished assignment for group at generation 1: {consumer-test-consumer-998335880-1613919110498-508-d51233d5-ba16-49dc-aa4d-3ba757d3626a=Assignment(partitions=[pat3-0, pat1-0, pat2-0])}
21/02/21 22:51:50.707 data-plane-kafka-request-handler-3 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-998335880-1613919110498 for generation 1
21/02/21 22:51:50.712 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Successfully joined group with generation 1
21/02/21 22:51:50.714 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Adding newly assigned partitions: pat2-0, pat3-0, pat1-0
21/02/21 22:51:50.716 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Found no committed offset for partition pat2-0
21/02/21 22:51:50.716 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Found no committed offset for partition pat3-0
21/02/21 22:51:50.716 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Found no committed offset for partition pat1-0
21/02/21 22:51:50.717 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Seeking to offset 3 for partition pat2-0
21/02/21 22:51:50.717 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Seeking to offset 4 for partition pat3-0
21/02/21 22:51:50.719 scala-execution-context-global-260 INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Resetting offset for partition pat1-0 to offset 0.
21/02/21 22:51:50.719 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919111000
21/02/21 22:51:50.719 streaming-start INFO JobGenerator: Started JobGenerator at 1613919111000 ms
21/02/21 22:51:50.719 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:50.719 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:51.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Seeking to LATEST offset of partition pat3-0
21/02/21 22:51:51.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Seeking to LATEST offset of partition pat1-0
21/02/21 22:51:51.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Seeking to LATEST offset of partition pat2-0
21/02/21 22:51:51.004 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Resetting offset for partition pat2-0 to offset 16.
21/02/21 22:51:51.004 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Resetting offset for partition pat3-0 to offset 16.
21/02/21 22:51:51.004 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Resetting offset for partition pat1-0 to offset 16.
21/02/21 22:51:51.008 JobGenerator INFO JobScheduler: Added jobs for time 1613919111000 ms
21/02/21 22:51:51.009 JobScheduler INFO JobScheduler: Starting job streaming job 1613919111000 ms.0 from job set of time 1613919111000 ms
21/02/21 22:51:51.010 streaming-job-executor-0 INFO DirectKafkaStreamSuite: pat3 0 4 16
21/02/21 22:51:51.010 streaming-job-executor-0 INFO DirectKafkaStreamSuite: pat2 0 3 16
21/02/21 22:51:51.010 streaming-job-executor-0 INFO DirectKafkaStreamSuite: pat1 0 0 16
21/02/21 22:51:51.029 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:199
21/02/21 22:51:51.030 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at DirectKafkaStreamSuite.scala:199) with 3 output partitions
21/02/21 22:51:51.030 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:199)
21/02/21 22:51:51.030 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:51.030 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:51.032 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:199), which has no missing parents
21/02/21 22:51:51.041 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.6 KiB, free 2.1 GiB)
21/02/21 22:51:51.044 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 2.1 GiB)
21/02/21 22:51:51.048 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:58912 (size: 3.2 KiB, free: 2.1 GiB)
21/02/21 22:51:51.048 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:51.049 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapPartitionsWithIndex at DirectKafkaStreamSuite.scala:199) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:51.049 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks
21/02/21 22:51:51.051 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.051 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.051 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.053 Executor task launch worker for task 1 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
21/02/21 22:51:51.053 Executor task launch worker for task 2 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
21/02/21 22:51:51.053 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:51.060 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic pat2, partition 0 offsets 3 -> 16
21/02/21 22:51:51.060 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic pat3, partition 0 offsets 4 -> 16
21/02/21 22:51:51.060 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic pat1, partition 0 offsets 0 -> 16
21/02/21 22:51:51.061 Executor task launch worker for task 1 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-998335880-1613919110498
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:51.064 Executor task launch worker for task 1 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:51.064 Executor task launch worker for task 1 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:51.064 Executor task launch worker for task 1 INFO AppInfoParser: Kafka startTimeMs: 1613919111064
21/02/21 22:51:51.064 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-509, groupId=spark-executor-test-consumer-998335880-1613919110498] Subscribed to partition(s): pat2-0
21/02/21 22:51:51.064 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat2-0 3
21/02/21 22:51:51.065 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-509, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 3 for partition pat2-0
21/02/21 22:51:51.065 Executor task launch worker for task 2 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-998335880-1613919110498
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:51.067 Executor task launch worker for task 1 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-509, groupId=spark-executor-test-consumer-998335880-1613919110498] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:51.068 Executor task launch worker for task 2 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:51.068 Executor task launch worker for task 2 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:51.068 Executor task launch worker for task 2 INFO AppInfoParser: Kafka startTimeMs: 1613919111068
21/02/21 22:51:51.068 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-510, groupId=spark-executor-test-consumer-998335880-1613919110498] Subscribed to partition(s): pat1-0
21/02/21 22:51:51.069 Executor task launch worker for task 2 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat1-0 0
21/02/21 22:51:51.069 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-510, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 0 for partition pat1-0
21/02/21 22:51:51.070 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-998335880-1613919110498
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:51.071 Executor task launch worker for task 2 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-510, groupId=spark-executor-test-consumer-998335880-1613919110498] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:51.074 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:51.074 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:51.074 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919111074
21/02/21 22:51:51.074 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-511, groupId=spark-executor-test-consumer-998335880-1613919110498] Subscribed to partition(s): pat3-0
21/02/21 22:51:51.075 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat3-0 4
21/02/21 22:51:51.075 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-511, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 4 for partition pat3-0
21/02/21 22:51:51.076 Executor task launch worker for task 1 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 928 bytes result sent to driver
21/02/21 22:51:51.078 Executor task launch worker for task 2 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 928 bytes result sent to driver
21/02/21 22:51:51.079 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-511, groupId=spark-executor-test-consumer-998335880-1613919110498] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:51.082 task-result-getter-0 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 31 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:51.082 task-result-getter-1 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 31 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:51.086 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 971 bytes result sent to driver
21/02/21 22:51:51.087 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 37 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:51.087 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:51.088 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:199) finished in 0.052 s
21/02/21 22:51:51.088 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:51.089 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:51.089 streaming-job-executor-0 INFO DAGScheduler: Job 0 finished: collect at DirectKafkaStreamSuite.scala:199, took 0.059290 s
21/02/21 22:51:51.091 JobScheduler INFO JobScheduler: Finished job streaming job 1613919111000 ms.0 from job set of time 1613919111000 ms
21/02/21 22:51:51.091 JobScheduler INFO JobScheduler: Starting job streaming job 1613919111000 ms.1 from job set of time 1613919111000 ms
21/02/21 22:51:51.125 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:217
21/02/21 22:51:51.126 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at DirectKafkaStreamSuite.scala:217) with 3 output partitions
21/02/21 22:51:51.126 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:217)
21/02/21 22:51:51.126 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:51.127 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:51.128 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at map at DirectKafkaStreamSuite.scala:217), which has no missing parents
21/02/21 22:51:51.131 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.0 KiB, free 2.1 GiB)
21/02/21 22:51:51.133 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 2.1 GiB)
21/02/21 22:51:51.134 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:58912 (size: 3.0 KiB, free: 2.1 GiB)
21/02/21 22:51:51.134 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:51.135 dag-scheduler-event-loop INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at DirectKafkaStreamSuite.scala:217) (first 15 tasks are for partitions Vector(0, 1, 2))
21/02/21 22:51:51.136 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks
21/02/21 22:51:51.137 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.138 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, DESKTOP-JPLSL4N, executor driver, partition 1, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.138 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, DESKTOP-JPLSL4N, executor driver, partition 2, PROCESS_LOCAL, 7224 bytes)
21/02/21 22:51:51.139 Executor task launch worker for task 3 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)
21/02/21 22:51:51.139 Executor task launch worker for task 5 INFO Executor: Running task 2.0 in stage 1.0 (TID 5)
21/02/21 22:51:51.139 Executor task launch worker for task 4 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)
21/02/21 22:51:51.143 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic pat3, partition 0 offsets 4 -> 16
21/02/21 22:51:51.143 Executor task launch worker for task 4 INFO KafkaRDD: Computing topic pat2, partition 0 offsets 3 -> 16
21/02/21 22:51:51.143 Executor task launch worker for task 3 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat3-0 4
21/02/21 22:51:51.143 Executor task launch worker for task 3 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-511, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 4 for partition pat3-0
21/02/21 22:51:51.143 Executor task launch worker for task 4 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat2-0 3
21/02/21 22:51:51.144 Executor task launch worker for task 4 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-509, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 3 for partition pat2-0
21/02/21 22:51:51.144 Executor task launch worker for task 5 INFO KafkaRDD: Computing topic pat1, partition 0 offsets 0 -> 16
21/02/21 22:51:51.144 Executor task launch worker for task 5 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-998335880-1613919110498 pat1-0 0
21/02/21 22:51:51.145 Executor task launch worker for task 5 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-998335880-1613919110498-510, groupId=spark-executor-test-consumer-998335880-1613919110498] Seeking to offset 0 for partition pat1-0
21/02/21 22:51:51.655 Executor task launch worker for task 3 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 938 bytes result sent to driver
21/02/21 22:51:51.655 Executor task launch worker for task 4 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 949 bytes result sent to driver
21/02/21 22:51:51.656 Executor task launch worker for task 5 INFO Executor: Finished task 2.0 in stage 1.0 (TID 5). 985 bytes result sent to driver
21/02/21 22:51:51.657 task-result-getter-1 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 519 ms on DESKTOP-JPLSL4N (executor driver) (1/3)
21/02/21 22:51:51.657 task-result-getter-0 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 519 ms on DESKTOP-JPLSL4N (executor driver) (2/3)
21/02/21 22:51:51.657 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 520 ms on DESKTOP-JPLSL4N (executor driver) (3/3)
21/02/21 22:51:51.657 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:51:51.658 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:217) finished in 0.529 s
21/02/21 22:51:51.658 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:51.658 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:51:51.659 streaming-job-executor-0 INFO DAGScheduler: Job 1 finished: collect at DirectKafkaStreamSuite.scala:217, took 0.533435 s
21/02/21 22:51:51.659 JobScheduler INFO JobScheduler: Finished job streaming job 1613919111000 ms.1 from job set of time 1613919111000 ms
21/02/21 22:51:51.659 JobScheduler INFO JobScheduler: Total delay: 0.659 s for time 1613919111000 ms (execution: 0.650 s)
21/02/21 22:51:51.660 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:51.660 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:51.727 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:51.728 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:51.728 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time 1613919111000
21/02/21 22:51:51.735 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Revoke previously assigned partitions pat2-0, pat3-0, pat1-0
21/02/21 22:51:51.735 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-998335880-1613919110498-508, groupId=test-consumer-998335880-1613919110498] Member consumer-test-consumer-998335880-1613919110498-508-d51233d5-ba16-49dc-aa4d-3ba757d3626a sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:51.736 data-plane-kafka-request-handler-3 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-998335880-1613919110498-508-d51233d5-ba16-49dc-aa4d-3ba757d3626a] in group test-consumer-998335880-1613919110498 has left, removing it from the group
21/02/21 22:51:51.736 data-plane-kafka-request-handler-3 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-998335880-1613919110498 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-998335880-1613919110498-508-d51233d5-ba16-49dc-aa4d-3ba757d3626a on LeaveGroup)
21/02/21 22:51:51.736 data-plane-kafka-request-handler-3 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-998335880-1613919110498 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:51.741 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:51.742 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:51.742 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:51.745 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:51.760 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:51.760 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:51.761 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:51.761 dispatcher-event-loop-3 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:51.767 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:51.767 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'pattern based subscription' =====

21/02/21 22:51:51.768 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:51.768 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:51.768 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'receiving from largest starting offset' =====

21/02/21 22:51:51.771 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic latest with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:51.771 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x35 zxid:0x4e txntype:-1 reqpath:n/a Error Path:/config/topics/latest Error:KeeperErrorCode = NoNode for /config/topics/latest
21/02/21 22:51:51.780 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(latest)], deleted topics: [Set()], new partition replica assignment [Map(latest-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:51.781 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for latest-0
21/02/21 22:51:51.791 data-plane-kafka-request-handler-7 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(latest-0)
21/02/21 22:51:51.798 data-plane-kafka-request-handler-7 INFO Log: [Log partition=latest-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:51.801 data-plane-kafka-request-handler-7 INFO Log: [Log partition=latest-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:51.802 data-plane-kafka-request-handler-7 INFO LogManager: Created log for partition latest-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\latest-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:51.802 data-plane-kafka-request-handler-7 INFO Partition: [Partition latest-0 broker=0] No checkpointed highwatermark is found for partition latest-0
21/02/21 22:51:51.802 data-plane-kafka-request-handler-7 INFO Partition: [Partition latest-0 broker=0] Log loaded for partition latest-0 with initial high watermark 0
21/02/21 22:51:51.802 data-plane-kafka-request-handler-7 INFO Partition: [Partition latest-0 broker=0] latest-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:51.880 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-480685484-1613919111880
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:51.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:51.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:51.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919111882
21/02/21 22:51:51.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Subscribed to partition(s): latest-0
21/02/21 22:51:51.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:51.884 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:51.884 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:51.884 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919111884
21/02/21 22:51:51.895 kafka-producer-network-thread | producer-9 INFO Metadata: [Producer clientId=producer-9] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:51.897 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:51.908 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Seeking to LATEST offset of partition latest-0
21/02/21 22:51:51.911 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:51.914 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Resetting offset for partition latest-0 to offset 10.
21/02/21 22:51:51.914 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Seeking to LATEST offset of partition latest-0
21/02/21 22:51:51.915 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Resetting offset for partition latest-0 to offset 10.
21/02/21 22:51:51.916 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-512, groupId=test-consumer-480685484-1613919111880] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:51.924 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:51.928 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:51.929 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:51.929 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:51.929 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:51.930 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:51.930 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:51.930 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:51.930 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:51.930 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:51.990 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 58957.
21/02/21 22:51:51.993 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:51.994 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:51.994 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:51.994 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:51.994 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:51.998 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-8e157a4d-145b-4660-a79a-3058d80c9f57
21/02/21 22:51:51.998 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:52.000 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:52.042 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:52.051 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58976.
21/02/21 22:51:52.051 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:58976
21/02/21 22:51:52.051 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:52.051 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58976, None)
21/02/21 22:51:52.051 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:58976 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 58976, None)
21/02/21 22:51:52.052 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 58976, None)
21/02/21 22:51:52.052 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 58976, None)
21/02/21 22:51:52.058 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:52.058 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:52.058 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-480685484-1613919111880
21/02/21 22:51:52.058 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:52.059 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-480685484-1613919111880
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:52.060 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.060 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.060 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112060
21/02/21 22:51:52.061 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Subscribed to topic(s): latest
21/02/21 22:51:52.063 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.063 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:52.064 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] (Re-)joining group
21/02/21 22:51:52.067 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] (Re-)joining group
21/02/21 22:51:52.068 data-plane-kafka-request-handler-0 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-480685484-1613919111880 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-480685484-1613919111880-513-5cf5e16f-34a1-4e68-8360-9d61b86b2657 with group instanceid None)
21/02/21 22:51:52.080 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-480685484-1613919111880 generation 1 (__consumer_offsets-0)
21/02/21 22:51:52.082 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Finished assignment for group at generation 1: {consumer-test-consumer-480685484-1613919111880-513-5cf5e16f-34a1-4e68-8360-9d61b86b2657=Assignment(partitions=[latest-0])}
21/02/21 22:51:52.083 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-480685484-1613919111880 for generation 1
21/02/21 22:51:52.088 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Successfully joined group with generation 1
21/02/21 22:51:52.090 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Adding newly assigned partitions: latest-0
21/02/21 22:51:52.092 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Setting offset for partition latest-0 to the committed offset FetchPosition{offset=10, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=127.0.0.1:58778 (id: 0 rack: null), epoch=0}}
21/02/21 22:51:52.114 streaming-start INFO DirectKafkaInputDStream: Slide time = 200 ms
21/02/21 22:51:52.114 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.114 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:52.114 streaming-start INFO DirectKafkaInputDStream: Remember interval = 200 ms
21/02/21 22:51:52.115 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@24e86585
21/02/21 22:51:52.115 streaming-start INFO MappedDStream: Slide time = 200 ms
21/02/21 22:51:52.115 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.115 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:52.115 streaming-start INFO MappedDStream: Remember interval = 200 ms
21/02/21 22:51:52.115 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2a2f8f68
21/02/21 22:51:52.115 streaming-start INFO ForEachDStream: Slide time = 200 ms
21/02/21 22:51:52.115 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.115 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:52.115 streaming-start INFO ForEachDStream: Remember interval = 200 ms
21/02/21 22:51:52.115 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2e100ef5
21/02/21 22:51:52.118 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919112200
21/02/21 22:51:52.118 streaming-start INFO JobGenerator: Started JobGenerator at 1613919112200 ms
21/02/21 22:51:52.118 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:52.118 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:52.119 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:52.121 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.121 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.121 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112121
21/02/21 22:51:52.125 kafka-producer-network-thread | producer-10 INFO Metadata: [Producer clientId=producer-10] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.127 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:52.138 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:52.138 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:52.139 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time -1
21/02/21 22:51:52.146 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Revoke previously assigned partitions latest-0
21/02/21 22:51:52.146 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-480685484-1613919111880-513, groupId=test-consumer-480685484-1613919111880] Member consumer-test-consumer-480685484-1613919111880-513-5cf5e16f-34a1-4e68-8360-9d61b86b2657 sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:52.147 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-480685484-1613919111880-513-5cf5e16f-34a1-4e68-8360-9d61b86b2657] in group test-consumer-480685484-1613919111880 has left, removing it from the group
21/02/21 22:51:52.147 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-480685484-1613919111880 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-480685484-1613919111880-513-5cf5e16f-34a1-4e68-8360-9d61b86b2657 on LeaveGroup)
21/02/21 22:51:52.147 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-480685484-1613919111880 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:52.152 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:52.153 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:52.154 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:52.157 dispatcher-event-loop-2 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:52.162 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:52.162 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:52.163 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:52.163 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:52.169 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:52.169 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'receiving from largest starting offset' =====

21/02/21 22:51:52.169 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:52.169 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:52.170 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'creating stream by offset' =====

21/02/21 22:51:52.173 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic offset with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:52.173 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x3c zxid:0x54 txntype:-1 reqpath:n/a Error Path:/config/topics/offset Error:KeeperErrorCode = NoNode for /config/topics/offset
21/02/21 22:51:52.183 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(offset)], deleted topics: [Set()], new partition replica assignment [Map(offset-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:52.183 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for offset-0
21/02/21 22:51:52.195 data-plane-kafka-request-handler-3 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(offset-0)
21/02/21 22:51:52.204 data-plane-kafka-request-handler-3 INFO Log: [Log partition=offset-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:52.207 data-plane-kafka-request-handler-3 INFO Log: [Log partition=offset-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms
21/02/21 22:51:52.208 data-plane-kafka-request-handler-3 INFO LogManager: Created log for partition offset-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\offset-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:52.208 data-plane-kafka-request-handler-3 INFO Partition: [Partition offset-0 broker=0] No checkpointed highwatermark is found for partition offset-0
21/02/21 22:51:52.209 data-plane-kafka-request-handler-3 INFO Partition: [Partition offset-0 broker=0] Log loaded for partition offset-0 with initial high watermark 0
21/02/21 22:51:52.209 data-plane-kafka-request-handler-3 INFO Partition: [Partition offset-0 broker=0] offset-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:52.282 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-86730456-1613919112282
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:52.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112285
21/02/21 22:51:52.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Subscribed to partition(s): offset-0
21/02/21 22:51:52.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:52.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112288
21/02/21 22:51:52.291 kafka-producer-network-thread | producer-11 INFO Metadata: [Producer clientId=producer-11] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.292 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:52.304 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Seeking to LATEST offset of partition offset-0
21/02/21 22:51:52.306 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.309 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Resetting offset for partition offset-0 to offset 10.
21/02/21 22:51:52.309 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Seeking to LATEST offset of partition offset-0
21/02/21 22:51:52.310 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Resetting offset for partition offset-0 to offset 10.
21/02/21 22:51:52.311 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-514, groupId=test-consumer-86730456-1613919112282] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:52.319 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:52.320 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:52.320 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:52.320 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:52.320 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:52.321 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:52.321 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:52.321 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:52.321 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:52.321 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:52.373 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59013.
21/02/21 22:51:52.376 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:52.377 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:52.377 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:52.377 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:52.377 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:52.380 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-67cde372-48b5-4430-a899-43b57acdfa3f
21/02/21 22:51:52.380 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:52.383 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:52.456 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:52.465 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59033.
21/02/21 22:51:52.465 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59033
21/02/21 22:51:52.466 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:52.466 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59033, None)
21/02/21 22:51:52.466 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59033 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59033, None)
21/02/21 22:51:52.467 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59033, None)
21/02/21 22:51:52.467 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59033, None)
21/02/21 22:51:52.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:52.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:52.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-86730456-1613919112282
21/02/21 22:51:52.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:52.474 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-86730456-1613919112282
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:52.476 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.476 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.476 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112476
21/02/21 22:51:52.476 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-515, groupId=test-consumer-86730456-1613919112282] Subscribed to partition(s): offset-0
21/02/21 22:51:52.478 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-515, groupId=test-consumer-86730456-1613919112282] Seeking to offset 11 for partition offset-0
21/02/21 22:51:52.479 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-515, groupId=test-consumer-86730456-1613919112282] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.502 streaming-start INFO DirectKafkaInputDStream: Slide time = 200 ms
21/02/21 22:51:52.502 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.502 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:52.502 streaming-start INFO DirectKafkaInputDStream: Remember interval = 200 ms
21/02/21 22:51:52.503 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@52e371d0
21/02/21 22:51:52.503 streaming-start INFO MappedDStream: Slide time = 200 ms
21/02/21 22:51:52.503 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.503 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:52.503 streaming-start INFO MappedDStream: Remember interval = 200 ms
21/02/21 22:51:52.503 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5c2485fc
21/02/21 22:51:52.503 streaming-start INFO ForEachDStream: Slide time = 200 ms
21/02/21 22:51:52.503 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.503 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:52.503 streaming-start INFO ForEachDStream: Remember interval = 200 ms
21/02/21 22:51:52.503 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@581b1aa0
21/02/21 22:51:52.507 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919112600
21/02/21 22:51:52.507 streaming-start INFO JobGenerator: Started JobGenerator at 1613919112600 ms
21/02/21 22:51:52.507 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:52.507 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:52.508 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:52.510 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.510 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.510 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112510
21/02/21 22:51:52.514 kafka-producer-network-thread | producer-12 INFO Metadata: [Producer clientId=producer-12] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.514 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:52.525 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:52.525 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:52.526 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time -1
21/02/21 22:51:52.530 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-86730456-1613919112282-515, groupId=test-consumer-86730456-1613919112282] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:52.537 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:52.538 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:52.539 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:52.541 dispatcher-event-loop-2 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:52.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:52.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:52.546 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:52.546 dispatcher-event-loop-2 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:52.551 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:52.551 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'creating stream by offset' =====

21/02/21 22:51:52.552 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:52.552 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:52.555 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'Direct Kafka stream report input information' =====

21/02/21 22:51:52.558 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic report-test with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:52.559 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x43 zxid:0x5a txntype:-1 reqpath:n/a Error Path:/config/topics/report-test Error:KeeperErrorCode = NoNode for /config/topics/report-test
21/02/21 22:51:52.568 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(report-test)], deleted topics: [Set()], new partition replica assignment [Map(report-test-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:52.568 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for report-test-0
21/02/21 22:51:52.579 data-plane-kafka-request-handler-4 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(report-test-0)
21/02/21 22:51:52.586 data-plane-kafka-request-handler-4 INFO Log: [Log partition=report-test-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:52.589 data-plane-kafka-request-handler-4 INFO Log: [Log partition=report-test-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:52.589 data-plane-kafka-request-handler-4 INFO LogManager: Created log for partition report-test-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\report-test-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:52.590 data-plane-kafka-request-handler-4 INFO Partition: [Partition report-test-0 broker=0] No checkpointed highwatermark is found for partition report-test-0
21/02/21 22:51:52.590 data-plane-kafka-request-handler-4 INFO Partition: [Partition report-test-0 broker=0] Log loaded for partition report-test-0 with initial high watermark 0
21/02/21 22:51:52.590 data-plane-kafka-request-handler-4 INFO Partition: [Partition report-test-0 broker=0] report-test-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:52.668 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:52.669 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.670 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.670 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919112669
21/02/21 22:51:52.673 kafka-producer-network-thread | producer-13 INFO Metadata: [Producer clientId=producer-13] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.674 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:52.685 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:52.686 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:52.687 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:52.687 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:52.733 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59065.
21/02/21 22:51:52.736 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:52.737 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:52.737 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:52.737 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:52.738 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:52.740 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-cf3f7bfb-0b8b-4e2c-a186-53158fa7b154
21/02/21 22:51:52.741 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:52.743 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:52.801 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:52.810 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59084.
21/02/21 22:51:52.810 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59084
21/02/21 22:51:52.811 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:52.811 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59084, None)
21/02/21 22:51:52.811 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59084 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59084, None)
21/02/21 22:51:52.812 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59084, None)
21/02/21 22:51:52.812 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59084, None)
21/02/21 22:51:52.817 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:52.817 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:52.817 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-266541751-1613919112684
21/02/21 22:51:52.817 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:52.835 streaming-start INFO DirectKafkaInputDStream: Slide time = 200 ms
21/02/21 22:51:52.835 streaming-start INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.835 streaming-start INFO DirectKafkaInputDStream: Checkpoint interval = null
21/02/21 22:51:52.835 streaming-start INFO DirectKafkaInputDStream: Remember interval = 200 ms
21/02/21 22:51:52.835 streaming-start INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5151877e
21/02/21 22:51:52.835 streaming-start INFO MappedDStream: Slide time = 200 ms
21/02/21 22:51:52.835 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.835 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:52.835 streaming-start INFO MappedDStream: Remember interval = 200 ms
21/02/21 22:51:52.836 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5f967324
21/02/21 22:51:52.836 streaming-start INFO ForEachDStream: Slide time = 200 ms
21/02/21 22:51:52.836 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:52.836 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:52.836 streaming-start INFO ForEachDStream: Remember interval = 200 ms
21/02/21 22:51:52.836 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5ba9ec0e
21/02/21 22:51:52.836 scala-execution-context-global-260 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-266541751-1613919112684
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:52.839 scala-execution-context-global-260 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:52.839 scala-execution-context-global-260 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:52.839 scala-execution-context-global-260 INFO AppInfoParser: Kafka startTimeMs: 1613919112839
21/02/21 22:51:52.839 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Subscribed to topic(s): report-test
21/02/21 22:51:52.841 scala-execution-context-global-260 INFO Metadata: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:52.842 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:52.842 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] (Re-)joining group
21/02/21 22:51:52.845 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] (Re-)joining group
21/02/21 22:51:52.846 data-plane-kafka-request-handler-1 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-266541751-1613919112684 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-266541751-1613919112684-516-8ce4b58a-7ad5-4997-abdd-25a5689bd066 with group instanceid None)
21/02/21 22:51:52.858 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-266541751-1613919112684 generation 1 (__consumer_offsets-0)
21/02/21 22:51:52.860 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Finished assignment for group at generation 1: {consumer-test-consumer-266541751-1613919112684-516-8ce4b58a-7ad5-4997-abdd-25a5689bd066=Assignment(partitions=[report-test-0])}
21/02/21 22:51:52.860 data-plane-kafka-request-handler-5 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-266541751-1613919112684 for generation 1
21/02/21 22:51:52.865 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Successfully joined group with generation 1
21/02/21 22:51:52.868 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Adding newly assigned partitions: report-test-0
21/02/21 22:51:52.869 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Found no committed offset for partition report-test-0
21/02/21 22:51:52.872 scala-execution-context-global-260 INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Resetting offset for partition report-test-0 to offset 0.
21/02/21 22:51:52.873 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919113000
21/02/21 22:51:52.873 streaming-start INFO JobGenerator: Started JobGenerator at 1613919113000 ms
21/02/21 22:51:52.873 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:52.873 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:53.002 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Seeking to LATEST offset of partition report-test-0
21/02/21 22:51:53.005 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Resetting offset for partition report-test-0 to offset 16.
21/02/21 22:51:53.010 JobGenerator INFO JobScheduler: Added jobs for time 1613919113000 ms
21/02/21 22:51:53.011 JobScheduler INFO JobScheduler: Starting job streaming job 1613919113000 ms.0 from job set of time 1613919113000 ms
21/02/21 22:51:53.021 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:512
21/02/21 22:51:53.021 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at DirectKafkaStreamSuite.scala:512) with 1 output partitions
21/02/21 22:51:53.021 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:512)
21/02/21 22:51:53.021 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:53.022 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:53.023 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at apply at OutcomeOf.scala:85), which has no missing parents
21/02/21 22:51:53.033 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.0 KiB, free 2.1 GiB)
21/02/21 22:51:53.035 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 2.1 GiB)
21/02/21 22:51:53.035 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:59084 (size: 3.0 KiB, free: 2.1 GiB)
21/02/21 22:51:53.036 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:53.036 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at apply at OutcomeOf.scala:85) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:53.037 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/02/21 22:51:53.039 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7231 bytes)
21/02/21 22:51:53.042 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:53.052 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic report-test, partition 0 offsets 0 -> 16
21/02/21 22:51:53.052 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-266541751-1613919112684
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:53.055 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.056 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.056 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919113055
21/02/21 22:51:53.056 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-266541751-1613919112684-517, groupId=spark-executor-test-consumer-266541751-1613919112684] Subscribed to partition(s): report-test-0
21/02/21 22:51:53.056 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-266541751-1613919112684 report-test-0 0
21/02/21 22:51:53.056 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-266541751-1613919112684-517, groupId=spark-executor-test-consumer-266541751-1613919112684] Seeking to offset 0 for partition report-test-0
21/02/21 22:51:53.059 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-266541751-1613919112684-517, groupId=spark-executor-test-consumer-266541751-1613919112684] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.063 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1028 bytes result sent to driver
21/02/21 22:51:53.065 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 27 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:53.065 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:53.065 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:512) finished in 0.035 s
21/02/21 22:51:53.065 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:53.066 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:53.066 streaming-job-executor-0 INFO DAGScheduler: Job 0 finished: collect at DirectKafkaStreamSuite.scala:512, took 0.044264 s
21/02/21 22:51:53.066 JobScheduler INFO JobScheduler: Finished job streaming job 1613919113000 ms.0 from job set of time 1613919113000 ms
21/02/21 22:51:53.067 JobScheduler INFO JobScheduler: Total delay: 0.066 s for time 1613919113000 ms (execution: 0.055 s)
21/02/21 22:51:53.067 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:53.067 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:53.080 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:53.081 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:53.081 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time 1613919113000
21/02/21 22:51:53.088 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Revoke previously assigned partitions report-test-0
21/02/21 22:51:53.089 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-266541751-1613919112684-516, groupId=test-consumer-266541751-1613919112684] Member consumer-test-consumer-266541751-1613919112684-516-8ce4b58a-7ad5-4997-abdd-25a5689bd066 sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:53.090 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-266541751-1613919112684-516-8ce4b58a-7ad5-4997-abdd-25a5689bd066] in group test-consumer-266541751-1613919112684 has left, removing it from the group
21/02/21 22:51:53.090 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-266541751-1613919112684 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-266541751-1613919112684-516-8ce4b58a-7ad5-4997-abdd-25a5689bd066 on LeaveGroup)
21/02/21 22:51:53.090 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-266541751-1613919112684 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:53.095 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:53.096 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:53.097 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:53.101 dispatcher-event-loop-2 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:53.112 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:53.112 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:53.112 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:53.112 dispatcher-event-loop-3 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:53.119 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:53.119 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'Direct Kafka stream report input information' =====

21/02/21 22:51:53.119 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:53.119 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:53.119 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with backpressure disabled' =====

21/02/21 22:51:53.120 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:53.122 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:53.173 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59100.
21/02/21 22:51:53.175 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:53.176 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:53.176 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:53.176 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:53.177 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:53.179 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-4830f30a-903a-4b88-af32-3d7da3282055
21/02/21 22:51:53.179 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:53.181 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:53.232 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:53.238 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59107.
21/02/21 22:51:53.238 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59107
21/02/21 22:51:53.238 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:53.238 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59107, None)
21/02/21 22:51:53.238 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59107 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59107, None)
21/02/21 22:51:53.239 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59107, None)
21/02/21 22:51:53.239 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59107, None)
21/02/21 22:51:53.244 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:53.244 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.244 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.244 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer--634438747-1613919113244
21/02/21 22:51:53.244 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:53.245 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.245 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.245 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-spark-executor-test-consumer--634438747-1613919113244
21/02/21 22:51:53.246 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer--634438747-1613919113244
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:53.248 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.248 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.248 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919113248
21/02/21 22:51:53.249 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--634438747-1613919113244-518, groupId=test-consumer--634438747-1613919113244] Subscribed to partition(s): maxMessagesPerPartition-0, maxMessagesPerPartition-1
21/02/21 22:51:53.249 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--634438747-1613919113244-518, groupId=test-consumer--634438747-1613919113244] Seeking to offset 0 for partition maxMessagesPerPartition-0
21/02/21 22:51:53.249 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--634438747-1613919113244-518, groupId=test-consumer--634438747-1613919113244] Seeking to offset 0 for partition maxMessagesPerPartition-1
21/02/21 22:51:53.255 data-plane-kafka-request-handler-0 INFO AdminZkClient: Creating topic maxMessagesPerPartition with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:53.256 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:setData cxid:0x87 zxid:0x60 txntype:-1 reqpath:n/a Error Path:/config/topics/maxMessagesPerPartition Error:KeeperErrorCode = NoNode for /config/topics/maxMessagesPerPartition
21/02/21 22:51:53.264 data-plane-kafka-request-handler-0 INFO KafkaApis: [KafkaApi-0] Auto creation of topic maxMessagesPerPartition with 1 partitions and replication factor 1 is successful
21/02/21 22:51:53.265 ScalaTest-main-running-DirectKafkaStreamSuite WARN NetworkClient: [Consumer clientId=consumer-test-consumer--634438747-1613919113244-518, groupId=test-consumer--634438747-1613919113244] Error while fetching metadata with correlation id 1 : {maxMessagesPerPartition=LEADER_NOT_AVAILABLE}
21/02/21 22:51:53.265 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer--634438747-1613919113244-518, groupId=test-consumer--634438747-1613919113244] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.265 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(maxMessagesPerPartition)], deleted topics: [Set()], new partition replica assignment [Map(maxMessagesPerPartition-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:53.266 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for maxMessagesPerPartition-0
21/02/21 22:51:53.269 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with backpressure disabled' =====

21/02/21 22:51:53.270 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:53.274 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:53.278 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(maxMessagesPerPartition-0)
21/02/21 22:51:53.279 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:53.279 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:53.279 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:53.280 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:53.285 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:53.286 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with no lag' =====

21/02/21 22:51:53.287 data-plane-kafka-request-handler-2 INFO Log: [Log partition=maxMessagesPerPartition-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:53.287 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:53.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:53.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.288 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:53.289 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:53.289 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:53.289 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:53.289 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:53.289 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:53.291 data-plane-kafka-request-handler-2 INFO Log: [Log partition=maxMessagesPerPartition-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms
21/02/21 22:51:53.291 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition maxMessagesPerPartition-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\maxMessagesPerPartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:53.292 data-plane-kafka-request-handler-2 INFO Partition: [Partition maxMessagesPerPartition-0 broker=0] No checkpointed highwatermark is found for partition maxMessagesPerPartition-0
21/02/21 22:51:53.292 data-plane-kafka-request-handler-2 INFO Partition: [Partition maxMessagesPerPartition-0 broker=0] Log loaded for partition maxMessagesPerPartition-0 with initial high watermark 0
21/02/21 22:51:53.292 data-plane-kafka-request-handler-2 INFO Partition: [Partition maxMessagesPerPartition-0 broker=0] maxMessagesPerPartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:53.333 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59117.
21/02/21 22:51:53.335 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:53.336 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:53.336 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:53.336 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:53.337 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:53.340 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-a6574cd5-cf79-4409-a3d4-2180fd7de32a
21/02/21 22:51:53.340 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:53.342 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:53.394 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:53.401 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59124.
21/02/21 22:51:53.401 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59124
21/02/21 22:51:53.401 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:53.401 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59124, None)
21/02/21 22:51:53.401 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59124 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59124, None)
21/02/21 22:51:53.402 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59124, None)
21/02/21 22:51:53.402 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59124, None)
21/02/21 22:51:53.406 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer--1287794402-1613919113407
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.407 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-spark-executor-test-consumer--1287794402-1613919113407
21/02/21 22:51:53.408 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer--1287794402-1613919113407
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919113410
21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1287794402-1613919113407-519, groupId=test-consumer--1287794402-1613919113407] Subscribed to partition(s): maxMessagesPerPartition-0, maxMessagesPerPartition-1
21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1287794402-1613919113407-519, groupId=test-consumer--1287794402-1613919113407] Seeking to offset 0 for partition maxMessagesPerPartition-0
21/02/21 22:51:53.410 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1287794402-1613919113407-519, groupId=test-consumer--1287794402-1613919113407] Seeking to offset 0 for partition maxMessagesPerPartition-1
21/02/21 22:51:53.413 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer--1287794402-1613919113407-519, groupId=test-consumer--1287794402-1613919113407] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.416 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with no lag' =====

21/02/21 22:51:53.416 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:53.418 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:53.422 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:53.422 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:53.423 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:53.423 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:53.428 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:53.429 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition respects max rate' =====

21/02/21 22:51:53.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:53.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:53.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:53.432 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:53.432 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:53.432 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:53.432 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:53.432 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:53.482 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59134.
21/02/21 22:51:53.484 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:53.484 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:53.485 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:53.485 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:53.485 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:53.487 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-58b1a0eb-2023-4045-96c8-0ca41755b89c
21/02/21 22:51:53.487 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:53.489 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:53.530 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:53.536 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59141.
21/02/21 22:51:53.536 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59141
21/02/21 22:51:53.536 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:53.536 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59141, None)
21/02/21 22:51:53.537 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59141 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59141, None)
21/02/21 22:51:53.537 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59141, None)
21/02/21 22:51:53.537 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59141, None)
21/02/21 22:51:53.541 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:53.541 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.541 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.541 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer--1484696717-1613919113541
21/02/21 22:51:53.541 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:53.542 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.542 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.542 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-spark-executor-test-consumer--1484696717-1613919113541
21/02/21 22:51:53.542 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer--1484696717-1613919113541
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919113545
21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1484696717-1613919113541-520, groupId=test-consumer--1484696717-1613919113541] Subscribed to partition(s): maxMessagesPerPartition-0, maxMessagesPerPartition-1
21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1484696717-1613919113541-520, groupId=test-consumer--1484696717-1613919113541] Seeking to offset 0 for partition maxMessagesPerPartition-0
21/02/21 22:51:53.545 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--1484696717-1613919113541-520, groupId=test-consumer--1484696717-1613919113541] Seeking to offset 0 for partition maxMessagesPerPartition-1
21/02/21 22:51:53.547 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer--1484696717-1613919113541-520, groupId=test-consumer--1484696717-1613919113541] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.547 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition respects max rate' =====

21/02/21 22:51:53.548 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:53.550 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:53.555 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:53.555 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:53.556 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:53.556 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:53.561 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:53.562 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'using rate controller' =====

21/02/21 22:51:53.564 ScalaTest-main-running-DirectKafkaStreamSuite INFO AdminZkClient: Creating topic backpressure with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:53.565 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0000 type:setData cxid:0x4a zxid:0x66 txntype:-1 reqpath:n/a Error Path:/config/topics/backpressure Error:KeeperErrorCode = NoNode for /config/topics/backpressure
21/02/21 22:51:53.576 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(backpressure)], deleted topics: [Set()], new partition replica assignment [Map(backpressure-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:53.576 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for backpressure-0
21/02/21 22:51:53.586 data-plane-kafka-request-handler-1 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(backpressure-0)
21/02/21 22:51:53.593 data-plane-kafka-request-handler-1 INFO Log: [Log partition=backpressure-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:53.596 data-plane-kafka-request-handler-1 INFO Log: [Log partition=backpressure-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:53.597 data-plane-kafka-request-handler-1 INFO LogManager: Created log for partition backpressure-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\backpressure-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:53.597 data-plane-kafka-request-handler-1 INFO Partition: [Partition backpressure-0 broker=0] No checkpointed highwatermark is found for partition backpressure-0
21/02/21 22:51:53.597 data-plane-kafka-request-handler-1 INFO Partition: [Partition backpressure-0 broker=0] Log loaded for partition backpressure-0 with initial high watermark 0
21/02/21 22:51:53.597 data-plane-kafka-request-handler-1 INFO Partition: [Partition backpressure-0 broker=0] backpressure-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:53.675 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.675 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.675 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1990075680-1613919113674
21/02/21 22:51:53.675 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:53.679 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:53.682 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.682 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.682 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919113682
21/02/21 22:51:53.686 kafka-producer-network-thread | producer-14 INFO Metadata: [Producer clientId=producer-14] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.710 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:53.752 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:53.753 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.753 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:53.753 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:53.753 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:53.754 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:53.754 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:53.754 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:53.754 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:53.754 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:53.805 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59155.
21/02/21 22:51:53.808 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:53.809 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:53.809 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:53.809 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:53.810 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:53.812 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-e019273d-41a5-4016-a4ac-fd7839769df7
21/02/21 22:51:53.812 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:53.814 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:53.849 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:53.853 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59162.
21/02/21 22:51:53.854 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59162
21/02/21 22:51:53.854 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:53.854 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59162, None)
21/02/21 22:51:53.854 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59162 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59162, None)
21/02/21 22:51:53.855 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59162, None)
21/02/21 22:51:53.855 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59162, None)
21/02/21 22:51:53.858 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:53.860 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:53.860 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:53.860 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1990075680-1613919113674
21/02/21 22:51:53.860 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:53.868 streaming-start INFO DirectKafkaStreamSuite$$anon$2: Slide time = 500 ms
21/02/21 22:51:53.868 streaming-start INFO DirectKafkaStreamSuite$$anon$2: Storage level = Serialized 1x Replicated
21/02/21 22:51:53.868 streaming-start INFO DirectKafkaStreamSuite$$anon$2: Checkpoint interval = null
21/02/21 22:51:53.868 streaming-start INFO DirectKafkaStreamSuite$$anon$2: Remember interval = 500 ms
21/02/21 22:51:53.868 streaming-start INFO DirectKafkaStreamSuite$$anon$2: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite$$anon$2@36dc45ab
21/02/21 22:51:53.868 streaming-start INFO MappedDStream: Slide time = 500 ms
21/02/21 22:51:53.868 streaming-start INFO MappedDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:53.868 streaming-start INFO MappedDStream: Checkpoint interval = null
21/02/21 22:51:53.868 streaming-start INFO MappedDStream: Remember interval = 500 ms
21/02/21 22:51:53.868 streaming-start INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@381bbd00
21/02/21 22:51:53.868 streaming-start INFO ForEachDStream: Slide time = 500 ms
21/02/21 22:51:53.868 streaming-start INFO ForEachDStream: Storage level = Serialized 1x Replicated
21/02/21 22:51:53.868 streaming-start INFO ForEachDStream: Checkpoint interval = null
21/02/21 22:51:53.869 streaming-start INFO ForEachDStream: Remember interval = 500 ms
21/02/21 22:51:53.869 streaming-start INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@3c242edb
21/02/21 22:51:53.869 scala-execution-context-global-260 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-1990075680-1613919113674
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:53.872 scala-execution-context-global-260 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:53.872 scala-execution-context-global-260 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:53.872 scala-execution-context-global-260 INFO AppInfoParser: Kafka startTimeMs: 1613919113872
21/02/21 22:51:53.872 scala-execution-context-global-260 INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Subscribed to topic(s): backpressure
21/02/21 22:51:53.874 scala-execution-context-global-260 INFO Metadata: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:53.875 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:53.875 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] (Re-)joining group
21/02/21 22:51:53.878 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] (Re-)joining group
21/02/21 22:51:53.880 data-plane-kafka-request-handler-7 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-1990075680-1613919113674 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-1990075680-1613919113674-521-331175ac-cf58-4e2b-96c0-42ea1309ec6e with group instanceid None)
21/02/21 22:51:53.891 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-1990075680-1613919113674 generation 1 (__consumer_offsets-0)
21/02/21 22:51:53.892 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Finished assignment for group at generation 1: {consumer-test-consumer-1990075680-1613919113674-521-331175ac-cf58-4e2b-96c0-42ea1309ec6e=Assignment(partitions=[backpressure-0])}
21/02/21 22:51:53.892 data-plane-kafka-request-handler-6 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-1990075680-1613919113674 for generation 1
21/02/21 22:51:53.897 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Successfully joined group with generation 1
21/02/21 22:51:53.899 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Adding newly assigned partitions: backpressure-0
21/02/21 22:51:53.900 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Found no committed offset for partition backpressure-0
21/02/21 22:51:53.903 scala-execution-context-global-260 INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Resetting offset for partition backpressure-0 to offset 0.
21/02/21 22:51:53.903 streaming-start INFO RecurringTimer: Started timer for JobGenerator at time 1613919114000
21/02/21 22:51:53.903 streaming-start INFO JobGenerator: Started JobGenerator at 1613919114000 ms
21/02/21 22:51:53.903 streaming-start INFO JobScheduler: Started JobScheduler
21/02/21 22:51:53.904 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext started
21/02/21 22:51:54.003 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Seeking to LATEST offset of partition backpressure-0
21/02/21 22:51:54.005 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Resetting offset for partition backpressure-0 to offset 5000.
21/02/21 22:51:54.012 JobGenerator INFO JobScheduler: Added jobs for time 1613919114000 ms
21/02/21 22:51:54.013 JobScheduler INFO JobScheduler: Starting job streaming job 1613919114000 ms.0 from job set of time 1613919114000 ms
21/02/21 22:51:54.030 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:605
21/02/21 22:51:54.031 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at DirectKafkaStreamSuite.scala:605) with 1 output partitions
21/02/21 22:51:54.031 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:605)
21/02/21 22:51:54.031 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:54.031 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:54.033 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at DirectKafkaStreamSuite.scala:605), which has no missing parents
21/02/21 22:51:54.038 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.2 KiB, free 2.1 GiB)
21/02/21 22:51:54.042 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:54.043 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:59162 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:54.044 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:54.044 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at DirectKafkaStreamSuite.scala:605) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:54.044 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/02/21 22:51:54.046 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7232 bytes)
21/02/21 22:51:54.049 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:54.055 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic backpressure, partition 0 offsets 0 -> 50
21/02/21 22:51:54.055 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-1990075680-1613919113674
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:54.058 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:54.058 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:54.058 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919114058
21/02/21 22:51:54.058 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1990075680-1613919113674-522, groupId=spark-executor-test-consumer-1990075680-1613919113674] Subscribed to partition(s): backpressure-0
21/02/21 22:51:54.058 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1990075680-1613919113674 backpressure-0 0
21/02/21 22:51:54.059 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1990075680-1613919113674-522, groupId=spark-executor-test-consumer-1990075680-1613919113674] Seeking to offset 0 for partition backpressure-0
21/02/21 22:51:54.062 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-1990075680-1613919113674-522, groupId=spark-executor-test-consumer-1990075680-1613919113674] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:54.069 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1097 bytes result sent to driver
21/02/21 22:51:54.072 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 26 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:54.072 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:54.073 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at DirectKafkaStreamSuite.scala:605) finished in 0.038 s
21/02/21 22:51:54.073 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:54.073 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:54.073 streaming-job-executor-0 INFO DAGScheduler: Job 0 finished: collect at DirectKafkaStreamSuite.scala:605, took 0.042864 s
21/02/21 22:51:54.075 JobScheduler INFO JobScheduler: Finished job streaming job 1613919114000 ms.0 from job set of time 1613919114000 ms
21/02/21 22:51:54.075 JobScheduler INFO JobScheduler: Total delay: 0.074 s for time 1613919114000 ms (execution: 0.061 s)
21/02/21 22:51:54.075 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:54.075 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:54.501 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Seeking to LATEST offset of partition backpressure-0
21/02/21 22:51:54.502 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Resetting offset for partition backpressure-0 to offset 5000.
21/02/21 22:51:54.505 JobGenerator INFO JobScheduler: Added jobs for time 1613919114500 ms
21/02/21 22:51:54.506 JobScheduler INFO JobScheduler: Starting job streaming job 1613919114500 ms.0 from job set of time 1613919114500 ms
21/02/21 22:51:54.523 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:605
21/02/21 22:51:54.524 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at DirectKafkaStreamSuite.scala:605) with 1 output partitions
21/02/21 22:51:54.524 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:605)
21/02/21 22:51:54.524 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:54.524 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:54.525 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at map at DirectKafkaStreamSuite.scala:605), which has no missing parents
21/02/21 22:51:54.528 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.2 KiB, free 2.1 GiB)
21/02/21 22:51:54.531 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:54.532 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:59162 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:54.532 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:54.533 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at map at DirectKafkaStreamSuite.scala:605) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:54.533 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/21 22:51:54.534 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7232 bytes)
21/02/21 22:51:54.534 Executor task launch worker for task 1 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/02/21 22:51:54.536 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic backpressure, partition 0 offsets 50 -> 75
21/02/21 22:51:54.537 Executor task launch worker for task 1 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 901 bytes result sent to driver
21/02/21 22:51:54.538 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:54.538 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:51:54.539 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at DirectKafkaStreamSuite.scala:605) finished in 0.013 s
21/02/21 22:51:54.539 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:54.539 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:51:54.539 streaming-job-executor-0 INFO DAGScheduler: Job 1 finished: collect at DirectKafkaStreamSuite.scala:605, took 0.015239 s
21/02/21 22:51:54.540 JobScheduler INFO JobScheduler: Finished job streaming job 1613919114500 ms.0 from job set of time 1613919114500 ms
21/02/21 22:51:54.540 JobScheduler INFO JobScheduler: Total delay: 0.040 s for time 1613919114500 ms (execution: 0.035 s)
21/02/21 22:51:54.541 JobGenerator INFO MapPartitionsRDD: Removing RDD 1 from persistence list
21/02/21 22:51:54.546 block-manager-slave-async-thread-pool-0 INFO BlockManager: Removing RDD 1
21/02/21 22:51:54.550 JobGenerator INFO KafkaRDD: Removing RDD 0 from persistence list
21/02/21 22:51:54.550 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:54.550 block-manager-slave-async-thread-pool-3 INFO BlockManager: Removing RDD 0
21/02/21 22:51:54.550 JobGenerator INFO InputInfoTracker: remove old batch metadata: 
21/02/21 22:51:55.001 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Seeking to LATEST offset of partition backpressure-0
21/02/21 22:51:55.003 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Resetting offset for partition backpressure-0 to offset 5000.
21/02/21 22:51:55.009 JobGenerator INFO JobScheduler: Added jobs for time 1613919115000 ms
21/02/21 22:51:55.009 JobScheduler INFO JobScheduler: Starting job streaming job 1613919115000 ms.0 from job set of time 1613919115000 ms
21/02/21 22:51:55.035 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:605
21/02/21 22:51:55.036 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at DirectKafkaStreamSuite.scala:605) with 1 output partitions
21/02/21 22:51:55.036 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at DirectKafkaStreamSuite.scala:605)
21/02/21 22:51:55.036 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:55.037 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:55.042 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at map at DirectKafkaStreamSuite.scala:605), which has no missing parents
21/02/21 22:51:55.047 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KiB, free 2.1 GiB)
21/02/21 22:51:55.053 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:55.056 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-JPLSL4N:59162 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:55.057 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:55.058 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at map at DirectKafkaStreamSuite.scala:605) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:55.058 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/02/21 22:51:55.060 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7232 bytes)
21/02/21 22:51:55.061 Executor task launch worker for task 2 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/02/21 22:51:55.065 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic backpressure, partition 0 offsets 75 -> 100
21/02/21 22:51:55.067 Executor task launch worker for task 2 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 901 bytes result sent to driver
21/02/21 22:51:55.070 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:55.070 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/02/21 22:51:55.070 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at DirectKafkaStreamSuite.scala:605) finished in 0.027 s
21/02/21 22:51:55.071 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:55.071 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/02/21 22:51:55.072 streaming-job-executor-0 INFO DAGScheduler: Job 2 finished: collect at DirectKafkaStreamSuite.scala:605, took 0.036955 s
21/02/21 22:51:55.073 JobScheduler INFO JobScheduler: Finished job streaming job 1613919115000 ms.0 from job set of time 1613919115000 ms
21/02/21 22:51:55.074 JobScheduler INFO JobScheduler: Total delay: 0.073 s for time 1613919115000 ms (execution: 0.064 s)
21/02/21 22:51:55.074 JobGenerator INFO MapPartitionsRDD: Removing RDD 4 from persistence list
21/02/21 22:51:55.076 JobGenerator INFO KafkaRDD: Removing RDD 3 from persistence list
21/02/21 22:51:55.076 block-manager-slave-async-thread-pool-6 INFO BlockManager: Removing RDD 4
21/02/21 22:51:55.078 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:55.078 JobGenerator INFO InputInfoTracker: remove old batch metadata: 1613919114000 ms
21/02/21 22:51:55.078 block-manager-slave-async-thread-pool-7 INFO BlockManager: Removing RDD 3
21/02/21 22:51:55.502 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Seeking to LATEST offset of partition backpressure-0
21/02/21 22:51:55.503 JobGenerator INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Resetting offset for partition backpressure-0 to offset 5000.
21/02/21 22:51:55.507 JobGenerator INFO JobScheduler: Added jobs for time 1613919115500 ms
21/02/21 22:51:55.507 JobScheduler INFO JobScheduler: Starting job streaming job 1613919115500 ms.0 from job set of time 1613919115500 ms
21/02/21 22:51:55.530 streaming-job-executor-0 INFO SparkContext: Starting job: collect at DirectKafkaStreamSuite.scala:605
21/02/21 22:51:55.531 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at DirectKafkaStreamSuite.scala:605) with 1 output partitions
21/02/21 22:51:55.532 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at DirectKafkaStreamSuite.scala:605)
21/02/21 22:51:55.532 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:55.532 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:55.533 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at map at DirectKafkaStreamSuite.scala:605), which has no missing parents
21/02/21 22:51:55.536 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.2 KiB, free 2.1 GiB)
21/02/21 22:51:55.545 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 2.1 GiB)
21/02/21 22:51:55.545 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-JPLSL4N:59162 (size: 3.1 KiB, free: 2.1 GiB)
21/02/21 22:51:55.546 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:55.547 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at map at DirectKafkaStreamSuite.scala:605) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:55.547 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/02/21 22:51:55.548 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7232 bytes)
21/02/21 22:51:55.548 Executor task launch worker for task 3 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/02/21 22:51:55.552 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic backpressure, partition 0 offsets 100 -> 110
21/02/21 22:51:55.554 Executor task launch worker for task 3 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 811 bytes result sent to driver
21/02/21 22:51:55.555 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:55.555 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/21 22:51:55.557 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at DirectKafkaStreamSuite.scala:605) finished in 0.021 s
21/02/21 22:51:55.557 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:55.557 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/21 22:51:55.558 streaming-job-executor-0 INFO DAGScheduler: Job 3 finished: collect at DirectKafkaStreamSuite.scala:605, took 0.027066 s
21/02/21 22:51:55.559 JobScheduler INFO JobScheduler: Finished job streaming job 1613919115500 ms.0 from job set of time 1613919115500 ms
21/02/21 22:51:55.559 JobScheduler INFO JobScheduler: Total delay: 0.059 s for time 1613919115500 ms (execution: 0.052 s)
21/02/21 22:51:55.560 JobGenerator INFO MapPartitionsRDD: Removing RDD 7 from persistence list
21/02/21 22:51:55.561 JobGenerator INFO KafkaRDD: Removing RDD 6 from persistence list
21/02/21 22:51:55.561 block-manager-slave-async-thread-pool-12 INFO BlockManager: Removing RDD 7
21/02/21 22:51:55.561 JobGenerator INFO ReceivedBlockTracker: Deleting batches: 
21/02/21 22:51:55.562 JobGenerator INFO InputInfoTracker: remove old batch metadata: 1613919114500 ms
21/02/21 22:51:55.562 block-manager-slave-async-thread-pool-13 INFO BlockManager: Removing RDD 6
21/02/21 22:51:55.567 ScalaTest-main-running-DirectKafkaStreamSuite INFO ReceiverTracker: ReceiverTracker stopped
21/02/21 22:51:55.568 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopping JobGenerator immediately
21/02/21 22:51:55.569 ScalaTest-main-running-DirectKafkaStreamSuite INFO RecurringTimer: Stopped timer for JobGenerator after time 1613919115500
21/02/21 22:51:55.582 scala-execution-context-global-260 INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Revoke previously assigned partitions backpressure-0
21/02/21 22:51:55.582 scala-execution-context-global-260 INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-1990075680-1613919113674-521, groupId=test-consumer-1990075680-1613919113674] Member consumer-test-consumer-1990075680-1613919113674-521-331175ac-cf58-4e2b-96c0-42ea1309ec6e sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:55.586 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-1990075680-1613919113674-521-331175ac-cf58-4e2b-96c0-42ea1309ec6e] in group test-consumer-1990075680-1613919113674 has left, removing it from the group
21/02/21 22:51:55.586 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-1990075680-1613919113674 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-1990075680-1613919113674-521-331175ac-cf58-4e2b-96c0-42ea1309ec6e on LeaveGroup)
21/02/21 22:51:55.587 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-1990075680-1613919113674 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:55.593 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobGenerator: Stopped JobGenerator
21/02/21 22:51:55.594 ScalaTest-main-running-DirectKafkaStreamSuite INFO JobScheduler: Stopped JobScheduler
21/02/21 22:51:55.595 ScalaTest-main-running-DirectKafkaStreamSuite INFO StreamingContext: StreamingContext stopped successfully
21/02/21 22:51:55.599 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:55.626 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:55.626 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:55.627 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:55.628 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:55.632 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:55.633 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'using rate controller' =====

21/02/21 22:51:55.633 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has already been stopped
21/02/21 22:51:55.633 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: SparkContext already stopped.
21/02/21 22:51:55.633 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'backpressure.initialRate should honor maxRatePerPartition' =====

21/02/21 22:51:55.637 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:55.640 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:55.640 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:55.640 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919115640
21/02/21 22:51:55.645 data-plane-kafka-request-handler-1 INFO AdminZkClient: Creating topic cb133495-a87a-4a0a-b09d-07fa41dac337 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:55.646 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:setData cxid:0x99 zxid:0x6c txntype:-1 reqpath:n/a Error Path:/config/topics/cb133495-a87a-4a0a-b09d-07fa41dac337 Error:KeeperErrorCode = NoNode for /config/topics/cb133495-a87a-4a0a-b09d-07fa41dac337
21/02/21 22:51:55.653 data-plane-kafka-request-handler-1 INFO KafkaApis: [KafkaApi-0] Auto creation of topic cb133495-a87a-4a0a-b09d-07fa41dac337 with 1 partitions and replication factor 1 is successful
21/02/21 22:51:55.653 kafka-producer-network-thread | producer-15 WARN NetworkClient: [Producer clientId=producer-15] Error while fetching metadata with correlation id 1 : {cb133495-a87a-4a0a-b09d-07fa41dac337=LEADER_NOT_AVAILABLE}
21/02/21 22:51:55.654 kafka-producer-network-thread | producer-15 INFO Metadata: [Producer clientId=producer-15] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:55.655 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(cb133495-a87a-4a0a-b09d-07fa41dac337)], deleted topics: [Set()], new partition replica assignment [Map(cb133495-a87a-4a0a-b09d-07fa41dac337-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:55.655 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for cb133495-a87a-4a0a-b09d-07fa41dac337-0
21/02/21 22:51:55.665 data-plane-kafka-request-handler-4 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cb133495-a87a-4a0a-b09d-07fa41dac337-0)
21/02/21 22:51:55.672 data-plane-kafka-request-handler-4 INFO Log: [Log partition=cb133495-a87a-4a0a-b09d-07fa41dac337-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:55.676 data-plane-kafka-request-handler-4 INFO Log: [Log partition=cb133495-a87a-4a0a-b09d-07fa41dac337-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms
21/02/21 22:51:55.676 data-plane-kafka-request-handler-4 INFO LogManager: Created log for partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\cb133495-a87a-4a0a-b09d-07fa41dac337-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:55.677 data-plane-kafka-request-handler-4 INFO Partition: [Partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 broker=0] No checkpointed highwatermark is found for partition cb133495-a87a-4a0a-b09d-07fa41dac337-0
21/02/21 22:51:55.677 data-plane-kafka-request-handler-4 INFO Partition: [Partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 broker=0] Log loaded for partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 with initial high watermark 0
21/02/21 22:51:55.678 data-plane-kafka-request-handler-4 INFO Partition: [Partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 broker=0] cb133495-a87a-4a0a-b09d-07fa41dac337-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:55.779 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:55.816 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:55.817 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:55.817 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:55.817 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:55.817 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:55.818 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:55.818 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:55.818 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:55.818 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:55.819 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:55.874 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59182.
21/02/21 22:51:55.876 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:55.877 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:55.877 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:55.878 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:55.878 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:55.880 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-112c2710-5b3d-4740-88b9-b77c9e70c422
21/02/21 22:51:55.881 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:55.882 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:55.933 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:55.939 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59189.
21/02/21 22:51:55.940 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59189
21/02/21 22:51:55.940 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:55.940 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59189, None)
21/02/21 22:51:55.940 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59189 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59189, None)
21/02/21 22:51:55.941 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59189, None)
21/02/21 22:51:55.941 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59189, None)
21/02/21 22:51:55.946 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:55.948 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:55.948 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:55.948 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-6524471-1613919115634
21/02/21 22:51:55.948 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:55.953 ScalaTest-main-running-DirectKafkaStreamSuite INFO PIDRateEstimator: Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
21/02/21 22:51:55.954 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-6524471-1613919115634
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:55.956 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:55.956 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:55.956 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919115956
21/02/21 22:51:55.956 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Subscribed to topic(s): cb133495-a87a-4a0a-b09d-07fa41dac337
21/02/21 22:51:55.958 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:55.959 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:55.959 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] (Re-)joining group
21/02/21 22:51:55.961 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] (Re-)joining group
21/02/21 22:51:55.963 data-plane-kafka-request-handler-0 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-6524471-1613919115634 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer-6524471-1613919115634-523-80f46ea4-b305-42cc-a03b-181258c97699 with group instanceid None)
21/02/21 22:51:55.975 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer-6524471-1613919115634 generation 1 (__consumer_offsets-0)
21/02/21 22:51:55.976 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Finished assignment for group at generation 1: {consumer-test-consumer-6524471-1613919115634-523-80f46ea4-b305-42cc-a03b-181258c97699=Assignment(partitions=[cb133495-a87a-4a0a-b09d-07fa41dac337-0])}
21/02/21 22:51:55.977 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer-6524471-1613919115634 for generation 1
21/02/21 22:51:55.981 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Successfully joined group with generation 1
21/02/21 22:51:55.983 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Adding newly assigned partitions: cb133495-a87a-4a0a-b09d-07fa41dac337-0
21/02/21 22:51:55.984 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Found no committed offset for partition cb133495-a87a-4a0a-b09d-07fa41dac337-0
21/02/21 22:51:55.987 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Resetting offset for partition cb133495-a87a-4a0a-b09d-07fa41dac337-0 to offset 0.
21/02/21 22:51:55.992 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Revoke previously assigned partitions cb133495-a87a-4a0a-b09d-07fa41dac337-0
21/02/21 22:51:55.992 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer-6524471-1613919115634-523, groupId=test-consumer-6524471-1613919115634] Member consumer-test-consumer-6524471-1613919115634-523-80f46ea4-b305-42cc-a03b-181258c97699 sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:55.992 data-plane-kafka-request-handler-7 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer-6524471-1613919115634-523-80f46ea4-b305-42cc-a03b-181258c97699] in group test-consumer-6524471-1613919115634 has left, removing it from the group
21/02/21 22:51:55.994 data-plane-kafka-request-handler-7 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer-6524471-1613919115634 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer-6524471-1613919115634-523-80f46ea4-b305-42cc-a03b-181258c97699 on LeaveGroup)
21/02/21 22:51:55.994 data-plane-kafka-request-handler-7 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer-6524471-1613919115634 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:55.997 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'backpressure.initialRate should honor maxRatePerPartition' =====

21/02/21 22:51:55.997 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:55.999 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:56.004 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:56.004 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:56.004 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:56.004 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:56.009 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:56.009 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'use backpressure.initialRate with backpressure' =====

21/02/21 22:51:56.011 ScalaTest-main-running-DirectKafkaStreamSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:58778]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:56.014 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:56.014 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:56.014 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919116014
21/02/21 22:51:56.020 data-plane-kafka-request-handler-0 INFO AdminZkClient: Creating topic edd9ccd4-7604-4c0a-a80f-415408f534b8 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:56.020 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51327bc0001 type:setData cxid:0xa5 zxid:0x72 txntype:-1 reqpath:n/a Error Path:/config/topics/edd9ccd4-7604-4c0a-a80f-415408f534b8 Error:KeeperErrorCode = NoNode for /config/topics/edd9ccd4-7604-4c0a-a80f-415408f534b8
21/02/21 22:51:56.028 data-plane-kafka-request-handler-0 INFO KafkaApis: [KafkaApi-0] Auto creation of topic edd9ccd4-7604-4c0a-a80f-415408f534b8 with 1 partitions and replication factor 1 is successful
21/02/21 22:51:56.028 kafka-producer-network-thread | producer-16 WARN NetworkClient: [Producer clientId=producer-16] Error while fetching metadata with correlation id 1 : {edd9ccd4-7604-4c0a-a80f-415408f534b8=LEADER_NOT_AVAILABLE}
21/02/21 22:51:56.028 kafka-producer-network-thread | producer-16 INFO Metadata: [Producer clientId=producer-16] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:56.029 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(edd9ccd4-7604-4c0a-a80f-415408f534b8)], deleted topics: [Set()], new partition replica assignment [Map(edd9ccd4-7604-4c0a-a80f-415408f534b8-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:56.029 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for edd9ccd4-7604-4c0a-a80f-415408f534b8-0
21/02/21 22:51:56.039 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(edd9ccd4-7604-4c0a-a80f-415408f534b8-0)
21/02/21 22:51:56.045 data-plane-kafka-request-handler-2 INFO Log: [Log partition=edd9ccd4-7604-4c0a-a80f-415408f534b8-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:56.049 data-plane-kafka-request-handler-2 INFO Log: [Log partition=edd9ccd4-7604-4c0a-a80f-415408f534b8-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms
21/02/21 22:51:56.049 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f281e813-8ec4-44b0-8be3-dd83e6d48f95\edd9ccd4-7604-4c0a-a80f-415408f534b8-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:56.050 data-plane-kafka-request-handler-2 INFO Partition: [Partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 broker=0] No checkpointed highwatermark is found for partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0
21/02/21 22:51:56.050 data-plane-kafka-request-handler-2 INFO Partition: [Partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 broker=0] Log loaded for partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 with initial high watermark 0
21/02/21 22:51:56.050 data-plane-kafka-request-handler-2 INFO Partition: [Partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 broker=0] edd9ccd4-7604-4c0a-a80f-415408f534b8-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:56.145 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaProducer: [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:56.180 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:56.181 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:56.181 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:56.181 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:56.181 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:56.182 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:56.182 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:56.182 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:56.183 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:56.183 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:56.261 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59205.
21/02/21 22:51:56.262 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:56.263 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:56.263 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:56.263 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:56.264 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:56.266 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-80924a6f-bdb2-4c9b-89db-3b79f00efb1c
21/02/21 22:51:56.266 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:56.268 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:56.316 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:56.322 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59212.
21/02/21 22:51:56.322 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59212
21/02/21 22:51:56.322 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:56.322 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59212, None)
21/02/21 22:51:56.322 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59212 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59212, None)
21/02/21 22:51:56.323 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59212, None)
21/02/21 22:51:56.323 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59212, None)
21/02/21 22:51:56.326 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:56.327 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:56.327 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:56.328 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer--2091858011-1613919116009
21/02/21 22:51:56.328 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:56.329 ScalaTest-main-running-DirectKafkaStreamSuite INFO PIDRateEstimator: Created PIDRateEstimator with proportional = 1.0, integral = 0.2, derivative = 0.0, min rate = 100.0
21/02/21 22:51:56.329 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:58778]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer--2091858011-1613919116009
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:56.330 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:56.331 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:56.331 ScalaTest-main-running-DirectKafkaStreamSuite INFO AppInfoParser: Kafka startTimeMs: 1613919116330
21/02/21 22:51:56.331 ScalaTest-main-running-DirectKafkaStreamSuite INFO KafkaConsumer: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Subscribed to topic(s): edd9ccd4-7604-4c0a-a80f-415408f534b8
21/02/21 22:51:56.334 ScalaTest-main-running-DirectKafkaStreamSuite INFO Metadata: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Cluster ID: 3ALoYqiCTcqUzzhZYEXVfQ
21/02/21 22:51:56.334 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Discovered group coordinator 127.0.0.1:58778 (id: 2147483647 rack: null)
21/02/21 22:51:56.334 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] (Re-)joining group
21/02/21 22:51:56.336 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] (Re-)joining group
21/02/21 22:51:56.337 data-plane-kafka-request-handler-1 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer--2091858011-1613919116009 in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-test-consumer--2091858011-1613919116009-524-8586bf3a-926d-4fdb-88a2-773ad9c10dae with group instanceid None)
21/02/21 22:51:56.350 executor-Rebalance INFO GroupCoordinator: [GroupCoordinator 0]: Stabilized group test-consumer--2091858011-1613919116009 generation 1 (__consumer_offsets-0)
21/02/21 22:51:56.350 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Finished assignment for group at generation 1: {consumer-test-consumer--2091858011-1613919116009-524-8586bf3a-926d-4fdb-88a2-773ad9c10dae=Assignment(partitions=[edd9ccd4-7604-4c0a-a80f-415408f534b8-0])}
21/02/21 22:51:56.351 data-plane-kafka-request-handler-4 INFO GroupCoordinator: [GroupCoordinator 0]: Assignment received from leader for group test-consumer--2091858011-1613919116009 for generation 1
21/02/21 22:51:56.355 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Successfully joined group with generation 1
21/02/21 22:51:56.358 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Adding newly assigned partitions: edd9ccd4-7604-4c0a-a80f-415408f534b8-0
21/02/21 22:51:56.359 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Found no committed offset for partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0
21/02/21 22:51:56.360 ScalaTest-main-running-DirectKafkaStreamSuite INFO SubscriptionState: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Resetting offset for partition edd9ccd4-7604-4c0a-a80f-415408f534b8-0 to offset 0.
21/02/21 22:51:56.366 ScalaTest-main-running-DirectKafkaStreamSuite INFO ConsumerCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Revoke previously assigned partitions edd9ccd4-7604-4c0a-a80f-415408f534b8-0
21/02/21 22:51:56.366 ScalaTest-main-running-DirectKafkaStreamSuite INFO AbstractCoordinator: [Consumer clientId=consumer-test-consumer--2091858011-1613919116009-524, groupId=test-consumer--2091858011-1613919116009] Member consumer-test-consumer--2091858011-1613919116009-524-8586bf3a-926d-4fdb-88a2-773ad9c10dae sending LeaveGroup request to coordinator 127.0.0.1:58778 (id: 2147483647 rack: null) due to the consumer is being closed
21/02/21 22:51:56.367 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Member[group.instance.id None, member.id consumer-test-consumer--2091858011-1613919116009-524-8586bf3a-926d-4fdb-88a2-773ad9c10dae] in group test-consumer--2091858011-1613919116009 has left, removing it from the group
21/02/21 22:51:56.367 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Preparing to rebalance group test-consumer--2091858011-1613919116009 in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: removing member consumer-test-consumer--2091858011-1613919116009-524-8586bf3a-926d-4fdb-88a2-773ad9c10dae on LeaveGroup)
21/02/21 22:51:56.367 data-plane-kafka-request-handler-2 INFO GroupCoordinator: [GroupCoordinator 0]: Group test-consumer--2091858011-1613919116009 with generation 2 is now empty (__consumer_offsets-0)
21/02/21 22:51:56.372 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'use backpressure.initialRate with backpressure' =====

21/02/21 22:51:56.372 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:56.375 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:56.379 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:56.379 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:56.379 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:56.380 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:56.385 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:56.385 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with zero offset and rate equal to the specified minimum with default 1' =====

21/02/21 22:51:56.386 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:56.386 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Submitted application: DirectKafkaStreamSuite
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:56.387 ScalaTest-main-running-DirectKafkaStreamSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:56.424 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'sparkDriver' on port 59224.
21/02/21 22:51:56.426 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:56.426 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:56.426 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:56.426 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:56.426 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:56.429 ScalaTest-main-running-DirectKafkaStreamSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-45b099ab-e86a-4851-b568-7a72a3993dae
21/02/21 22:51:56.429 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:56.430 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:56.464 ScalaTest-main-running-DirectKafkaStreamSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:56.468 ScalaTest-main-running-DirectKafkaStreamSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59231.
21/02/21 22:51:56.468 ScalaTest-main-running-DirectKafkaStreamSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59231
21/02/21 22:51:56.468 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:56.468 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59231, None)
21/02/21 22:51:56.469 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59231 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59231, None)
21/02/21 22:51:56.470 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59231, None)
21/02/21 22:51:56.470 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59231, None)
21/02/21 22:51:56.472 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
21/02/21 22:51:56.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:56.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:56.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer--1663219785-1613919116385
21/02/21 22:51:56.474 ScalaTest-main-running-DirectKafkaStreamSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:56.475 ScalaTest-main-running-DirectKafkaStreamSuite INFO DirectKafkaStreamSuite: 

===== FINISHED o.a.s.streaming.kafka010.DirectKafkaStreamSuite: 'maxMessagesPerPartition with zero offset and rate equal to the specified minimum with default 1' =====

21/02/21 22:51:56.475 ScalaTest-main-running-DirectKafkaStreamSuite WARN StreamingContext: StreamingContext has not been started yet
21/02/21 22:51:56.477 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:51:56.480 ScalaTest-main-running-DirectKafkaStreamSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:51:56.480 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManager: BlockManager stopped
21/02/21 22:51:56.480 ScalaTest-main-running-DirectKafkaStreamSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:51:56.480 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:51:56.484 ScalaTest-main-running-DirectKafkaStreamSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:51:56.486 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shutting down
21/02/21 22:51:56.486 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Starting controlled shutdown
21/02/21 22:51:56.490 controller-event-thread INFO KafkaController: [Controller id=0] Shutting down broker 0
21/02/21 22:51:56.491 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Controlled shutdown succeeded
21/02/21 22:51:56.492 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutting down
21/02/21 22:51:56.492 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Stopped
21/02/21 22:51:56.492 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutdown completed
21/02/21 22:51:56.492 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopping socket server request processors
21/02/21 22:51:56.497 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopped socket server request processors
21/02/21 22:51:56.497 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shutting down
21/02/21 22:51:56.498 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shut down completely
21/02/21 22:51:56.500 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutting down
21/02/21 22:51:56.553 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Stopped
21/02/21 22:51:56.553 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutdown completed
21/02/21 22:51:56.553 ScalaTest-main-running-DiscoverySuite INFO KafkaApis: [KafkaApi-0] Shutdown complete.
21/02/21 22:51:56.553 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutting down
21/02/21 22:51:56.750 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Stopped
21/02/21 22:51:56.750 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutdown completed
21/02/21 22:51:56.750 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutting down.
21/02/21 22:51:56.751 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
21/02/21 22:51:56.751 ScalaTest-main-running-DiscoverySuite INFO TransactionStateManager: [Transaction State Manager 0]: Shutdown complete
21/02/21 22:51:56.751 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutting down
21/02/21 22:51:56.753 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Stopped
21/02/21 22:51:56.753 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutdown completed
21/02/21 22:51:56.753 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutdown complete.
21/02/21 22:51:56.753 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutting down.
21/02/21 22:51:56.753 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutting down
21/02/21 22:51:56.909 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutdown completed
21/02/21 22:51:56.909 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Stopped
21/02/21 22:51:56.909 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutting down
21/02/21 22:51:56.951 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Stopped
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutdown completed
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutdown complete.
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shutting down
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutting down
21/02/21 22:51:56.951 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Stopped
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutdown completed
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutting down
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutdown completed
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutting down
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutdown completed
21/02/21 22:51:56.951 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutting down
21/02/21 22:51:57.069 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Stopped
21/02/21 22:51:57.069 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutdown completed
21/02/21 22:51:57.069 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutting down
21/02/21 22:51:57.129 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Stopped
21/02/21 22:51:57.129 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutdown completed
21/02/21 22:51:57.129 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutting down
21/02/21 22:51:57.315 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Stopped
21/02/21 22:51:57.315 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutdown completed
21/02/21 22:51:57.315 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutting down
21/02/21 22:51:57.327 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Stopped
21/02/21 22:51:57.327 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutdown completed
21/02/21 22:51:57.339 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shut down completely
21/02/21 22:51:57.339 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutting down.
21/02/21 22:51:57.340 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:51:57.340 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:51:57.340 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:51:57.340 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:51:57.341 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=offset-0] Writing producer snapshot at offset 20
21/02/21 22:51:57.363 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=basic2-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.377 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=edd9ccd4-7604-4c0a-a80f-415408f534b8-0] Writing producer snapshot at offset 5000
21/02/21 22:51:57.393 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=backpressure-0] Writing producer snapshot at offset 5000
21/02/21 22:51:57.409 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=basic3-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.422 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=latest-0] Writing producer snapshot at offset 20
21/02/21 22:51:57.435 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=pat3-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.449 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=pat2-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.464 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=cb133495-a87a-4a0a-b09d-07fa41dac337-0] Writing producer snapshot at offset 5000
21/02/21 22:51:57.480 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=advanced3-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.493 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=__consumer_offsets-0] Writing producer snapshot at offset 28
21/02/21 22:51:57.510 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=report-test-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.524 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=basic1-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.537 pool-265-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=pat1-0] Writing producer snapshot at offset 16
21/02/21 22:51:57.588 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutdown complete.
21/02/21 22:51:57.588 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutting down
21/02/21 22:51:57.588 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Stopped
21/02/21 22:51:57.588 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutdown completed
21/02/21 22:51:57.589 ScalaTest-main-running-DiscoverySuite INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Stopped partition state machine
21/02/21 22:51:57.589 ScalaTest-main-running-DiscoverySuite INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Stopped replica state machine
21/02/21 22:51:57.589 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutting down
21/02/21 22:51:57.589 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Stopped
21/02/21 22:51:57.589 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutdown completed
21/02/21 22:51:57.591 ScalaTest-main-running-DiscoverySuite INFO KafkaController: [Controller id=0] Resigned
21/02/21 22:51:57.591 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closing.
21/02/21 22:51:57.592 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c51327bc0001
21/02/21 22:51:57.595 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c51327bc0001 closed
21/02/21 22:51:57.595 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c51327bc0001
21/02/21 22:51:57.596 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closed.
21/02/21 22:51:57.596 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutting down
21/02/21 22:51:57.597 NIOServerCxn.Factory:/127.0.0.1:0 WARN NIOServerCnxn: caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x177c51327bc0001, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:230)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)
	at java.lang.Thread.run(Thread.java:748)
21/02/21 22:51:57.602 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:58775 which had sessionid 0x177c51327bc0001
21/02/21 22:51:58.003 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Stopped
21/02/21 22:51:58.003 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutdown completed
21/02/21 22:51:58.003 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutting down
21/02/21 22:51:58.005 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Stopped
21/02/21 22:51:58.005 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutdown completed
21/02/21 22:51:58.005 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutting down
21/02/21 22:51:59.006 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Stopped
21/02/21 22:51:59.006 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutdown completed
21/02/21 22:51:59.006 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutting down socket server
21/02/21 22:51:59.032 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutdown completed
21/02/21 22:51:59.036 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shut down completed
21/02/21 22:51:59.151 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closing.
21/02/21 22:51:59.151 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c51327bc0000
21/02/21 22:51:59.155 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c51327bc0000 closed
21/02/21 22:51:59.156 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c51327bc0000
21/02/21 22:51:59.156 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closed.
21/02/21 22:51:59.156 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:58772 which had sessionid 0x177c51327bc0000
21/02/21 22:51:59.156 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method
21/02/21 22:51:59.157 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: shutting down
21/02/21 22:51:59.157 ScalaTest-main-running-DiscoverySuite INFO SessionTrackerImpl: Shutting down
21/02/21 22:51:59.157 ScalaTest-main-running-DiscoverySuite INFO PrepRequestProcessor: Shutting down
21/02/21 22:51:59.157 ScalaTest-main-running-DiscoverySuite INFO SyncRequestProcessor: Shutting down
21/02/21 22:51:59.157 ProcessThread(sid:0 cport:58769): INFO PrepRequestProcessor: PrepRequestProcessor exited loop!
21/02/21 22:51:59.157 SyncThread:0 INFO SyncRequestProcessor: SyncRequestProcessor exited!
21/02/21 22:51:59.157 ScalaTest-main-running-DiscoverySuite INFO FinalRequestProcessor: shutdown of request processor complete
21/02/21 22:51:59.161 ScalaTest-main-running-DiscoverySuite WARN KafkaTestUtils: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-18745cb3-16c8-4724-bded-25c36f16ea30\version-2\log.1
21/02/21 22:51:59.167 ScalaTest-main-running-DiscoverySuite WARN DirectKafkaStreamSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.streaming.kafka010.DirectKafkaStreamSuite, thread names: ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:58769), scala-execution-context-global-260 =====

21/02/21 22:51:59.169 ScalaTest-main-running-DiscoverySuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO ResourceUtils: ==============================================================
21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO SparkContext: Submitted application: KafkaRDDSuite
21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:51:59.170 ScalaTest-main-running-DiscoverySuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:51:59.171 ScalaTest-main-running-DiscoverySuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:51:59.171 ScalaTest-main-running-DiscoverySuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:51:59.171 ScalaTest-main-running-DiscoverySuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:51:59.223 ScalaTest-main-running-DiscoverySuite INFO Utils: Successfully started service 'sparkDriver' on port 59253.
21/02/21 22:51:59.225 ScalaTest-main-running-DiscoverySuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:51:59.225 ScalaTest-main-running-DiscoverySuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:51:59.225 ScalaTest-main-running-DiscoverySuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:51:59.226 ScalaTest-main-running-DiscoverySuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:51:59.226 ScalaTest-main-running-DiscoverySuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:51:59.228 ScalaTest-main-running-DiscoverySuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-b6e65b08-e20a-417d-8a19-80889a411a96
21/02/21 22:51:59.228 ScalaTest-main-running-DiscoverySuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:51:59.229 ScalaTest-main-running-DiscoverySuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:51:59.265 ScalaTest-main-running-DiscoverySuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:51:59.271 ScalaTest-main-running-DiscoverySuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59272.
21/02/21 22:51:59.271 ScalaTest-main-running-DiscoverySuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59272
21/02/21 22:51:59.271 ScalaTest-main-running-DiscoverySuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:51:59.271 ScalaTest-main-running-DiscoverySuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59272, None)
21/02/21 22:51:59.272 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59272 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59272, None)
21/02/21 22:51:59.272 ScalaTest-main-running-DiscoverySuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59272, None)
21/02/21 22:51:59.272 ScalaTest-main-running-DiscoverySuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59272, None)
21/02/21 22:51:59.281 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: Created server with tickTime 500 minSessionTimeout 1000 maxSessionTimeout 10000 datadir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c22739fa-1d69-491b-96ee-67c7695c94db\version-2 snapdir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-2d71c448-187a-4f08-9814-19cc1c54cdce\version-2
21/02/21 22:51:59.282 ScalaTest-main-running-DiscoverySuite INFO NIOServerCnxnFactory: binding to port /127.0.0.1:0
21/02/21 22:51:59.284 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Initializing a new session to 127.0.0.1:59275.
21/02/21 22:51:59.284 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:59275 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@395a573c
21/02/21 22:51:59.285 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Waiting until connected.
21/02/21 22:51:59.286 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:59275. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:59.286 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:59275, initiating session
21/02/21 22:51:59.286 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:59278
21/02/21 22:51:59.287 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:59278
21/02/21 22:51:59.287 SyncThread:0 INFO FileTxnLog: Creating new log file: log.1
21/02/21 22:51:59.291 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c51367b30000 with negotiated timeout 10000 for client /127.0.0.1:59278
21/02/21 22:51:59.291 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:59275, sessionid = 0x177c51367b30000, negotiated timeout = 10000
21/02/21 22:51:59.292 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Connected.
21/02/21 22:51:59.294 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: starting
21/02/21 22:51:59.294 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Connecting to zookeeper on 127.0.0.1:59275
21/02/21 22:51:59.294 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:59275.
21/02/21 22:51:59.294 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:59275 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@379f9555
21/02/21 22:51:59.296 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Waiting until connected.
21/02/21 22:51:59.296 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Opening socket connection to server 127.0.0.1/127.0.0.1:59275. Will not attempt to authenticate using SASL (unknown error)
21/02/21 22:51:59.296 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Socket connection established to 127.0.0.1/127.0.0.1:59275, initiating session
21/02/21 22:51:59.296 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:59281
21/02/21 22:51:59.296 NIOServerCxn.Factory:/127.0.0.1:0 INFO ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:59281
21/02/21 22:51:59.299 SyncThread:0 INFO ZooKeeperServer: Established session 0x177c51367b30001 with negotiated timeout 6000 for client /127.0.0.1:59281
21/02/21 22:51:59.299 ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) INFO ClientCnxn: Session establishment complete on server 127.0.0.1/127.0.0.1:59275, sessionid = 0x177c51367b30001, negotiated timeout = 6000
21/02/21 22:51:59.299 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Connected.
21/02/21 22:51:59.302 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30001 type:create cxid:0x2 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
21/02/21 22:51:59.312 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30001 type:create cxid:0x6 zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
21/02/21 22:51:59.319 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30001 type:create cxid:0x9 zxid:0xb txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
21/02/21 22:51:59.347 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30001 type:create cxid:0x15 zxid:0x16 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
21/02/21 22:51:59.353 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: Cluster ID = KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:51:59.354 ScalaTest-main-running-DiscoverySuite WARN BrokerMetadataCheckpoint: No meta.properties file under dir C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\meta.properties
21/02/21 22:51:59.357 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:59275
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:59.360 ScalaTest-main-running-DiscoverySuite INFO KafkaConfig: KafkaConfig values: 
	advertised.host.name = 127.0.0.1
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 10
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 127.0.0.1
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83
	log.dirs = null
	log.flush.interval.messages = 1
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 1
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1500
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:59275
	zookeeper.connection.timeout.ms = 60000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

21/02/21 22:51:59.363 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Starting
21/02/21 22:51:59.363 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Starting
21/02/21 22:51:59.363 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Starting
21/02/21 22:51:59.367 ScalaTest-main-running-DiscoverySuite INFO LogManager: Loading logs.
21/02/21 22:51:59.369 ScalaTest-main-running-DiscoverySuite INFO LogManager: Logs loading complete in 2 ms.
21/02/21 22:51:59.370 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log cleanup with a period of 300000 ms.
21/02/21 22:51:59.371 ScalaTest-main-running-DiscoverySuite INFO LogManager: Starting log flusher with a default period of 9223372036854775807 ms.
21/02/21 22:51:59.371 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Starting the log cleaner
21/02/21 22:51:59.383 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:51:59.411 ScalaTest-main-running-DiscoverySuite INFO Acceptor: Awaiting socket connections on 127.0.0.1:59284.
21/02/21 22:51:59.415 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(127.0.0.1,0,ListenerName(PLAINTEXT),PLAINTEXT)
21/02/21 22:51:59.415 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started 1 acceptor threads for data-plane
21/02/21 22:51:59.416 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Starting
21/02/21 22:51:59.417 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Starting
21/02/21 22:51:59.417 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Starting
21/02/21 22:51:59.419 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Starting
21/02/21 22:51:59.421 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Starting
21/02/21 22:51:59.422 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Creating /brokers/ids/0 (is it secure? false)
21/02/21 22:51:59.427 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Stat of the created znode at /brokers/ids/0 is: 25,25,1613919119423,1613919119423,1,0,0,105769803401330689,190,0,25

21/02/21 22:51:59.427 ScalaTest-main-running-DiscoverySuite INFO KafkaZkClient: Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(127.0.0.1,59284,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25
21/02/21 22:51:59.435 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Starting
21/02/21 22:51:59.436 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Starting
21/02/21 22:51:59.437 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Starting
21/02/21 22:51:59.438 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Starting
21/02/21 22:51:59.438 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Starting up.
21/02/21 22:51:59.439 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Startup complete.
21/02/21 22:51:59.439 group-metadata-manager-0 INFO GroupMetadataManager: [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds.
21/02/21 22:51:59.441 controller-event-thread INFO KafkaZkClient: Successfully created /controller_epoch with initial epoch 0
21/02/21 22:51:59.444 controller-event-thread INFO KafkaController: [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
21/02/21 22:51:59.444 controller-event-thread INFO KafkaController: [Controller id=0] Registering handlers
21/02/21 22:51:59.444 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
21/02/21 22:51:59.445 controller-event-thread INFO KafkaController: [Controller id=0] Deleting log dir event notifications
21/02/21 22:51:59.446 controller-event-thread INFO KafkaController: [Controller id=0] Deleting isr change notifications
21/02/21 22:51:59.446 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Starting up.
21/02/21 22:51:59.446 controller-event-thread INFO KafkaController: [Controller id=0] Initializing controller context
21/02/21 22:51:59.447 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Startup complete.
21/02/21 22:51:59.447 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Starting
21/02/21 22:51:59.449 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Starting
21/02/21 22:51:59.449 controller-event-thread INFO KafkaController: [Controller id=0] Initialized broker epochs cache: Map(0 -> 25)
21/02/21 22:51:59.450 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Starting
21/02/21 22:51:59.453 controller-event-thread INFO KafkaController: [Controller id=0] Currently active brokers in the cluster: Set(0)
21/02/21 22:51:59.454 controller-event-thread INFO KafkaController: [Controller id=0] Currently shutting brokers in the cluster: Set()
21/02/21 22:51:59.454 controller-event-thread INFO KafkaController: [Controller id=0] Current list of topics in the cluster: Set()
21/02/21 22:51:59.454 controller-event-thread INFO KafkaController: [Controller id=0] Fetching topic deletions in progress
21/02/21 22:51:59.454 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Started data-plane processors for 1 acceptors
21/02/21 22:51:59.454 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:59.454 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:59.454 ScalaTest-main-running-DiscoverySuite INFO AppInfoParser: Kafka startTimeMs: 1613919119454
21/02/21 22:51:59.454 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Starting
21/02/21 22:51:59.454 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] started
21/02/21 22:51:59.455 ScalaTest-main-running-DiscoverySuite INFO Utils: Successfully started service 'KafkaBroker' on port 59284.
21/02/21 22:51:59.455 controller-event-thread INFO KafkaController: [Controller id=0] List of topics to be deleted: 
21/02/21 22:51:59.455 controller-event-thread INFO KafkaController: [Controller id=0] List of topics ineligible for deletion: 
21/02/21 22:51:59.455 controller-event-thread INFO KafkaController: [Controller id=0] Initializing topic deletion manager
21/02/21 22:51:59.455 controller-event-thread INFO TopicDeletionManager: [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
21/02/21 22:51:59.455 controller-event-thread INFO KafkaController: [Controller id=0] Sending update metadata request
21/02/21 22:51:59.455 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Initializing replica state
21/02/21 22:51:59.455 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering online replica state changes
21/02/21 22:51:59.455 controller-event-thread INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
21/02/21 22:51:59.455 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Initializing partition state
21/02/21 22:51:59.455 controller-event-thread INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Triggering online partition state changes
21/02/21 22:51:59.456 controller-event-thread INFO KafkaController: [Controller id=0] Ready to serve as the new controller with epoch 1
21/02/21 22:51:59.456 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaRDDSuite: 'basic usage' =====

21/02/21 22:51:59.457 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Controller 0 connected to 127.0.0.1:59284 (id: 0 rack: null) for sending state change requests
21/02/21 22:51:59.458 controller-event-thread INFO KafkaController: [Controller id=0] Partitions undergoing preferred replica election: 
21/02/21 22:51:59.458 controller-event-thread INFO KafkaController: [Controller id=0] Partitions that completed preferred replica election: 
21/02/21 22:51:59.458 controller-event-thread INFO KafkaController: [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
21/02/21 22:51:59.458 controller-event-thread INFO KafkaController: [Controller id=0] Resuming preferred replica election for partitions: 
21/02/21 22:51:59.458 controller-event-thread INFO KafkaController: [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
21/02/21 22:51:59.458 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30001 type:multi cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
21/02/21 22:51:59.460 controller-event-thread INFO KafkaController: [Controller id=0] Starting the controller scheduler
21/02/21 22:51:59.460 ScalaTest-main-running-KafkaRDDSuite INFO AdminZkClient: Creating topic topicbasic-672398021-1613919119456 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:51:59.461 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30000 type:setData cxid:0x4 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/topicbasic-672398021-1613919119456 Error:KeeperErrorCode = NoNode for /config/topics/topicbasic-672398021-1613919119456
21/02/21 22:51:59.469 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topicbasic-672398021-1613919119456)], deleted topics: [Set()], new partition replica assignment [Map(topicbasic-672398021-1613919119456-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:51:59.469 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topicbasic-672398021-1613919119456-0
21/02/21 22:51:59.478 data-plane-kafka-request-handler-2 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topicbasic-672398021-1613919119456-0)
21/02/21 22:51:59.485 data-plane-kafka-request-handler-2 INFO Log: [Log partition=topicbasic-672398021-1613919119456-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 0 with message format version 2
21/02/21 22:51:59.487 data-plane-kafka-request-handler-2 INFO Log: [Log partition=topicbasic-672398021-1613919119456-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms
21/02/21 22:51:59.488 data-plane-kafka-request-handler-2 INFO LogManager: Created log for partition topicbasic-672398021-1613919119456-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topicbasic-672398021-1613919119456-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:51:59.488 data-plane-kafka-request-handler-2 INFO Partition: [Partition topicbasic-672398021-1613919119456-0 broker=0] No checkpointed highwatermark is found for partition topicbasic-672398021-1613919119456-0
21/02/21 22:51:59.488 data-plane-kafka-request-handler-2 INFO Partition: [Partition topicbasic-672398021-1613919119456-0 broker=0] Log loaded for partition topicbasic-672398021-1613919119456-0 with initial high watermark 0
21/02/21 22:51:59.488 data-plane-kafka-request-handler-2 INFO Partition: [Partition topicbasic-672398021-1613919119456-0 broker=0] topicbasic-672398021-1613919119456-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:51:59.501 SessionTracker INFO SessionTrackerImpl: SessionTrackerImpl exited loop!
21/02/21 22:51:59.570 ScalaTest-main-running-KafkaRDDSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:59284]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:51:59.572 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:59.572 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:59.572 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka startTimeMs: 1613919119572
21/02/21 22:51:59.579 kafka-producer-network-thread | producer-17 INFO Metadata: [Producer clientId=producer-17] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:51:59.580 ScalaTest-main-running-KafkaRDDSuite INFO KafkaProducer: [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:51:59.594 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:51:59.595 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:51:59.595 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1816117090-1613919119594
21/02/21 22:51:59.595 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:51:59.626 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:139
21/02/21 22:51:59.628 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at KafkaRDDSuite.scala:139) with 1 output partitions
21/02/21 22:51:59.628 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at KafkaRDDSuite.scala:139)
21/02/21 22:51:59.628 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:59.628 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:59.630 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:51:59.638 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:51:59.641 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:51:59.643 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:51:59.645 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:59.646 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:59.646 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/02/21 22:51:59.649 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:51:59.652 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:51:59.676 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:51:59.677 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:59284]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-1816117090-1613919119594
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:51:59.684 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:51:59.684 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:51:59.684 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919119684
21/02/21 22:51:59.685 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Subscribed to partition(s): topicbasic-672398021-1613919119456-0
21/02/21 22:51:59.685 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:51:59.685 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:51:59.690 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:51:59.703 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 865 bytes result sent to driver
21/02/21 22:51:59.706 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:51:59.706 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:51:59.707 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at KafkaRDDSuite.scala:139) finished in 0.074 s
21/02/21 22:51:59.708 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:51:59.708 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:51:59.708 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 0 finished: collect at KafkaRDDSuite.scala:139, took 0.082352 s
21/02/21 22:51:59.733 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: count at KafkaRDDSuite.scala:143
21/02/21 22:51:59.735 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (count at KafkaRDDSuite.scala:143) with 1 output partitions
21/02/21 22:51:59.735 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (count at KafkaRDDSuite.scala:143)
21/02/21 22:51:59.735 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:51:59.735 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:51:59.735 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:51:59.741 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KiB, free 2.1 GiB)
21/02/21 22:51:59.746 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:51:59.748 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:51:59.749 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:51:59.750 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:51:59.750 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/21 22:51:59.752 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:51:59.753 Executor task launch worker for task 1 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/02/21 22:51:59.759 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:51:59.759 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:51:59.759 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:00.270 Executor task launch worker for task 1 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 832 bytes result sent to driver
21/02/21 22:52:00.271 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 519 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:00.272 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:52:00.272 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (count at KafkaRDDSuite.scala:143) finished in 0.534 s
21/02/21 22:52:00.272 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:00.273 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:52:00.273 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 1 finished: count at KafkaRDDSuite.scala:143, took 0.540220 s
21/02/21 22:52:00.279 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: countApprox at KafkaRDDSuite.scala:144
21/02/21 22:52:00.287 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (countApprox at KafkaRDDSuite.scala:144) with 1 output partitions
21/02/21 22:52:00.287 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (countApprox at KafkaRDDSuite.scala:144)
21/02/21 22:52:00.287 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:00.287 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:00.287 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:52:00.289 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Job finished: countApprox at KafkaRDDSuite.scala:144, took 0.0104139 s
21/02/21 22:52:00.290 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.4 KiB, free 2.1 GiB)
21/02/21 22:52:00.294 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 2.1 GiB)
21/02/21 22:52:00.294 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.8 KiB, free: 2.1 GiB)
21/02/21 22:52:00.294 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:00.295 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:00.295 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/02/21 22:52:00.297 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:00.297 Executor task launch worker for task 2 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/02/21 22:52:00.300 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:52:00.300 Executor task launch worker for task 2 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:00.301 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:00.806 Executor task launch worker for task 2 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 789 bytes result sent to driver
21/02/21 22:52:00.807 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 510 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:00.807 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/02/21 22:52:00.808 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (countApprox at KafkaRDDSuite.scala:144) finished in 0.519 s
21/02/21 22:52:00.808 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:00.808 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/02/21 22:52:00.820 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: isEmpty at KafkaRDDSuite.scala:145
21/02/21 22:52:00.821 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (isEmpty at KafkaRDDSuite.scala:145) with 1 output partitions
21/02/21 22:52:00.821 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (isEmpty at KafkaRDDSuite.scala:145)
21/02/21 22:52:00.821 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:00.821 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:00.822 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:52:00.824 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:00.827 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:00.828 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:00.828 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:00.829 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:00.829 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/02/21 22:52:00.830 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:00.830 Executor task launch worker for task 3 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/02/21 22:52:00.836 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:52:00.837 Executor task launch worker for task 3 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:00.837 Executor task launch worker for task 3 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:01.343 Executor task launch worker for task 3 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 800 bytes result sent to driver
21/02/21 22:52:01.344 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 515 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:01.344 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/21 22:52:01.345 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (isEmpty at KafkaRDDSuite.scala:145) finished in 0.522 s
21/02/21 22:52:01.345 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:01.345 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/21 22:52:01.346 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 3 finished: isEmpty at KafkaRDDSuite.scala:145, took 0.525307 s
21/02/21 22:52:01.357 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:146
21/02/21 22:52:01.357 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (take at KafkaRDDSuite.scala:146) with 1 output partitions
21/02/21 22:52:01.358 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (take at KafkaRDDSuite.scala:146)
21/02/21 22:52:01.358 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:01.358 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:01.358 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:52:01.361 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:01.363 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:01.364 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:01.365 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:01.366 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:01.366 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
21/02/21 22:52:01.367 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:01.368 Executor task launch worker for task 4 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
21/02/21 22:52:01.369 Executor task launch worker for task 4 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:52:01.369 Executor task launch worker for task 4 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:01.369 Executor task launch worker for task 4 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:01.877 Executor task launch worker for task 4 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 757 bytes result sent to driver
21/02/21 22:52:01.877 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 510 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:01.877 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/02/21 22:52:01.878 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (take at KafkaRDDSuite.scala:146) finished in 0.518 s
21/02/21 22:52:01.878 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:01.878 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/02/21 22:52:01.879 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 4 finished: take at KafkaRDDSuite.scala:146, took 0.522668 s
21/02/21 22:52:01.897 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:147
21/02/21 22:52:01.898 dag-scheduler-event-loop INFO DAGScheduler: Got job 5 (take at KafkaRDDSuite.scala:147) with 1 output partitions
21/02/21 22:52:01.898 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 5 (take at KafkaRDDSuite.scala:147)
21/02/21 22:52:01.898 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:01.899 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:01.900 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:52:01.904 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:01.907 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:01.909 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:01.910 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:01.911 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:01.911 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/02/21 22:52:01.913 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:01.914 Executor task launch worker for task 5 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
21/02/21 22:52:01.918 Executor task launch worker for task 5 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:52:01.918 Executor task launch worker for task 5 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:01.918 Executor task launch worker for task 5 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:02.425 Executor task launch worker for task 5 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 757 bytes result sent to driver
21/02/21 22:52:02.426 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 514 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:02.426 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/02/21 22:52:02.427 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 5 (take at KafkaRDDSuite.scala:147) finished in 0.524 s
21/02/21 22:52:02.427 dag-scheduler-event-loop INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:02.427 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/02/21 22:52:02.427 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 5 finished: take at KafkaRDDSuite.scala:147, took 0.529728 s
21/02/21 22:52:02.438 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:148
21/02/21 22:52:02.439 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (take at KafkaRDDSuite.scala:148) with 1 output partitions
21/02/21 22:52:02.439 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 6 (take at KafkaRDDSuite.scala:148)
21/02/21 22:52:02.439 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:02.439 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:02.440 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137), which has no missing parents
21/02/21 22:52:02.443 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:02.445 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:02.446 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:02.447 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:02.447 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:137) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:02.447 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
21/02/21 22:52:02.448 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:02.449 Executor task launch worker for task 6 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
21/02/21 22:52:02.451 Executor task launch worker for task 6 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 4
21/02/21 22:52:02.451 Executor task launch worker for task 6 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:02.451 Executor task launch worker for task 6 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:02.957 Executor task launch worker for task 6 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 779 bytes result sent to driver
21/02/21 22:52:02.958 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 510 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:02.959 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/02/21 22:52:02.959 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 6 (take at KafkaRDDSuite.scala:148) finished in 0.518 s
21/02/21 22:52:02.959 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:02.960 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/02/21 22:52:02.960 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 6 finished: take at KafkaRDDSuite.scala:148, took 0.521755 s
21/02/21 22:52:02.960 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:02.960 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:02.960 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1816117090-1613919119594
21/02/21 22:52:02.961 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:02.962 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:02.963 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:02.963 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1816117090-1613919119594
21/02/21 22:52:02.963 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:02.981 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:159
21/02/21 22:52:02.982 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (collect at KafkaRDDSuite.scala:159) with 1 output partitions
21/02/21 22:52:02.982 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (collect at KafkaRDDSuite.scala:159)
21/02/21 22:52:02.982 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:02.982 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:02.983 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[4] at map at KafkaRDDSuite.scala:159), which has no missing parents
21/02/21 22:52:02.987 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:02.991 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:02.992 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-JPLSL4N:59272 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:02.993 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:02.994 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[4] at map at KafkaRDDSuite.scala:159) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:02.994 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
21/02/21 22:52:02.996 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7254 bytes)
21/02/21 22:52:02.996 Executor task launch worker for task 7 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
21/02/21 22:52:03.001 Executor task launch worker for task 7 INFO KafkaRDD: Computing topic topicbasic-672398021-1613919119456, partition 0 offsets 0 -> 5
21/02/21 22:52:03.001 Executor task launch worker for task 7 INFO InternalKafkaConsumer: Initial fetch for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 0
21/02/21 22:52:03.001 Executor task launch worker for task 7 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1816117090-1613919119594-525, groupId=spark-executor-test-consumer-1816117090-1613919119594] Seeking to offset 0 for partition topicbasic-672398021-1613919119456-0
21/02/21 22:52:04.461 controller-event-thread INFO KafkaController: [Controller id=0] Processing automatic preferred replica leader election
21/02/21 22:52:13.513 Executor task launch worker for task 7 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)
java.lang.IllegalArgumentException: requirement failed: Failed to get records for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 4 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.get(KafkaDataConsumer.scala:142)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get(KafkaDataConsumer.scala:39)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get$(KafkaDataConsumer.scala:38)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.get(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:257)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:225)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/02/21 22:52:13.554 task-result-getter-3 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7, DESKTOP-JPLSL4N, executor driver): java.lang.IllegalArgumentException: requirement failed: Failed to get records for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 4 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.get(KafkaDataConsumer.scala:142)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get(KafkaDataConsumer.scala:39)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get$(KafkaDataConsumer.scala:38)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.get(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:257)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:225)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

21/02/21 22:52:13.556 task-result-getter-3 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job
21/02/21 22:52:13.557 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/02/21 22:52:13.559 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 7
21/02/21 22:52:13.559 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled
21/02/21 22:52:13.560 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (collect at KafkaRDDSuite.scala:159) failed in 10.575 s due to Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, DESKTOP-JPLSL4N, executor driver): java.lang.IllegalArgumentException: requirement failed: Failed to get records for spark-executor-test-consumer-1816117090-1613919119594 topicbasic-672398021-1613919119456-0 4 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.get(KafkaDataConsumer.scala:142)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get(KafkaDataConsumer.scala:39)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.get$(KafkaDataConsumer.scala:38)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.get(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:257)
	at org.apache.spark.streaming.kafka010.KafkaRDDIterator.next(KafkaRDD.scala:225)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
21/02/21 22:52:13.561 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 7 failed: collect at KafkaRDDSuite.scala:159, took 10.579415 s
21/02/21 22:52:13.562 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaRDDSuite: 'basic usage' =====

21/02/21 22:52:13.563 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaRDDSuite: 'compacted topic' =====

21/02/21 22:52:13.566 dispatcher-event-loop-2 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:52:13.613 ScalaTest-main-running-KafkaRDDSuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:52:13.614 ScalaTest-main-running-KafkaRDDSuite INFO BlockManager: BlockManager stopped
21/02/21 22:52:13.614 ScalaTest-main-running-KafkaRDDSuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:52:13.614 dispatcher-event-loop-2 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Running Spark version 3.0.1
21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO ResourceUtils: ==============================================================
21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO ResourceUtils: Resources for spark.driver:

21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO ResourceUtils: ==============================================================
21/02/21 22:52:13.620 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Submitted application: KafkaRDDSuite
21/02/21 22:52:13.621 ScalaTest-main-running-KafkaRDDSuite INFO SecurityManager: Changing view acls to: User
21/02/21 22:52:13.621 ScalaTest-main-running-KafkaRDDSuite INFO SecurityManager: Changing modify acls to: User
21/02/21 22:52:13.621 ScalaTest-main-running-KafkaRDDSuite INFO SecurityManager: Changing view acls groups to: 
21/02/21 22:52:13.621 ScalaTest-main-running-KafkaRDDSuite INFO SecurityManager: Changing modify acls groups to: 
21/02/21 22:52:13.621 ScalaTest-main-running-KafkaRDDSuite INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(User); groups with view permissions: Set(); users  with modify permissions: Set(User); groups with modify permissions: Set()
21/02/21 22:52:13.663 ScalaTest-main-running-KafkaRDDSuite INFO Utils: Successfully started service 'sparkDriver' on port 59322.
21/02/21 22:52:13.665 ScalaTest-main-running-KafkaRDDSuite INFO SparkEnv: Registering MapOutputTracker
21/02/21 22:52:13.665 ScalaTest-main-running-KafkaRDDSuite INFO SparkEnv: Registering BlockManagerMaster
21/02/21 22:52:13.665 ScalaTest-main-running-KafkaRDDSuite INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/21 22:52:13.665 ScalaTest-main-running-KafkaRDDSuite INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/21 22:52:13.666 ScalaTest-main-running-KafkaRDDSuite INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/21 22:52:13.667 ScalaTest-main-running-KafkaRDDSuite INFO DiskBlockManager: Created local directory at C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\blockmgr-c30654b1-b9cc-410e-8005-3732095a33a4
21/02/21 22:52:13.668 ScalaTest-main-running-KafkaRDDSuite INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
21/02/21 22:52:13.669 ScalaTest-main-running-KafkaRDDSuite INFO SparkEnv: Registering OutputCommitCoordinator
21/02/21 22:52:13.708 ScalaTest-main-running-KafkaRDDSuite INFO Executor: Starting executor ID driver on host DESKTOP-JPLSL4N
21/02/21 22:52:13.717 ScalaTest-main-running-KafkaRDDSuite INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59341.
21/02/21 22:52:13.717 ScalaTest-main-running-KafkaRDDSuite INFO NettyBlockTransferService: Server created on DESKTOP-JPLSL4N:59341
21/02/21 22:52:13.717 ScalaTest-main-running-KafkaRDDSuite INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/21 22:52:13.717 ScalaTest-main-running-KafkaRDDSuite INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59341, None)
21/02/21 22:52:13.718 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-JPLSL4N:59341 with 2.1 GiB RAM, BlockManagerId(driver, DESKTOP-JPLSL4N, 59341, None)
21/02/21 22:52:13.718 ScalaTest-main-running-KafkaRDDSuite INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-JPLSL4N, 59341, None)
21/02/21 22:52:13.718 ScalaTest-main-running-KafkaRDDSuite INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-JPLSL4N, 59341, None)
21/02/21 22:52:13.732 ScalaTest-main-running-KafkaRDDSuite INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 0 with message format version 2
21/02/21 22:52:13.734 ScalaTest-main-running-KafkaRDDSuite INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 0 ms
21/02/21 22:52:13.753 ScalaTest-main-running-KafkaRDDSuite INFO ProducerStateManager: [ProducerStateManager partition=topiccompacted--305301047-1613919133722-0] Writing producer snapshot at offset 7
21/02/21 22:52:13.757 ScalaTest-main-running-KafkaRDDSuite INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Rolled new log segment at offset 7 in 0 ms.
21/02/21 22:52:13.758 ScalaTest-main-running-KafkaRDDSuite INFO LogCleaner: Starting the log cleaner
21/02/21 22:52:13.759 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Starting
21/02/21 22:52:13.769 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Beginning cleaning of log topiccompacted--305301047-1613919133722-0.
21/02/21 22:52:13.770 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Building offset map for topiccompacted--305301047-1613919133722-0...
21/02/21 22:52:13.771 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Building offset map for log topiccompacted--305301047-1613919133722-0 for 1 segments in offset range [0, 7).
21/02/21 22:52:13.789 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Offset map for log topiccompacted--305301047-1613919133722-0 complete.
21/02/21 22:52:13.791 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Cleaning log topiccompacted--305301047-1613919133722-0 (cleaning prior to Sun Feb 21 06:52:13 PST 2021, discarding tombstones prior to Wed Dec 31 16:00:00 PST 1969)...
21/02/21 22:52:13.798 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Cleaning LogSegment(baseOffset=0, size=490, lastModifiedTime=1613919133747, largestTime=1613919133747) in log topiccompacted--305301047-1613919133722-0 into 0 with deletion horizon 0, retaining deletes.
21/02/21 22:52:13.815 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Swapping in cleaned segment LogSegment(baseOffset=0, size=210, lastModifiedTime=1613919133747, largestTime=1613919133747) for segment(s) List(LogSegment(baseOffset=0, size=490, lastModifiedTime=1613919133747, largestTime=1613919133747)) in log Log(dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0, topic=topiccompacted--305301047-1613919133722, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=7)
21/02/21 22:52:13.833 kafka-log-cleaner-thread-0 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Scheduling segments for deletion List(LogSegment(baseOffset=0, size=490, lastModifiedTime=1613919133747, largestTime=1613919133747))
21/02/21 22:52:13.840 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: 
	Log cleaner thread 0 cleaned log topiccompacted--305301047-1613919133722-0 (dirty section = [0, 0])
	0.0 MB of log processed in 0.1 seconds (0.0 MB/sec).
	Indexed 0.0 MB in 0.0 seconds (0.0 Mb/sec, 28.6% of total time)
	Buffer utilization: 0.0%
	Cleaned 0.0 MB in 0.1 seconds (0.0 Mb/sec, 71.4% of total time)
	Start size: 0.0 MB (7 messages)
	End size: 0.0 MB (3 messages)
	57.1% size reduction (57.1% fewer messages)

21/02/21 22:52:13.842 kafka-log-cleaner-thread-0 WARN LogCleaner: [kafka-log-cleaner-thread-0]: Unexpected exception thrown when cleaning log Log(dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0, topic=topiccompacted--305301047-1613919133722, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=7). Marking its partition (topiccompacted--305301047-1613919133722-0) as uncleanable
kafka.log.LogCleaningException: key not found: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:349)
	at kafka.log.LogCleaner$CleanerThread.tryCleanFilthiestLog(LogCleaner.scala:325)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:314)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
Caused by: java.util.NoSuchElementException: key not found: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83
	at scala.collection.immutable.Map$Map1.apply(Map.scala:114)
	at kafka.log.LogCleanerManager.$anonfun$updateCheckpoints$1(LogCleanerManager.scala:370)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.LogCleanerManager.updateCheckpoints(LogCleanerManager.scala:369)
	at kafka.log.LogCleanerManager.$anonfun$doneCleaning$1(LogCleanerManager.scala:434)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.log.LogCleanerManager.doneCleaning(LogCleanerManager.scala:432)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:383)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:345)
	... 3 more
21/02/21 22:52:14.765 ScalaTest-main-running-KafkaRDDSuite INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:52:14.765 ScalaTest-main-running-KafkaRDDSuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:52:14.766 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:52:14.766 ScalaTest-main-running-KafkaRDDSuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:52:14.776 ScalaTest-main-running-KafkaRDDSuite INFO AdminZkClient: Creating topic topiccompacted--305301047-1613919133722 with configuration {segment.bytes=256, segment.ms=1, cleanup.policy=compact, flush.messages=1} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:52:14.778 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30000 type:setData cxid:0xb zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/topiccompacted--305301047-1613919133722 Error:KeeperErrorCode = NoNode for /config/topics/topiccompacted--305301047-1613919133722
21/02/21 22:52:14.788 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topiccompacted--305301047-1613919133722)], deleted topics: [Set()], new partition replica assignment [Map(topiccompacted--305301047-1613919133722-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:52:14.788 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topiccompacted--305301047-1613919133722-0
21/02/21 22:52:14.801 data-plane-kafka-request-handler-1 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topiccompacted--305301047-1613919133722-0)
21/02/21 22:52:14.820 data-plane-kafka-request-handler-1 ERROR Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Could not find offset index file corresponding to log file C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0\00000000000000000007.log, recovering segment and rebuilding index files...
21/02/21 22:52:14.820 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 7 with message format version 2
21/02/21 22:52:14.825 data-plane-kafka-request-handler-1 INFO ProducerStateManager: [ProducerStateManager partition=topiccompacted--305301047-1613919133722-0] Loading producer state from snapshot file 'C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0\00000000000000000007.snapshot'
21/02/21 22:52:14.843 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Recovering unflushed segment 0
21/02/21 22:52:14.844 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 0 with message format version 2
21/02/21 22:52:14.860 data-plane-kafka-request-handler-1 INFO ProducerStateManager: [ProducerStateManager partition=topiccompacted--305301047-1613919133722-0] Writing producer snapshot at offset 7
21/02/21 22:52:14.862 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Recovering unflushed segment 7
21/02/21 22:52:14.862 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 7 with message format version 2
21/02/21 22:52:14.867 data-plane-kafka-request-handler-1 INFO ProducerStateManager: [ProducerStateManager partition=topiccompacted--305301047-1613919133722-0] Loading producer state from snapshot file 'C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0\00000000000000000007.snapshot'
21/02/21 22:52:14.889 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 7 with message format version 2
21/02/21 22:52:14.894 data-plane-kafka-request-handler-1 INFO ProducerStateManager: [ProducerStateManager partition=topiccompacted--305301047-1613919133722-0] Loading producer state from snapshot file 'C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0\00000000000000000007.snapshot'
21/02/21 22:52:14.894 data-plane-kafka-request-handler-1 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Completed load of log with 2 segments, log start offset 0 and log end offset 7 in 90 ms
21/02/21 22:52:14.895 data-plane-kafka-request-handler-1 INFO LogManager: Created log for partition topiccompacted--305301047-1613919133722-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 256, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 1, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:52:14.896 data-plane-kafka-request-handler-1 INFO Partition: [Partition topiccompacted--305301047-1613919133722-0 broker=0] No checkpointed highwatermark is found for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:14.896 data-plane-kafka-request-handler-1 INFO Partition: [Partition topiccompacted--305301047-1613919133722-0 broker=0] Log loaded for partition topiccompacted--305301047-1613919133722-0 with initial high watermark 0
21/02/21 22:52:14.896 data-plane-kafka-request-handler-1 INFO Partition: [Partition topiccompacted--305301047-1613919133722-0 broker=0] topiccompacted--305301047-1613919133722-0 starts at leader epoch 0 from offset 7 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:52:14.988 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:14.988 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:14.988 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-169577036-1613919134988
21/02/21 22:52:14.988 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:15.002 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:204
21/02/21 22:52:15.003 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (collect at KafkaRDDSuite.scala:204) with 1 output partitions
21/02/21 22:52:15.003 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at KafkaRDDSuite.scala:204)
21/02/21 22:52:15.003 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:15.003 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:15.003 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:15.007 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:15.010 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:15.011 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:15.012 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:15.013 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:15.013 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/02/21 22:52:15.014 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:15.015 Executor task launch worker for task 0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/02/21 22:52:15.019 Executor task launch worker for task 0 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:15.020 Executor task launch worker for task 0 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:59284]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-169577036-1613919134988
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:52:15.023 Executor task launch worker for task 0 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:52:15.023 Executor task launch worker for task 0 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:52:15.023 Executor task launch worker for task 0 INFO AppInfoParser: Kafka startTimeMs: 1613919135023
21/02/21 22:52:15.023 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Subscribed to partition(s): topiccompacted--305301047-1613919133722-0
21/02/21 22:52:15.024 Executor task launch worker for task 0 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:15.024 Executor task launch worker for task 0 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:15.026 Executor task launch worker for task 0 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:52:15.032 Executor task launch worker for task 0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 891 bytes result sent to driver
21/02/21 22:52:15.033 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 19 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:15.033 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/02/21 22:52:15.033 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at KafkaRDDSuite.scala:204) finished in 0.028 s
21/02/21 22:52:15.034 dag-scheduler-event-loop INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:15.034 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/02/21 22:52:15.034 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 0 finished: collect at KafkaRDDSuite.scala:204, took 0.031943 s
21/02/21 22:52:15.043 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: count at KafkaRDDSuite.scala:208
21/02/21 22:52:15.044 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (count at KafkaRDDSuite.scala:208) with 1 output partitions
21/02/21 22:52:15.044 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (count at KafkaRDDSuite.scala:208)
21/02/21 22:52:15.044 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:15.044 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:15.045 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:15.047 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KiB, free 2.1 GiB)
21/02/21 22:52:15.049 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 2.1 GiB)
21/02/21 22:52:15.049 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.8 KiB, free: 2.1 GiB)
21/02/21 22:52:15.050 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:15.050 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:15.050 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/21 22:52:15.051 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:15.052 Executor task launch worker for task 1 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/02/21 22:52:15.053 Executor task launch worker for task 1 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:15.054 Executor task launch worker for task 1 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:15.054 Executor task launch worker for task 1 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:15.559 Executor task launch worker for task 1 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 832 bytes result sent to driver
21/02/21 22:52:15.561 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 510 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:15.561 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/21 22:52:15.562 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (count at KafkaRDDSuite.scala:208) finished in 0.517 s
21/02/21 22:52:15.562 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:15.563 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/21 22:52:15.563 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 1 finished: count at KafkaRDDSuite.scala:208, took 0.520558 s
21/02/21 22:52:15.565 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: countApprox at KafkaRDDSuite.scala:209
21/02/21 22:52:15.571 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Job finished: countApprox at KafkaRDDSuite.scala:209, took 0.0056899 s
21/02/21 22:52:15.571 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (countApprox at KafkaRDDSuite.scala:209) with 1 output partitions
21/02/21 22:52:15.571 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (countApprox at KafkaRDDSuite.scala:209)
21/02/21 22:52:15.571 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:15.571 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:15.572 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:15.576 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.4 KiB, free 2.1 GiB)
21/02/21 22:52:15.580 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 2.1 GiB)
21/02/21 22:52:15.581 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.8 KiB, free: 2.1 GiB)
21/02/21 22:52:15.582 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:15.582 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:15.582 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
21/02/21 22:52:15.584 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:15.584 Executor task launch worker for task 2 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/02/21 22:52:15.588 Executor task launch worker for task 2 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:15.588 Executor task launch worker for task 2 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:15.588 Executor task launch worker for task 2 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:16.093 Executor task launch worker for task 2 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 832 bytes result sent to driver
21/02/21 22:52:16.094 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 511 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:16.094 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/02/21 22:52:16.095 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (countApprox at KafkaRDDSuite.scala:209) finished in 0.521 s
21/02/21 22:52:16.095 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:16.095 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/02/21 22:52:16.104 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: isEmpty at KafkaRDDSuite.scala:210
21/02/21 22:52:16.104 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (isEmpty at KafkaRDDSuite.scala:210) with 1 output partitions
21/02/21 22:52:16.105 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (isEmpty at KafkaRDDSuite.scala:210)
21/02/21 22:52:16.105 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:16.105 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:16.106 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:16.107 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:16.111 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:16.112 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:16.112 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:16.112 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:16.112 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/02/21 22:52:16.112 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:16.113 Executor task launch worker for task 3 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/02/21 22:52:16.114 Executor task launch worker for task 3 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:16.114 Executor task launch worker for task 3 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:16.114 Executor task launch worker for task 3 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:16.622 Executor task launch worker for task 3 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 777 bytes result sent to driver
21/02/21 22:52:16.623 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 511 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:16.624 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/21 22:52:16.624 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (isEmpty at KafkaRDDSuite.scala:210) finished in 0.517 s
21/02/21 22:52:16.625 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:16.625 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/21 22:52:16.625 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 3 finished: isEmpty at KafkaRDDSuite.scala:210, took 0.520275 s
21/02/21 22:52:16.639 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:211
21/02/21 22:52:16.640 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (take at KafkaRDDSuite.scala:211) with 1 output partitions
21/02/21 22:52:16.640 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (take at KafkaRDDSuite.scala:211)
21/02/21 22:52:16.640 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:16.641 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:16.641 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:16.644 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:16.646 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:16.648 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:16.648 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:16.649 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:16.649 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
21/02/21 22:52:16.650 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:16.651 Executor task launch worker for task 4 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
21/02/21 22:52:16.654 Executor task launch worker for task 4 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:16.655 Executor task launch worker for task 4 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:16.655 Executor task launch worker for task 4 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:17.161 Executor task launch worker for task 4 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 820 bytes result sent to driver
21/02/21 22:52:17.162 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 512 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:17.162 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/02/21 22:52:17.163 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (take at KafkaRDDSuite.scala:211) finished in 0.520 s
21/02/21 22:52:17.163 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:17.163 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/02/21 22:52:17.163 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 4 finished: take at KafkaRDDSuite.scala:211, took 0.523590 s
21/02/21 22:52:17.174 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:212
21/02/21 22:52:17.174 dag-scheduler-event-loop INFO DAGScheduler: Got job 5 (take at KafkaRDDSuite.scala:212) with 1 output partitions
21/02/21 22:52:17.174 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 5 (take at KafkaRDDSuite.scala:212)
21/02/21 22:52:17.175 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:17.175 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:17.175 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:17.178 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:17.181 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:17.182 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:17.183 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:17.183 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:17.183 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/02/21 22:52:17.185 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:17.185 Executor task launch worker for task 5 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
21/02/21 22:52:17.188 Executor task launch worker for task 5 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:17.189 Executor task launch worker for task 5 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:17.189 Executor task launch worker for task 5 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:17.694 Executor task launch worker for task 5 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 820 bytes result sent to driver
21/02/21 22:52:17.694 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 509 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:17.695 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/02/21 22:52:17.695 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 5 (take at KafkaRDDSuite.scala:212) finished in 0.519 s
21/02/21 22:52:17.695 dag-scheduler-event-loop INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:17.695 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/02/21 22:52:17.696 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 5 finished: take at KafkaRDDSuite.scala:212, took 0.522784 s
21/02/21 22:52:17.707 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDDSuite.scala:213
21/02/21 22:52:17.708 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (take at KafkaRDDSuite.scala:213) with 1 output partitions
21/02/21 22:52:17.708 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 6 (take at KafkaRDDSuite.scala:213)
21/02/21 22:52:17.708 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:17.708 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:17.709 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202), which has no missing parents
21/02/21 22:52:17.712 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:17.715 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:17.717 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:17.717 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:17.718 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[1] at map at KafkaRDDSuite.scala:202) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:17.718 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
21/02/21 22:52:17.719 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:17.720 Executor task launch worker for task 6 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
21/02/21 22:52:17.722 Executor task launch worker for task 6 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 7
21/02/21 22:52:17.722 Executor task launch worker for task 6 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:17.722 Executor task launch worker for task 6 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:18.230 Executor task launch worker for task 6 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 848 bytes result sent to driver
21/02/21 22:52:18.231 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 512 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:18.231 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/02/21 22:52:18.231 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 6 (take at KafkaRDDSuite.scala:213) finished in 0.520 s
21/02/21 22:52:18.232 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:18.232 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/02/21 22:52:18.232 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 6 finished: take at KafkaRDDSuite.scala:213, took 0.523893 s
21/02/21 22:52:18.232 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:18.232 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:18.233 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-169577036-1613919134988
21/02/21 22:52:18.233 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:18.245 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: take at KafkaRDD.scala:113
21/02/21 22:52:18.246 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (take at KafkaRDD.scala:113) with 1 output partitions
21/02/21 22:52:18.246 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (take at KafkaRDD.scala:113)
21/02/21 22:52:18.246 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:18.246 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:18.246 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (KafkaRDD[2] at RDD at KafkaRDD.scala:55), which has no missing parents
21/02/21 22:52:18.249 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 3.0 KiB, free 2.1 GiB)
21/02/21 22:52:18.251 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1989.0 B, free 2.1 GiB)
21/02/21 22:52:18.252 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 1989.0 B, free: 2.1 GiB)
21/02/21 22:52:18.253 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:18.253 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (KafkaRDD[2] at RDD at KafkaRDD.scala:55) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:18.253 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
21/02/21 22:52:18.254 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:18.255 Executor task launch worker for task 7 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
21/02/21 22:52:18.257 Executor task launch worker for task 7 INFO KafkaRDD: Beginning offset 0 is the same as ending offset skipping topiccompacted--305301047-1613919133722 0
21/02/21 22:52:18.258 Executor task launch worker for task 7 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 783 bytes result sent to driver
21/02/21 22:52:18.259 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:18.259 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/02/21 22:52:18.259 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (take at KafkaRDD.scala:113) finished in 0.011 s
21/02/21 22:52:18.260 dag-scheduler-event-loop INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:18.260 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/02/21 22:52:18.260 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 7 finished: take at KafkaRDD.scala:113, took 0.014684 s
21/02/21 22:52:18.261 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:18.261 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:18.261 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-169577036-1613919134988
21/02/21 22:52:18.261 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:18.275 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:224
21/02/21 22:52:18.275 dag-scheduler-event-loop INFO DAGScheduler: Got job 8 (collect at KafkaRDDSuite.scala:224) with 1 output partitions
21/02/21 22:52:18.276 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 8 (collect at KafkaRDDSuite.scala:224)
21/02/21 22:52:18.276 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:18.276 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:18.276 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[4] at map at KafkaRDDSuite.scala:224), which has no missing parents
21/02/21 22:52:18.278 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:18.280 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:18.281 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:18.282 dag-scheduler-event-loop INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:18.282 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[4] at map at KafkaRDDSuite.scala:224) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:18.282 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
21/02/21 22:52:18.283 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7259 bytes)
21/02/21 22:52:18.283 Executor task launch worker for task 8 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
21/02/21 22:52:18.286 Executor task launch worker for task 8 INFO KafkaRDD: Computing topic topiccompacted--305301047-1613919133722, partition 0 offsets 0 -> 8
21/02/21 22:52:18.286 Executor task launch worker for task 8 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 0
21/02/21 22:52:18.286 Executor task launch worker for task 8 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-169577036-1613919134988-526, groupId=spark-executor-test-consumer-169577036-1613919134988] Seeking to offset 0 for partition topiccompacted--305301047-1613919133722-0
21/02/21 22:52:28.791 Executor task launch worker for task 8 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)
java.lang.IllegalArgumentException: requirement failed: Failed to get records for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.compactedNext(KafkaDataConsumer.scala:185)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:59)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext$(KafkaDataConsumer.scala:58)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:304)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:268)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/02/21 22:52:28.794 task-result-getter-0 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8, DESKTOP-JPLSL4N, executor driver): java.lang.IllegalArgumentException: requirement failed: Failed to get records for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.compactedNext(KafkaDataConsumer.scala:185)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:59)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext$(KafkaDataConsumer.scala:58)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:304)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:268)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

21/02/21 22:52:28.794 task-result-getter-0 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
21/02/21 22:52:28.794 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/02/21 22:52:28.794 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 8
21/02/21 22:52:28.794 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage cancelled
21/02/21 22:52:28.794 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 8 (collect at KafkaRDDSuite.scala:224) failed in 10.517 s due to Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, DESKTOP-JPLSL4N, executor driver): java.lang.IllegalArgumentException: requirement failed: Failed to get records for compacted spark-executor-test-consumer-169577036-1613919134988 topiccompacted--305301047-1613919133722-0 after polling for 10000
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.compactedNext(KafkaDataConsumer.scala:185)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:59)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer.compactedNext$(KafkaDataConsumer.scala:58)
	at org.apache.spark.streaming.kafka010.KafkaDataConsumer$CachedKafkaDataConsumer.compactedNext(KafkaDataConsumer.scala:218)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:304)
	at org.apache.spark.streaming.kafka010.CompactedKafkaRDDIterator.next(KafkaRDD.scala:268)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
21/02/21 22:52:28.795 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 8 failed: collect at KafkaRDDSuite.scala:224, took 10.519853 s
21/02/21 22:52:28.796 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaRDDSuite: 'compacted topic' =====

21/02/21 22:52:28.796 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaRDDSuite: 'iterator boundary conditions' =====

21/02/21 22:52:28.799 ScalaTest-main-running-KafkaRDDSuite INFO AdminZkClient: Creating topic topicboundary-924818255-1613919148797 with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0))
21/02/21 22:52:28.800 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x177c51367b30000 type:setData cxid:0x12 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/config/topics/topicboundary-924818255-1613919148797 Error:KeeperErrorCode = NoNode for /config/topics/topicboundary-924818255-1613919148797
21/02/21 22:52:28.811 controller-event-thread INFO KafkaController: [Controller id=0] New topics: [Set(topicboundary-924818255-1613919148797)], deleted topics: [Set()], new partition replica assignment [Map(topicboundary-924818255-1613919148797-0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
21/02/21 22:52:28.811 controller-event-thread INFO KafkaController: [Controller id=0] New partition creation callback for topicboundary-924818255-1613919148797-0
21/02/21 22:52:28.822 data-plane-kafka-request-handler-7 INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topicboundary-924818255-1613919148797-0)
21/02/21 22:52:28.831 data-plane-kafka-request-handler-7 INFO Log: [Log partition=topicboundary-924818255-1613919148797-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Loading producer state till offset 0 with message format version 2
21/02/21 22:52:28.835 data-plane-kafka-request-handler-7 INFO Log: [Log partition=topicboundary-924818255-1613919148797-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms
21/02/21 22:52:28.836 data-plane-kafka-request-handler-7 INFO LogManager: Created log for partition topicboundary-924818255-1613919148797-0 in C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topicboundary-924818255-1613919148797-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 1, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
21/02/21 22:52:28.836 data-plane-kafka-request-handler-7 INFO Partition: [Partition topicboundary-924818255-1613919148797-0 broker=0] No checkpointed highwatermark is found for partition topicboundary-924818255-1613919148797-0
21/02/21 22:52:28.837 data-plane-kafka-request-handler-7 INFO Partition: [Partition topicboundary-924818255-1613919148797-0 broker=0] Log loaded for partition topicboundary-924818255-1613919148797-0 with initial high watermark 0
21/02/21 22:52:28.837 data-plane-kafka-request-handler-7 INFO Partition: [Partition topicboundary-924818255-1613919148797-0 broker=0] topicboundary-924818255-1613919148797-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
21/02/21 22:52:28.911 ScalaTest-main-running-KafkaRDDSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:59284]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:52:28.915 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:52:28.915 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:52:28.915 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka startTimeMs: 1613919148915
21/02/21 22:52:28.921 kafka-producer-network-thread | producer-18 INFO Metadata: [Producer clientId=producer-18] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:52:28.922 ScalaTest-main-running-KafkaRDDSuite INFO KafkaProducer: [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:52:28.935 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:28.935 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:28.935 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1081335133-1613919148911
21/02/21 22:52:28.935 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:28.956 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:248
21/02/21 22:52:28.956 dag-scheduler-event-loop INFO DAGScheduler: Got job 9 (collect at KafkaRDDSuite.scala:248) with 1 output partitions
21/02/21 22:52:28.956 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 9 (collect at KafkaRDDSuite.scala:248)
21/02/21 22:52:28.956 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:28.956 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:28.957 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[6] at map at KafkaRDDSuite.scala:248), which has no missing parents
21/02/21 22:52:28.959 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.9 KiB, free 2.1 GiB)
21/02/21 22:52:29.008 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:29.010 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.010 dag-scheduler-event-loop INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:29.011 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[6] at map at KafkaRDDSuite.scala:248) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:29.011 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
21/02/21 22:52:29.012 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7257 bytes)
21/02/21 22:52:29.012 Executor task launch worker for task 9 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
21/02/21 22:52:29.015 Executor task launch worker for task 9 INFO KafkaRDD: Computing topic topicboundary-924818255-1613919148797, partition 0 offsets 0 -> 18
21/02/21 22:52:29.015 Executor task launch worker for task 9 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:59284]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-test-consumer-1081335133-1613919148911
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

21/02/21 22:52:29.018 Executor task launch worker for task 9 INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:52:29.018 Executor task launch worker for task 9 INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:52:29.018 Executor task launch worker for task 9 INFO AppInfoParser: Kafka startTimeMs: 1613919149018
21/02/21 22:52:29.019 Executor task launch worker for task 9 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1081335133-1613919148911-527, groupId=spark-executor-test-consumer-1081335133-1613919148911] Subscribed to partition(s): topicboundary-924818255-1613919148797-0
21/02/21 22:52:29.019 Executor task launch worker for task 9 INFO InternalKafkaConsumer: Initial fetch for compacted spark-executor-test-consumer-1081335133-1613919148911 topicboundary-924818255-1613919148797-0 0
21/02/21 22:52:29.019 Executor task launch worker for task 9 INFO KafkaConsumer: [Consumer clientId=consumer-spark-executor-test-consumer-1081335133-1613919148911-527, groupId=spark-executor-test-consumer-1081335133-1613919148911] Seeking to offset 0 for partition topicboundary-924818255-1613919148797-0
21/02/21 22:52:29.020 Executor task launch worker for task 9 INFO Metadata: [Consumer clientId=consumer-spark-executor-test-consumer-1081335133-1613919148911-527, groupId=spark-executor-test-consumer-1081335133-1613919148911] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:52:29.024 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_2_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.8 KiB, free: 2.1 GiB)
21/02/21 22:52:29.026 Executor task launch worker for task 9 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 878 bytes result sent to driver
21/02/21 22:52:29.027 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 15 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:29.027 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
21/02/21 22:52:29.028 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 9 (collect at KafkaRDDSuite.scala:248) finished in 0.070 s
21/02/21 22:52:29.028 dag-scheduler-event-loop INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:29.028 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
21/02/21 22:52:29.028 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 9 finished: collect at KafkaRDDSuite.scala:248, took 0.071928 s
21/02/21 22:52:29.028 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:29.028 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:29.029 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1081335133-1613919148911
21/02/21 22:52:29.029 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:29.030 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_8_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.030 ScalaTest-main-running-KafkaRDDSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:59284]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:52:29.032 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:52:29.032 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:52:29.032 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka startTimeMs: 1613919149032
21/02/21 22:52:29.035 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_3_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.036 kafka-producer-network-thread | producer-19 INFO Metadata: [Producer clientId=producer-19] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:52:29.037 ScalaTest-main-running-KafkaRDDSuite INFO KafkaProducer: [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:52:29.042 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.046 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_7_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 1989.0 B, free: 2.1 GiB)
21/02/21 22:52:29.051 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_5_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.054 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_4_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.058 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_6_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.064 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-JPLSL4N:59341 in memory (size: 2.8 KiB, free: 2.1 GiB)
21/02/21 22:52:29.076 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:260
21/02/21 22:52:29.076 dag-scheduler-event-loop INFO DAGScheduler: Got job 10 (collect at KafkaRDDSuite.scala:260) with 1 output partitions
21/02/21 22:52:29.076 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 10 (collect at KafkaRDDSuite.scala:260)
21/02/21 22:52:29.076 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:29.076 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:29.076 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[8] at map at KafkaRDDSuite.scala:260), which has no missing parents
21/02/21 22:52:29.078 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:29.080 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:29.081 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.081 dag-scheduler-event-loop INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:29.082 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[8] at map at KafkaRDDSuite.scala:260) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:29.082 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
21/02/21 22:52:29.083 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7257 bytes)
21/02/21 22:52:29.083 Executor task launch worker for task 10 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
21/02/21 22:52:29.085 Executor task launch worker for task 10 INFO KafkaRDD: Beginning offset 18 is the same as ending offset skipping topicboundary-924818255-1613919148797 0
21/02/21 22:52:29.086 Executor task launch worker for task 10 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 751 bytes result sent to driver
21/02/21 22:52:29.086 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 3 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:29.086 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
21/02/21 22:52:29.087 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 10 (collect at KafkaRDDSuite.scala:260) finished in 0.009 s
21/02/21 22:52:29.087 dag-scheduler-event-loop INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:29.087 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
21/02/21 22:52:29.087 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 10 finished: collect at KafkaRDDSuite.scala:260, took 0.011511 s
21/02/21 22:52:29.087 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding enable.auto.commit to false for executor
21/02/21 22:52:29.088 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding auto.offset.reset to none for executor
21/02/21 22:52:29.088 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding executor group.id to spark-executor-test-consumer-1081335133-1613919148911
21/02/21 22:52:29.088 ScalaTest-main-running-KafkaRDDSuite WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
21/02/21 22:52:29.088 ScalaTest-main-running-KafkaRDDSuite INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:59284]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

21/02/21 22:52:29.091 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka version: 2.4.1
21/02/21 22:52:29.091 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka commitId: c57222ae8cd7866b
21/02/21 22:52:29.091 ScalaTest-main-running-KafkaRDDSuite INFO AppInfoParser: Kafka startTimeMs: 1613919149091
21/02/21 22:52:29.094 kafka-producer-network-thread | producer-20 INFO Metadata: [Producer clientId=producer-20] Cluster ID: KN4Tvo-zTrCFSZ_pAAFOuA
21/02/21 22:52:29.095 ScalaTest-main-running-KafkaRDDSuite INFO KafkaProducer: [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
21/02/21 22:52:29.114 ScalaTest-main-running-KafkaRDDSuite INFO SparkContext: Starting job: collect at KafkaRDDSuite.scala:269
21/02/21 22:52:29.114 dag-scheduler-event-loop INFO DAGScheduler: Got job 11 (collect at KafkaRDDSuite.scala:269) with 1 output partitions
21/02/21 22:52:29.114 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 11 (collect at KafkaRDDSuite.scala:269)
21/02/21 22:52:29.114 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
21/02/21 22:52:29.114 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
21/02/21 22:52:29.114 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[10] at map at KafkaRDDSuite.scala:269), which has no missing parents
21/02/21 22:52:29.116 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 4.8 KiB, free 2.1 GiB)
21/02/21 22:52:29.118 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 2.1 GiB)
21/02/21 22:52:29.119 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on DESKTOP-JPLSL4N:59341 (size: 2.9 KiB, free: 2.1 GiB)
21/02/21 22:52:29.119 dag-scheduler-event-loop INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1223
21/02/21 22:52:29.120 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[10] at map at KafkaRDDSuite.scala:269) (first 15 tasks are for partitions Vector(0))
21/02/21 22:52:29.120 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
21/02/21 22:52:29.121 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, DESKTOP-JPLSL4N, executor driver, partition 0, PROCESS_LOCAL, 7257 bytes)
21/02/21 22:52:29.121 Executor task launch worker for task 11 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
21/02/21 22:52:29.124 Executor task launch worker for task 11 INFO KafkaRDD: Computing topic topicboundary-924818255-1613919148797, partition 0 offsets 18 -> 19
21/02/21 22:52:29.127 Executor task launch worker for task 11 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 755 bytes result sent to driver
21/02/21 22:52:29.127 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 6 ms on DESKTOP-JPLSL4N (executor driver) (1/1)
21/02/21 22:52:29.127 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
21/02/21 22:52:29.128 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 11 (collect at KafkaRDDSuite.scala:269) finished in 0.013 s
21/02/21 22:52:29.128 dag-scheduler-event-loop INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/21 22:52:29.128 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
21/02/21 22:52:29.128 ScalaTest-main-running-KafkaRDDSuite INFO DAGScheduler: Job 11 finished: collect at KafkaRDDSuite.scala:269, took 0.014754 s
21/02/21 22:52:29.129 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaRDDSuite: 'iterator boundary conditions' =====

21/02/21 22:52:29.129 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== TEST OUTPUT FOR o.a.s.streaming.kafka010.KafkaRDDSuite: 'executor sorting' =====

21/02/21 22:52:29.135 ScalaTest-main-running-KafkaRDDSuite INFO KafkaRDDSuite: 

===== FINISHED o.a.s.streaming.kafka010.KafkaRDDSuite: 'executor sorting' =====

21/02/21 22:52:29.138 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/21 22:52:29.208 ScalaTest-main-running-DiscoverySuite INFO MemoryStore: MemoryStore cleared
21/02/21 22:52:29.208 ScalaTest-main-running-DiscoverySuite INFO BlockManager: BlockManager stopped
21/02/21 22:52:29.208 ScalaTest-main-running-DiscoverySuite INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/21 22:52:29.209 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/21 22:52:29.217 ScalaTest-main-running-DiscoverySuite INFO SparkContext: Successfully stopped SparkContext
21/02/21 22:52:29.217 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shutting down
21/02/21 22:52:29.217 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Starting controlled shutdown
21/02/21 22:52:29.220 controller-event-thread INFO KafkaController: [Controller id=0] Shutting down broker 0
21/02/21 22:52:29.220 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] Controlled shutdown succeeded
21/02/21 22:52:29.221 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutting down
21/02/21 22:52:29.221 /config/changes-event-process-thread INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Stopped
21/02/21 22:52:29.221 ScalaTest-main-running-DiscoverySuite INFO ZkNodeChangeNotificationListener$ChangeEventProcessThread: [/config/changes-event-process-thread]: Shutdown completed
21/02/21 22:52:29.221 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopping socket server request processors
21/02/21 22:52:29.224 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Stopped socket server request processors
21/02/21 22:52:29.224 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shutting down
21/02/21 22:52:29.224 ScalaTest-main-running-DiscoverySuite INFO KafkaRequestHandlerPool: [data-plane Kafka Request Handler on Broker 0], shut down completely
21/02/21 22:52:29.225 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutting down
21/02/21 22:52:29.335 ExpirationReaper-0-AlterAcls INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Stopped
21/02/21 22:52:29.335 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-AlterAcls]: Shutdown completed
21/02/21 22:52:29.335 ScalaTest-main-running-DiscoverySuite INFO KafkaApis: [KafkaApi-0] Shutdown complete.
21/02/21 22:52:29.335 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutting down
21/02/21 22:52:29.387 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Beginning cleaning of log topiccompacted--305301047-1613919133722-0.
21/02/21 22:52:29.387 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Building offset map for topiccompacted--305301047-1613919133722-0...
21/02/21 22:52:29.401 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Building offset map for log topiccompacted--305301047-1613919133722-0 for 1 segments in offset range [0, 7).
21/02/21 22:52:29.402 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Offset map for log topiccompacted--305301047-1613919133722-0 complete.
21/02/21 22:52:29.402 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Cleaning log topiccompacted--305301047-1613919133722-0 (cleaning prior to Sun Feb 21 06:52:13 PST 2021, discarding tombstones prior to Wed Dec 31 16:00:00 PST 1969)...
21/02/21 22:52:29.404 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Cleaning LogSegment(baseOffset=0, size=210, lastModifiedTime=1613919133747, largestTime=1613919133747) in log topiccompacted--305301047-1613919133722-0 into 0 with deletion horizon 0, retaining deletes.
21/02/21 22:52:29.418 kafka-log-cleaner-thread-0 INFO LogCleaner: Cleaner 0: Swapping in cleaned segment LogSegment(baseOffset=0, size=210, lastModifiedTime=1613919133747, largestTime=1613919133747) for segment(s) List(LogSegment(baseOffset=0, size=210, lastModifiedTime=1613919133747, largestTime=1613919133747)) in log Log(dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0, topic=topiccompacted--305301047-1613919133722, partition=0, highWatermark=7, lastStableOffset=7, logStartOffset=0, logEndOffset=7)
21/02/21 22:52:29.428 kafka-log-cleaner-thread-0 INFO Log: [Log partition=topiccompacted--305301047-1613919133722-0, dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83] Scheduling segments for deletion List(LogSegment(baseOffset=0, size=210, lastModifiedTime=1613919133747, largestTime=1613919133747))
21/02/21 22:52:29.436 kafka-log-cleaner-thread-0 WARN LogCleaner: [kafka-log-cleaner-thread-0]: Unexpected exception thrown when cleaning log Log(dir=C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-f57cd96b-f838-47bb-a8ec-59f870cdee83\topiccompacted--305301047-1613919133722-0, topic=topiccompacted--305301047-1613919133722, partition=0, highWatermark=7, lastStableOffset=7, logStartOffset=0, logEndOffset=7). Marking its partition (topiccompacted--305301047-1613919133722-0) as uncleanable
kafka.log.LogCleaningException: Kafka scheduler is not running.
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:349)
	at kafka.log.LogCleaner$CleanerThread.tryCleanFilthiestLog(LogCleaner.scala:325)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:314)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
Caused by: java.lang.IllegalStateException: Kafka scheduler is not running.
	at kafka.utils.KafkaScheduler.ensureRunning(KafkaScheduler.scala:149)
	at kafka.utils.KafkaScheduler.schedule(KafkaScheduler.scala:112)
	at kafka.log.Log.deleteSegmentFiles(Log.scala:2236)
	at kafka.log.Log.$anonfun$replaceSegments$6(Log.scala:2295)
	at kafka.log.Log.$anonfun$replaceSegments$6$adapted(Log.scala:2290)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Log.replaceSegments(Log.scala:2290)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:605)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:530)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:529)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:529)
	at kafka.log.Cleaner.clean(LogCleaner.scala:503)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:372)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:345)
	... 3 more
21/02/21 22:52:29.518 ExpirationReaper-0-topic INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Stopped
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-topic]: Shutdown completed
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutting down.
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO ProducerIdManager: [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO TransactionStateManager: [Transaction State Manager 0]: Shutdown complete
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutting down
21/02/21 22:52:29.518 TxnMarkerSenderThread-0 INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Stopped
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO TransactionMarkerChannelManager: [Transaction Marker Channel Manager 0]: Shutdown completed
21/02/21 22:52:29.518 ScalaTest-main-running-DiscoverySuite INFO TransactionCoordinator: [TransactionCoordinator id=0] Shutdown complete.
21/02/21 22:52:29.519 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutting down.
21/02/21 22:52:29.519 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutting down
21/02/21 22:52:29.523 ExpirationReaper-0-Heartbeat INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Stopped
21/02/21 22:52:29.523 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Heartbeat]: Shutdown completed
21/02/21 22:52:29.523 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutting down
21/02/21 22:52:29.536 ExpirationReaper-0-Rebalance INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Stopped
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Rebalance]: Shutdown completed
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO GroupCoordinator: [GroupCoordinator 0]: Shutdown complete.
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shutting down
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutting down
21/02/21 22:52:29.536 LogDirFailureHandler INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Stopped
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager$LogDirFailureHandler: [LogDirFailureHandler]: Shutdown completed
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutting down
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaFetcherManager: [ReplicaFetcherManager on broker 0] shutdown completed
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutting down
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO ReplicaAlterLogDirsManager: [ReplicaAlterLogDirsManager on broker 0] shutdown completed
21/02/21 22:52:29.536 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutting down
21/02/21 22:52:29.656 ExpirationReaper-0-Fetch INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Stopped
21/02/21 22:52:29.656 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Fetch]: Shutdown completed
21/02/21 22:52:29.656 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutting down
21/02/21 22:52:29.722 ExpirationReaper-0-Produce INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Stopped
21/02/21 22:52:29.722 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-Produce]: Shutdown completed
21/02/21 22:52:29.722 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutting down
21/02/21 22:52:29.902 ExpirationReaper-0-DeleteRecords INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Stopped
21/02/21 22:52:29.902 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-DeleteRecords]: Shutdown completed
21/02/21 22:52:29.902 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutting down
21/02/21 22:52:30.103 ExpirationReaper-0-ElectLeader INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Stopped
21/02/21 22:52:30.103 ScalaTest-main-running-DiscoverySuite INFO DelayedOperationPurgatory$ExpiredOperationReaper: [ExpirationReaper-0-ElectLeader]: Shutdown completed
21/02/21 22:52:30.116 ScalaTest-main-running-DiscoverySuite INFO ReplicaManager: [ReplicaManager broker=0] Shut down completely
21/02/21 22:52:30.116 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutting down.
21/02/21 22:52:30.116 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: Shutting down the log cleaner.
21/02/21 22:52:30.116 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutting down
21/02/21 22:52:30.116 kafka-log-cleaner-thread-0 INFO LogCleaner: [kafka-log-cleaner-thread-0]: Stopped
21/02/21 22:52:30.116 ScalaTest-main-running-DiscoverySuite INFO LogCleaner: [kafka-log-cleaner-thread-0]: Shutdown completed
21/02/21 22:52:30.117 pool-312-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topicbasic-672398021-1613919119456-0] Writing producer snapshot at offset 4
21/02/21 22:52:30.126 pool-312-thread-1 INFO ProducerStateManager: [ProducerStateManager partition=topicboundary-924818255-1613919148797-0] Writing producer snapshot at offset 41
21/02/21 22:52:30.169 ScalaTest-main-running-DiscoverySuite INFO LogManager: Shutdown complete.
21/02/21 22:52:30.169 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutting down
21/02/21 22:52:30.169 controller-event-thread INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Stopped
21/02/21 22:52:30.169 ScalaTest-main-running-DiscoverySuite INFO ControllerEventManager$ControllerEventThread: [ControllerEventThread controllerId=0] Shutdown completed
21/02/21 22:52:30.170 ScalaTest-main-running-DiscoverySuite INFO ZkPartitionStateMachine: [PartitionStateMachine controllerId=0] Stopped partition state machine
21/02/21 22:52:30.170 ScalaTest-main-running-DiscoverySuite INFO ZkReplicaStateMachine: [ReplicaStateMachine controllerId=0] Stopped replica state machine
21/02/21 22:52:30.170 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutting down
21/02/21 22:52:30.170 Controller-0-to-broker-0-send-thread INFO RequestSendThread: [RequestSendThread controllerId=0] Stopped
21/02/21 22:52:30.170 ScalaTest-main-running-DiscoverySuite INFO RequestSendThread: [RequestSendThread controllerId=0] Shutdown completed
21/02/21 22:52:30.172 ScalaTest-main-running-DiscoverySuite INFO KafkaController: [Controller id=0] Resigned
21/02/21 22:52:30.172 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closing.
21/02/21 22:52:30.173 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c51367b30001
21/02/21 22:52:30.176 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c51367b30001 closed
21/02/21 22:52:30.176 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c51367b30001
21/02/21 22:52:30.177 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:59281 which had sessionid 0x177c51367b30001
21/02/21 22:52:30.177 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient Kafka server] Closed.
21/02/21 22:52:30.177 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutting down
21/02/21 22:52:30.384 ThrottledChannelReaper-Fetch INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Stopped
21/02/21 22:52:30.384 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Fetch]: Shutdown completed
21/02/21 22:52:30.384 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutting down
21/02/21 22:52:31.384 ThrottledChannelReaper-Produce INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Stopped
21/02/21 22:52:31.384 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Produce]: Shutdown completed
21/02/21 22:52:31.384 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutting down
21/02/21 22:52:32.381 ThrottledChannelReaper-Request INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Stopped
21/02/21 22:52:32.381 ScalaTest-main-running-DiscoverySuite INFO ClientQuotaManager$ThrottledChannelReaper: [ThrottledChannelReaper-Request]: Shutdown completed
21/02/21 22:52:32.381 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutting down socket server
21/02/21 22:52:32.395 ScalaTest-main-running-DiscoverySuite INFO SocketServer: [SocketServer brokerId=0] Shutdown completed
21/02/21 22:52:32.396 ScalaTest-main-running-DiscoverySuite INFO KafkaServer: [KafkaServer id=0] shut down completed
21/02/21 22:52:32.429 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closing.
21/02/21 22:52:32.429 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: Processed session termination for sessionid: 0x177c51367b30000
21/02/21 22:52:32.434 ScalaTest-main-running-DiscoverySuite INFO ZooKeeper: Session: 0x177c51367b30000 closed
21/02/21 22:52:32.434 ScalaTest-main-running-DiscoverySuite-EventThread INFO ClientCnxn: EventThread shut down for session: 0x177c51367b30000
21/02/21 22:52:32.434 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxn: Closed socket connection for client /127.0.0.1:59278 which had sessionid 0x177c51367b30000
21/02/21 22:52:32.434 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperClient: [ZooKeeperClient] Closed.
21/02/21 22:52:32.434 NIOServerCxn.Factory:/127.0.0.1:0 INFO NIOServerCnxnFactory: NIOServerCnxn factory exited run method
21/02/21 22:52:32.435 ScalaTest-main-running-DiscoverySuite INFO ZooKeeperServer: shutting down
21/02/21 22:52:32.435 ScalaTest-main-running-DiscoverySuite INFO SessionTrackerImpl: Shutting down
21/02/21 22:52:32.435 ScalaTest-main-running-DiscoverySuite INFO PrepRequestProcessor: Shutting down
21/02/21 22:52:32.435 ScalaTest-main-running-DiscoverySuite INFO SyncRequestProcessor: Shutting down
21/02/21 22:52:32.435 ProcessThread(sid:0 cport:59275): INFO PrepRequestProcessor: PrepRequestProcessor exited loop!
21/02/21 22:52:32.435 SyncThread:0 INFO SyncRequestProcessor: SyncRequestProcessor exited!
21/02/21 22:52:32.435 ScalaTest-main-running-DiscoverySuite INFO FinalRequestProcessor: shutdown of request processor complete
21/02/21 22:52:32.439 ScalaTest-main-running-DiscoverySuite WARN KafkaTestUtils: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c22739fa-1d69-491b-96ee-67c7695c94db\version-2\log.1
21/02/21 22:52:32.444 ScalaTest-main-running-DiscoverySuite WARN KafkaRDDSuite: 

===== POSSIBLE THREAD LEAK IN SUITE o.a.s.streaming.kafka010.KafkaRDDSuite, thread names: ScalaTest-main-running-DiscoverySuite-SendThread(127.0.0.1:59275) =====

21/02/21 22:52:32.498 Thread-1 INFO ShutdownHookManager: Shutdown hook called
21/02/21 22:52:32.499 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c22739fa-1d69-491b-96ee-67c7695c94db
21/02/21 22:52:32.500 SessionTracker INFO SessionTrackerImpl: SessionTrackerImpl exited loop!
21/02/21 22:52:32.504 Thread-1 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c22739fa-1d69-491b-96ee-67c7695c94db
java.io.IOException: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c22739fa-1d69-491b-96ee-67c7695c94db\version-2\log.1
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
21/02/21 22:52:32.504 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-c8be9ef5-ba2a-4523-ab70-8dc2543b4887
21/02/21 22:52:32.507 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-fa748bea-6e08-420f-9dd0-681fe31ab7c1
21/02/21 22:52:32.513 Thread-1 INFO ShutdownHookManager: Deleting directory C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-18745cb3-16c8-4724-bded-25c36f16ea30
21/02/21 22:52:32.517 Thread-1 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-18745cb3-16c8-4724-bded-25c36f16ea30
java.io.IOException: Failed to delete: C:\Users\User\sparkJanuary\external\kafka-0-10\target\tmp\spark-18745cb3-16c8-4724-bded-25c36f16ea30\version-2\log.1
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
